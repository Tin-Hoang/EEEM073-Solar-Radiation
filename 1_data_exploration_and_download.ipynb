{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d30269",
   "metadata": {},
   "source": [
    "# Data Exploration and Download\n",
    "\n",
    "This notebook demonstrates exploring and downloading the National Solar Radiation Database (NSRDB) data for our project.\n",
    "\n",
    "The NSRDB data is provided from Amazon Web Services using the HDF Group's Highly Scalable Data Service (HSDS).\n",
    "These slicing methods would also work with the WIND Toolkit data available via HSDS at /nrel/wtk/\n",
    "\n",
    "This notebook is based on: https://github.com/NREL/hsds-examples\n",
    "\n",
    "Recommended way to run this is to use GitHub Codespaces.\n",
    " 1. Go to: https://github.com/NREL/hsds-examples\n",
    " 2. Create a new codespace based on that repo.\n",
    " 3. Copy this notebook to the codespace.\n",
    " 4. Install the dependencies: `pip install NREL-rex`\n",
    " 5. Run the notebook.\n",
    "\n",
    "If you want to run this notebook locally just to check the data, you can do the following:\n",
    " 1. Get your API key from: https://developer.nrel.gov/signup/\n",
    " 2. Install the dependencies: `pip install NREL-rex h5pyd`\n",
    " 3. Configure the API key by calling: `hsconfigure` and follow the instructions.\n",
    " 4. Run the notebook.\n",
    "\n",
    "**Note:** The dev API key is limited to certain requests per day, and very slow. So it is highly recommended to use GitHub Codespaces.\n",
    "\n",
    "For this project, we will download the NSRDB data for the 10 years from 2011 to 2020 for the location of **Ho Chi Minh City, Vietnam**.\n",
    "\n",
    "If you have trouble running the notebook, you can download the raw data from the following link:\n",
    "- Google Drive: https://drive.google.com/file/d/1U1RQHxjY50E8aTbF6RBiP08I-kvS6RSN/view?usp=sharing\n",
    "\n",
    "and extract it to the `data/raw/` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26b9a9",
   "metadata": {},
   "source": [
    "# NSRDB Data Exploration\n",
    "## 0. Choose the NSRDB file\n",
    "Here we select the Himawari 7 data for the year 2018, which should cover Vietnam area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c03bc562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from rex import NSRDBX\n",
    "\n",
    "NSRDB_FILE = '/nrel/nsrdb/himawari/himawari7/himawari7_2018.h5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c237c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# 1. Data exploration\n",
    "## 1.1 Check connection to HSDS and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0e6402",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>timezone</th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>county</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gid</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-15.55</td>\n",
       "      <td>-179.979996</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-15.59</td>\n",
       "      <td>-179.979996</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-15.63</td>\n",
       "      <td>-179.979996</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-15.67</td>\n",
       "      <td>-179.979996</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-15.71</td>\n",
       "      <td>-179.979996</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     latitude   longitude  elevation  timezone country state county\n",
       "gid                                                                \n",
       "0      -15.55 -179.979996          0        12    None  None   None\n",
       "1      -15.59 -179.979996          0        12    None  None   None\n",
       "2      -15.63 -179.979996          0        12    None  None   None\n",
       "3      -15.67 -179.979996          0        12    None  None   None\n",
       "4      -15.71 -179.979996          9        12    None  None   None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2018-01-01 00:00:00+00:00', '2018-01-01 00:30:00+00:00',\n",
       "               '2018-01-01 01:00:00+00:00', '2018-01-01 01:30:00+00:00',\n",
       "               '2018-01-01 02:00:00+00:00', '2018-01-01 02:30:00+00:00',\n",
       "               '2018-01-01 03:00:00+00:00', '2018-01-01 03:30:00+00:00',\n",
       "               '2018-01-01 04:00:00+00:00', '2018-01-01 04:30:00+00:00',\n",
       "               ...\n",
       "               '2018-12-31 19:00:00+00:00', '2018-12-31 19:30:00+00:00',\n",
       "               '2018-12-31 20:00:00+00:00', '2018-12-31 20:30:00+00:00',\n",
       "               '2018-12-31 21:00:00+00:00', '2018-12-31 21:30:00+00:00',\n",
       "               '2018-12-31 22:00:00+00:00', '2018-12-31 22:30:00+00:00',\n",
       "               '2018-12-31 23:00:00+00:00', '2018-12-31 23:30:00+00:00'],\n",
       "              dtype='datetime64[ns, UTC]', length=17520, freq=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "with NSRDBX(NSRDB_FILE, hsds=True) as f:\n",
    "    meta = f.meta\n",
    "    display(meta.head())\n",
    "    time_index = f.time_index\n",
    "    display(time_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870d3348",
   "metadata": {},
   "source": [
    "## 1.2 List all fields available in the NSRDB file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f92013f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSRDB File: /nrel/nsrdb/himawari/himawari7/himawari7_2018.h5\n",
      "\n",
      "Available datasets (28):\n",
      "1. alpha - Shape: (17520, 2170782), Type: uint8 (35.4 GB)\n",
      "2. aod - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "3. asymmetry - Shape: (17520, 2170782), Type: int8 (35.4 GB)\n",
      "4. meta - Shape: (2170782,), Type: [('latitude', '<f4'), ('longitude', '<f4'), ('elevation', '<i2'), ('timezone', '<i2'), ('country', 'S36'), ('state', 'S28'), ('county', 'S43')] (246.4 MB)\n",
      "5. ssa - Shape: (17520, 2170782), Type: uint8 (35.4 GB)\n",
      "6. time_index - Shape: (17520,), Type: |S19 (325.1 KB)\n",
      "7. ozone - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "8. solar_zenith_angle - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "9. surface_albedo - Shape: (17520, 2170782), Type: uint8 (35.4 GB)\n",
      "10. total_precipitable_water - Shape: (17520, 2170782), Type: uint8 (35.4 GB)\n",
      "11. clearsky_dhi - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "12. clearsky_dni - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "13. clearsky_ghi - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "14. cld_opd_dcomp - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "15. cld_press_acha - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "16. cld_reff_dcomp - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "17. cloud_fill_flag - Shape: (17520, 2170782), Type: uint8 (35.4 GB)\n",
      "18. cloud_type - Shape: (17520, 2170782), Type: int8 (35.4 GB)\n",
      "19. dew_point - Shape: (17520, 2170782), Type: int16 (70.8 GB)\n",
      "20. relative_humidity - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "21. surface_pressure - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "22. dhi - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "23. dni - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "24. fill_flag - Shape: (17520, 2170782), Type: uint8 (35.4 GB)\n",
      "25. ghi - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "26. air_temperature - Shape: (17520, 2170782), Type: int16 (70.8 GB)\n",
      "27. wind_direction - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "28. wind_speed - Shape: (17520, 2170782), Type: uint16 (70.8 GB)\n",
      "\n",
      "Metadata fields (7):\n",
      "1. latitude - Range: -54.9900016784668 to 59.970001220703125\n",
      "2. longitude - Range: -179.97999572753906 to 179.97999572753906\n",
      "3. elevation - Range: -893 to 7607\n",
      "4. timezone - Range: -12 to 14\n",
      "5. country - 54 unique values (sample: None, Fiji, United States, New Zealand, Wallis and Futuna...)\n",
      "6. state - 734 unique values (sample: None, Northern, Eastern, Alaska, Northern Islands...)\n",
      "7. county - 9071 unique values (sample: None, Cakaudrove, Lau, Aleutians West, Kermadec Islands...)\n",
      "\n",
      "Time index: Shape (17520,)\n",
      "  Start: 2018-01-01 00:00:00+00:00\n",
      "  End: 2018-12-31 23:30:00+00:00\n",
      "\n",
      "Data dimensions summary:\n",
      "  Metadata entries: 2170782\n"
     ]
    }
   ],
   "source": [
    "def list_nsrdb_fields(nsrdb_file, hsds=True):\n",
    "    \"\"\"List all fields available in an NSRDB file using NSRDBX with dataset shapes.\n",
    "\n",
    "    Args:\n",
    "        nsrdb_file (str): Path to the NSRDB file.\n",
    "        hsds (bool, optional): Whether to use HSDS (remote) or local file. Defaults to True.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with NSRDBX(nsrdb_file, hsds=hsds) as f:\n",
    "            # Print basic file info\n",
    "            print(f\"NSRDB File: {nsrdb_file}\")\n",
    "\n",
    "            # Get and print datasets with their shapes\n",
    "            datasets = f.datasets\n",
    "            print(f\"\\nAvailable datasets ({len(datasets)}):\")\n",
    "\n",
    "            # Get the h5 file handler to access shape without loading data\n",
    "            h5 = f.h5\n",
    "\n",
    "            for i, ds_name in enumerate(datasets, 1):\n",
    "                try:\n",
    "                    # Access dataset shape directly without loading data\n",
    "                    if ds_name in h5:\n",
    "                        ds_shape = h5[ds_name].shape\n",
    "                        dtype = h5[ds_name].dtype\n",
    "\n",
    "                        # Format the shape as a tuple string\n",
    "                        shape_str = str(ds_shape)\n",
    "\n",
    "                        # Calculate approximate size in memory if loaded\n",
    "                        memory_estimate = np.prod(ds_shape) * dtype.itemsize\n",
    "\n",
    "                        size_str = \"\"\n",
    "                        if memory_estimate < 1024:\n",
    "                            size_str = f\"{memory_estimate} bytes\"\n",
    "                        elif memory_estimate < 1024**2:\n",
    "                            size_str = f\"{memory_estimate/1024:.1f} KB\"\n",
    "                        elif memory_estimate < 1024**3:\n",
    "                            size_str = f\"{memory_estimate/(1024**2):.1f} MB\"\n",
    "                        elif memory_estimate < 1024**4:\n",
    "                            size_str = f\"{memory_estimate/(1024**3):.1f} GB\"\n",
    "                        else:\n",
    "                            size_str = f\"{memory_estimate/(1024**4):.1f} TB\"\n",
    "\n",
    "                        print(f\"{i}. {ds_name} - Shape: {shape_str}, Type: {dtype} ({size_str})\")\n",
    "                    else:\n",
    "                        print(f\"{i}. {ds_name} - Shape: Unknown (not directly accessible)\")\n",
    "                except Exception as ds_error:\n",
    "                    print(f\"{i}. {ds_name} - Error getting shape: {ds_error}\")\n",
    "\n",
    "            # Get metadata fields\n",
    "            meta = f.meta\n",
    "            print(f\"\\nMetadata fields ({len(meta.columns)}):\")\n",
    "            for i, field in enumerate(meta.columns, 1):\n",
    "                # Show sample of unique values for categorical fields\n",
    "                try:\n",
    "                    if meta[field].dtype == 'object':\n",
    "                        # Get sample of unique non-null values\n",
    "                        sample_values = meta[field].dropna().unique()[:5]\n",
    "                        num_unique = meta[field].nunique(dropna=True)\n",
    "                        sample_str = \", \".join([str(v) for v in sample_values])\n",
    "\n",
    "                        if num_unique <= 5:\n",
    "                            print(f\"{i}. {field} - {num_unique} unique values: {sample_str}\")\n",
    "                        else:\n",
    "                            print(f\"{i}. {field} - {num_unique} unique values (sample: {sample_str}...)\")\n",
    "                    else:\n",
    "                        # For numeric fields, show range\n",
    "                        min_val = meta[field].min()\n",
    "                        max_val = meta[field].max()\n",
    "                        print(f\"{i}. {field} - Range: {min_val} to {max_val}\")\n",
    "                except Exception as field_error:\n",
    "                    print(f\"{i}. {field} - Error analyzing: {str(field_error)[:50]}...\")\n",
    "\n",
    "            # Print time_index info\n",
    "            try:\n",
    "                time_index = f.time_index\n",
    "                time_shape = len(time_index)\n",
    "                print(f\"\\nTime index: Shape ({time_shape},)\")\n",
    "                print(f\"  Start: {time_index[0]}\")\n",
    "                print(f\"  End: {time_index[-1]}\")\n",
    "            except Exception as time_error:\n",
    "                print(f\"\\nError getting time index: {time_error}\")\n",
    "\n",
    "            # Print data dimensions summary\n",
    "            print(\"\\nData dimensions summary:\")\n",
    "            print(f\"  Metadata entries: {len(meta)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Example usage\n",
    "list_nsrdb_fields(NSRDB_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b403b41b",
   "metadata": {},
   "source": [
    "## 1.3 List all unique countries in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52b6cd1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "ename": "ResourceRuntimeError",
     "evalue": "Error retrieving data from \"<HDF5 dataset \"meta\": shape (2170782,), type \"|V119\">\" for slice: \"(slice(None, None, None),)\". Detected OSError/IOError from h5pyd. This is not a rex error and please do not submit a bug report. this is likely due to HSDS server limits, especially if you are using an NREL developer API key. For more details, see: https://nrel.github.io/rex/misc/examples.hsds.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/urllib3/response.py:754\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 754\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m    756\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    757\u001b[0m     \u001b[38;5;66;03m# FIXME: Ideally we'd like to include the url in the ReadTimeoutError but\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;66;03m# there is yet no clean way to get at it from this context.\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/urllib3/response.py:900\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    891\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce_content_length\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[38;5;66;03m# raised during streaming, so all calls with incorrect\u001b[39;00m\n\u001b[1;32m    899\u001b[0m         \u001b[38;5;66;03m# Content-Length are caught.\u001b[39;00m\n\u001b[0;32m--> 900\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp_bytes_read, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining)\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read1 \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    902\u001b[0m     (amt \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength_remaining \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m    903\u001b[0m ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[38;5;66;03m# `http.client.HTTPResponse`, so we close it here.\u001b[39;00m\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;66;03m# See https://github.com/python/cpython/issues/113199\u001b[39;00m\n",
      "\u001b[0;31mIncompleteRead\u001b[0m: IncompleteRead(51842628 bytes read, 206480430 more expected)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/requests/models.py:820\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 820\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/urllib3/response.py:1066\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1066\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data:\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/urllib3/response.py:983\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m<\u001b[39m amt \u001b[38;5;129;01mand\u001b[39;00m data:\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# TODO make sure to initially read enough data to get past the headers\u001b[39;00m\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;66;03m# For example, the GZ file header takes 10 bytes, we don't want to read\u001b[39;00m\n\u001b[1;32m    982\u001b[0m     \u001b[38;5;66;03m# it one byte at a time\u001b[39;00m\n\u001b[0;32m--> 983\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m     decoded_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode(data, decode_content, flush_decoder)\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/urllib3/response.py:878\u001b[0m, in \u001b[0;36mHTTPResponse._raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    876\u001b[0m fp_closed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 878\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_error_catcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfp_closed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.11-linux-x86_64-gnu/lib/python3.11/contextlib.py:158\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen\u001b[38;5;241m.\u001b[39mthrow(typ, value, traceback)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/urllib3/response.py:778\u001b[0m, in \u001b[0;36mHTTPResponse._error_catcher\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m         arg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection broken: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 778\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ProtocolError(arg, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (HTTPException, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mProtocolError\u001b[0m: ('Connection broken: IncompleteRead(51842628 bytes read, 206480430 more expected)', IncompleteRead(51842628 bytes read, 206480430 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/h5pyd/_hl/dataset.py:1205\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1205\u001b[0m     rsp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ioe:\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/h5pyd/_hl/base.py:986\u001b[0m, in \u001b[0;36mHLObject.GET\u001b[0;34m(self, req, params, use_cache, format)\u001b[0m\n\u001b[1;32m    985\u001b[0m downloaded_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 986\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhttp_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrsp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miter_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHTTP_CHUNK_SIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhttp_chunk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# filter out keep alive chunks\u001b[39;49;00m\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/requests/models.py:822\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m DecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mChunkedEncodingError\u001b[0m: ('Connection broken: IncompleteRead(51842628 bytes read, 206480430 more expected)', IncompleteRead(51842628 bytes read, 206480430 more expected))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/rex/resource.py:504\u001b[0m, in \u001b[0;36mResourceDataset._extract_ds_slice\u001b[0;34m(self, ds_slice)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 504\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mslices\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/h5pyd/_hl/dataset.py:1214\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, args, new_dtype)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1214\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError retrieving data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mioe\u001b[38;5;241m.\u001b[39merrno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rsp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1216\u001b[0m     \u001b[38;5;66;03m# hexencoded response?\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m     \u001b[38;5;66;03m# this is returned by API Gateway for lamba responses\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: Error retrieving data: None",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mResourceRuntimeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m unique_countries\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get and print the list of unique countries\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m unique_countries \u001b[38;5;241m=\u001b[39m \u001b[43mget_unique_countries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNSRDB_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_countries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unique countries in the NSRDB dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m country \u001b[38;5;129;01min\u001b[39;00m unique_countries:\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mget_unique_countries\u001b[0;34m(nsrdb_filepath)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_unique_countries\u001b[39m(nsrdb_filepath):\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get all unique countries from the NSRDB file.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m        list: Sorted list of unique country names.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mNSRDBX\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnsrdb_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhsds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Get metadata\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Extract unique countries\u001b[39;49;00m\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mget_unique_countries\u001b[0;34m(nsrdb_filepath)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get all unique countries from the NSRDB file.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;03m    list: Sorted list of unique country names.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m NSRDBX(nsrdb_filepath, hsds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Get metadata\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     meta \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# Extract unique countries\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     unique_countries \u001b[38;5;241m=\u001b[39m meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/rex/resource_extraction/resource_extraction.py:214\u001b[0m, in \u001b[0;36mResourceX.meta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmeta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m    Resource meta data DataFrame\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    meta : pandas.DataFrame\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta\u001b[49m\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/rex/resource.py:838\u001b[0m, in \u001b[0;36mBaseResource.meta\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5:\n\u001b[0;32m--> 838\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    839\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    840\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is not a valid dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/rex/resource.py:1316\u001b[0m, in \u001b[0;36mBaseResource._get_meta\u001b[0;34m(self, ds_name, ds_slice)\u001b[0m\n\u001b[1;32m   1313\u001b[0m     sites \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mslice\u001b[39m(sites, sites \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1315\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh5[ds_name]\n\u001b[0;32m-> 1316\u001b[0m meta \u001b[38;5;241m=\u001b[39m \u001b[43mResourceDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msites\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sites, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   1319\u001b[0m     stop \u001b[38;5;241m=\u001b[39m sites\u001b[38;5;241m.\u001b[39mstop\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/rex/resource.py:597\u001b[0m, in \u001b[0;36mResourceDataset.extract\u001b[0;34m(cls, ds, ds_slice, scale_attr, add_attr, unscale)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;124;03mExtract data from Resource Dataset\u001b[39;00m\n\u001b[1;32m    579\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[38;5;124;03m    Flag to unscale dataset data, by default True\u001b[39;00m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    594\u001b[0m dset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(ds, scale_attr\u001b[38;5;241m=\u001b[39mscale_attr, add_attr\u001b[38;5;241m=\u001b[39madd_attr,\n\u001b[1;32m    595\u001b[0m            unscale\u001b[38;5;241m=\u001b[39munscale)\n\u001b[0;32m--> 597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mds_slice\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/rex/resource.py:77\u001b[0m, in \u001b[0;36mResourceDataset.__getitem__\u001b[0;34m(self, ds_slice)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, ds_slice):\n\u001b[1;32m     75\u001b[0m     ds_slice \u001b[38;5;241m=\u001b[39m parse_slice(ds_slice)\n\u001b[0;32m---> 77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ds_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_slice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/rex/resource.py:567\u001b[0m, in \u001b[0;36mResourceDataset._get_ds_slice\u001b[0;34m(self, ds_slice)\u001b[0m\n\u001b[1;32m    565\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_list_slice(ds_slice)\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_ds_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_slice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unscale:\n\u001b[1;32m    570\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unscale_data(out)\n",
      "File \u001b[0;32m~/.venv/master/lib/python3.11/site-packages/rex/resource.py:519\u001b[0m, in \u001b[0;36mResourceDataset._extract_ds_slice\u001b[0;34m(self, ds_slice)\u001b[0m\n\u001b[1;32m    512\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Detected OSError/IOError from h5pyd. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis is not a rex error and please do not submit \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    514\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma bug report. this is likely due to HSDS server \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    515\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlimits, especially if you are using an NREL \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    516\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdeveloper API key. For more details, see: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    517\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://nrel.github.io/rex/misc/examples.hsds.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    518\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(msg)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResourceRuntimeError(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;66;03m# check to see if idx_slice needs to be applied\u001b[39;00m\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(s \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(s, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    523\u001b[0m        \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m idx_slice):\n",
      "\u001b[0;31mResourceRuntimeError\u001b[0m: Error retrieving data from \"<HDF5 dataset \"meta\": shape (2170782,), type \"|V119\">\" for slice: \"(slice(None, None, None),)\". Detected OSError/IOError from h5pyd. This is not a rex error and please do not submit a bug report. this is likely due to HSDS server limits, especially if you are using an NREL developer API key. For more details, see: https://nrel.github.io/rex/misc/examples.hsds.html"
     ]
    }
   ],
   "source": [
    "# Function to get all unique countries from the NSRDB file\n",
    "def get_unique_countries(nsrdb_filepath):\n",
    "    \"\"\"Get all unique countries from the NSRDB file.\n",
    "\n",
    "    Args:\n",
    "        nsrdb_filepath (str): Path to the NSRDB file.\n",
    "\n",
    "    Returns:\n",
    "        list: Sorted list of unique country names.\n",
    "    \"\"\"\n",
    "    with NSRDBX(nsrdb_filepath, hsds=True) as f:\n",
    "        # Get metadata\n",
    "        meta = f.meta\n",
    "        # Extract unique countries\n",
    "        unique_countries = meta['country'].unique()\n",
    "        # Filter out None values and sort\n",
    "        unique_countries = [country for country in unique_countries if country is not None]\n",
    "        unique_countries.sort()\n",
    "    return unique_countries\n",
    "\n",
    "# Get and print the list of unique countries\n",
    "unique_countries = get_unique_countries(NSRDB_FILE)\n",
    "print(f\"There are {len(unique_countries)} unique countries in the NSRDB dataset:\")\n",
    "for country in unique_countries:\n",
    "    print(f\"- {country}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393977a2",
   "metadata": {},
   "source": [
    "## 1.4 List all states and counties within a specific country (Vietnam for this example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553dfd80",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def list_states_counties_by_country(country_name, nsrdb_file=NSRDB_FILE):\n",
    "    \"\"\"List all states and counties within a specific country from NSRDB data with counts.\n",
    "\n",
    "    Args:\n",
    "        country_name (str): Name of the country to filter by (e.g., \"Vietnam\").\n",
    "        nsrdb_file (str, optional): Path to the NSRDB file. Defaults to NSRDB_FILE.\n",
    "    \"\"\"\n",
    "    with NSRDBX(nsrdb_file, hsds=True) as f:\n",
    "        # Get metadata\n",
    "        meta = f.meta\n",
    "\n",
    "        # Filter by country\n",
    "        country_meta = meta[meta['country'] == country_name]\n",
    "\n",
    "        if country_meta.empty:\n",
    "            print(f\"No data found for country: {country_name}\")\n",
    "            return\n",
    "\n",
    "        # Get unique states\n",
    "        states = country_meta['state'].unique()\n",
    "        states = [s for s in states if not pd.isna(s)]\n",
    "\n",
    "        print(f\"Country: {country_name}\")\n",
    "        print(f\"Total locations: {len(country_meta)}\")\n",
    "        print(f\"Number of states/provinces: {len(states)}\")\n",
    "\n",
    "        # Display each state and its counties with counts\n",
    "        for state in sorted(states):\n",
    "            state_data = country_meta[country_meta['state'] == state]\n",
    "            state_count = len(state_data)\n",
    "\n",
    "            counties = state_data['county'].unique()\n",
    "            counties = [c for c in counties if not pd.isna(c)]\n",
    "\n",
    "            print(f\"\\nState/Province: {state} ({state_count} data points)\")\n",
    "            print(f\"  Number of counties/districts: {len(counties)}\")\n",
    "\n",
    "            if len(counties) > 0:\n",
    "                print(\"  Counties/Districts:\")\n",
    "                for county in sorted(counties):\n",
    "                    county_count = len(state_data[state_data['county'] == county])\n",
    "                    print(f\"    - {county} ({county_count} data points)\")\n",
    "            else:\n",
    "                print(\"  No county/district information available\")\n",
    "\n",
    "        # Check for entries without state information\n",
    "        no_state = country_meta[pd.isna(country_meta['state'])]\n",
    "        if not no_state.empty:\n",
    "            print(\"\\nLocations without state/province information:\")\n",
    "            print(f\"  Count: {len(no_state)}\")\n",
    "\n",
    "            # Try to identify any counties that might exist without state info\n",
    "            no_state_counties = no_state['county'].unique()\n",
    "            no_state_counties = [c for c in no_state_counties if not pd.isna(c)]\n",
    "\n",
    "            if len(no_state_counties) > 0:\n",
    "                print(\"  Counties/Districts without state information:\")\n",
    "                for county in sorted(no_state_counties):\n",
    "                    county_count = len(no_state[no_state['county'] == county])\n",
    "                    print(f\"    - {county} ({county_count} data points)\")\n",
    "\n",
    "        # Add a summary with percentages\n",
    "        if len(country_meta) > 0:\n",
    "            print(f\"\\nSummary for {country_name}:\")\n",
    "\n",
    "            # Top 5 states by data point count\n",
    "            if len(states) > 0:\n",
    "                state_counts = country_meta.groupby('state').size().sort_values(ascending=False)\n",
    "                print(\"\\nTop states by data points:\")\n",
    "                for i, (state, count) in enumerate(state_counts.head(5).items(), 1):\n",
    "                    if pd.isna(state):\n",
    "                        state_name = \"Unknown\"\n",
    "                    else:\n",
    "                        state_name = state\n",
    "                    percentage = (count / len(country_meta)) * 100\n",
    "                    print(f\"  {i}. {state_name}: {count} points ({percentage:.1f}%)\")\n",
    "\n",
    "# Example usage - list all states and counties in Vietnam\n",
    "list_states_counties_by_country(\"Vietnam\", nsrdb_file=NSRDB_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415f6a32",
   "metadata": {},
   "source": [
    "## 1.5 Plot GHI heatmap for Vietnam country\n",
    "Visualize GHI for Vietnam map at noon (5:00 AM UTC -> 12:00 PM Vietnam time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73802be5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def plot_ghi_heatmap(region_name, date, region_type='country', nsrdb_file=NSRDB_FILE, figsize=None, cmap='YlOrRd', aspect_equal=False):\n",
    "    \"\"\"Plot a GHI heatmap for a region (country or state) at a specific date/time.\n",
    "\n",
    "    Args:\n",
    "        region_name (str): Name of the region to plot.\n",
    "        date (str): Date in format 'YYYY-MM-DD HH:MM:SS'.\n",
    "        region_type (str, optional): Type of region, either 'country' or 'state'. Defaults to 'country'.\n",
    "        nsrdb_file (str, optional): Path to the NSRDB file. Defaults to NSRDB_FILE.\n",
    "        figsize (tuple, optional): Figure size (width, height). If None, uses (12, 20) for country and (10, 8) for state.\n",
    "        cmap (str, optional): Colormap to use for the heatmap. Defaults to 'YlOrRd'.\n",
    "        aspect_equal (bool, optional): Whether to set aspect ratio to equal. Defaults to False for country, True for state.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Set default parameters based on region type\n",
    "    if figsize is None:\n",
    "        figsize = (12, 20) if region_type == 'country' else (10, 8)\n",
    "\n",
    "    region_col = region_type.lower()  # 'country' or 'state'\n",
    "    if aspect_equal is None:\n",
    "        aspect_equal = (region_type == 'state')  # True for state, False for country\n",
    "\n",
    "    with NSRDBX(nsrdb_file, hsds=True) as f:\n",
    "        # Get GHI data for the region\n",
    "        ghi_map = f.get_timestep_map('ghi', date, region=region_name, region_col=region_col)\n",
    "\n",
    "        if ghi_map is None or len(ghi_map) == 0:\n",
    "            print(f\"No data available for {region_type} {region_name} at {date}\")\n",
    "            return\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        # Create heatmap using scatter plot with colormap\n",
    "        scatter = ax.scatter(ghi_map['longitude'], ghi_map['latitude'],\n",
    "                  c=ghi_map['ghi'], cmap=cmap,\n",
    "                  s=50 if region_type == 'state' else 20,\n",
    "                  alpha=0.8,\n",
    "                  edgecolors='k' if region_type == 'state' else 'none',\n",
    "                  linewidths=0.5 if region_type == 'state' else 0)\n",
    "\n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=ax)\n",
    "        cbar.set_label('GHI (W/m²)', rotation=270, labelpad=15)\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Longitude')\n",
    "        ax.set_ylabel('Latitude')\n",
    "        ax.set_title(f'GHI Heatmap for {region_name} at {date}')\n",
    "\n",
    "        # Adjust aspect ratio if needed\n",
    "        if aspect_equal:\n",
    "            ax.set_aspect('equal')\n",
    "\n",
    "        # Add grid for better readability\n",
    "        ax.grid(alpha=0.3)\n",
    "\n",
    "        # Add timestamp annotation for country-level plots\n",
    "        if region_type == 'country':\n",
    "            ax.annotate(f'Timestamp: {date}', xy=(0.02, 0.02), xycoords='figure fraction')\n",
    "\n",
    "        # Add statistics annotation for state-level plots\n",
    "        if region_type == 'state':\n",
    "            stats_text = (f\"Min: {ghi_map['ghi'].min():.1f} W/m²\\n\"\n",
    "                         f\"Max: {ghi_map['ghi'].max():.1f} W/m²\\n\"\n",
    "                         f\"Mean: {ghi_map['ghi'].mean():.1f} W/m²\")\n",
    "            ax.annotate(stats_text, xy=(0.02, 0.02), xycoords='axes fraction',\n",
    "                      bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print statistics\n",
    "        print(f\"GHI Statistics for {region_type} {region_name} at {date}:\")\n",
    "        print(f\"  Number of data points: {len(ghi_map)}\")\n",
    "        print(f\"  Min GHI: {ghi_map['ghi'].min():.2f} W/m²\")\n",
    "        print(f\"  Max GHI: {ghi_map['ghi'].max():.2f} W/m²\")\n",
    "        print(f\"  Mean GHI: {ghi_map['ghi'].mean():.2f} W/m²\")\n",
    "        if region_type == 'country':  # Only include median for country-level plots as in the original\n",
    "            print(f\"  Median GHI: {ghi_map['ghi'].median():.2f} W/m²\")\n",
    "\n",
    "# Visualize GHI for Vietnam at noon (5:00 AM UTC)\n",
    "plot_ghi_heatmap(\"Vietnam\", '2018-03-19 05:00:00', region_type='country', figsize=(12, 20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6284a610",
   "metadata": {},
   "source": [
    "## 1.6 Plot GHI heatmap for Hồ Chí Minh state\n",
    "Plot for Ho Chi Minh city region only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8748f7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Example usage - visualize GHI for Hồ Chí Minh\n",
    "plot_ghi_heatmap(\"Hồ Chí Minh\", '2018-03-19 05:00:00', region_type='state', figsize=(12, 10), aspect_equal=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f2d03e",
   "metadata": {},
   "source": [
    "## 1.7 Plot GHI time series for a few days\n",
    "Plot GHI time series for a specific location over a date range.\n",
    "\n",
    "The GHI should go up around 12:00 PM (noon), and be zero at night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8534a037",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_ghi_timeseries(location_id, start_date, end_date, nsrdb_file=NSRDB_FILE, figsize=(14, 6)):\n",
    "    \"\"\"Plot GHI time series for a specific location over a date range.\n",
    "\n",
    "    Args:\n",
    "        location_id (int): Location ID to plot time series for.\n",
    "        start_date (str): Start date in format 'YYYY-MM-DD'.\n",
    "        end_date (str): End date in format 'YYYY-MM-DD'.\n",
    "        nsrdb_file (str, optional): Path to the NSRDB file. Defaults to NSRDB_FILE.\n",
    "        figsize (tuple, optional): Figure size (width, height). Defaults to (14, 6).\n",
    "    \"\"\"\n",
    "    with NSRDBX(nsrdb_file, hsds=True) as f:\n",
    "        # Check if the location exists\n",
    "        meta = f.meta\n",
    "        if location_id not in meta.index:\n",
    "            print(f\"Location ID {location_id} not found in the dataset\")\n",
    "            return\n",
    "\n",
    "        # Get lat/lon for the location\n",
    "        lat = meta.loc[location_id, 'latitude']\n",
    "        lon = meta.loc[location_id, 'longitude']\n",
    "\n",
    "        # Convert date strings to timezone-aware timestamps (UTC)\n",
    "        ts_start = pd.Timestamp(start_date).tz_localize('UTC')\n",
    "        ts_end = pd.Timestamp(end_date).tz_localize('UTC')\n",
    "\n",
    "        # Get time slice for the date range\n",
    "        time_index = f.time_index\n",
    "        mask = (time_index >= ts_start) & (time_index <= ts_end)\n",
    "        if not any(mask):\n",
    "            print(f\"No data available for the date range {start_date} to {end_date}\")\n",
    "            return\n",
    "\n",
    "        # Get GHI time series for the location using get_gid_ts instead of get_time_series\n",
    "        ghi_ts = f.get_gid_ts('ghi', location_id)\n",
    "\n",
    "        # Filter to the date range\n",
    "        ghi_ts = ghi_ts[mask]\n",
    "        times = time_index[mask]\n",
    "\n",
    "        # Convert UTC times to Ho Chi Minh time (UTC+7)\n",
    "        ho_chi_minh_times = times.tz_convert('Asia/Ho_Chi_Minh')\n",
    "\n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "        # Plot time series with Ho Chi Minh time\n",
    "        ax.plot(ho_chi_minh_times, ghi_ts, 'o-', color='orange', markersize=4, linewidth=1.5)\n",
    "\n",
    "        # Add day/night shading for better readability (using local time)\n",
    "        start_date_local = ts_start.tz_convert('Asia/Ho_Chi_Minh').date()\n",
    "        end_date_local = ts_end.tz_convert('Asia/Ho_Chi_Minh').date()\n",
    "        date_list = pd.date_range(start=start_date_local, end=end_date_local, freq='D')\n",
    "\n",
    "        for i, date in enumerate(date_list):\n",
    "            if i % 2 == 0:  # Shade every other day\n",
    "                ax.axvspan(\n",
    "                    pd.Timestamp(date).tz_localize('Asia/Ho_Chi_Minh'),\n",
    "                    pd.Timestamp(date + pd.Timedelta(days=1)).tz_localize('Asia/Ho_Chi_Minh'),\n",
    "                    alpha=0.1, color='gray'\n",
    "                )\n",
    "\n",
    "        # Set labels and title\n",
    "        ax.set_xlabel('Local Time (UTC+7)')\n",
    "        ax.set_ylabel('GHI (W/m²)')\n",
    "        ax.set_title(f'GHI Time Series at {lat:.4f}°N, {lon:.4f}°E\\n{start_date} to {end_date} (Ho Chi Minh Time)')\n",
    "\n",
    "        # Format x-axis to show dates properly\n",
    "        ax.xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        # Add grid for better readability\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "        # Add annotation with location info\n",
    "        location_info = meta.loc[location_id]\n",
    "        location_text = f\"Location ID: {location_id}\\n\"\n",
    "        if not pd.isna(location_info['state']):\n",
    "            location_text += f\"State: {location_info['state']}\\n\"\n",
    "        if not pd.isna(location_info['county']):\n",
    "            location_text += f\"County: {location_info['county']}\"\n",
    "\n",
    "        ax.annotate(location_text, xy=(0.02, 0.96), xycoords='axes fraction',\n",
    "                  bbox=dict(boxstyle=\"round,pad=0.5\", fc=\"white\", alpha=0.8),\n",
    "                  verticalalignment='top')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print some statistics\n",
    "        print(f\"GHI Statistics for location {location_id} from {start_date} to {end_date}:\")\n",
    "        print(f\"  Number of data points: {len(ghi_ts)}\")\n",
    "        print(f\"  Min GHI: {ghi_ts.min():.2f} W/m²\")\n",
    "        print(f\"  Max GHI: {ghi_ts.max():.2f} W/m²\")\n",
    "        print(f\"  Mean GHI: {ghi_ts.mean():.2f} W/m²\")\n",
    "        print(f\"  Daily pattern: {'Clear' if ghi_ts.max() > 800 else 'Cloudy'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a06913",
   "metadata": {},
   "source": [
    "Get a random location ID from Hồ Chí Minh to use for time series plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c46855c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_sample_location_ids(region, region_col='state', n=5, nsrdb_file=NSRDB_FILE):\n",
    "    \"\"\"Get sample location IDs from a specific region for time series analysis.\n",
    "\n",
    "    Args:\n",
    "        region (str): Region name to filter by.\n",
    "        region_col (str, optional): Column name to filter on ('state', 'country', etc.). Defaults to 'state'.\n",
    "        n (int, optional): Number of sample locations to return. Defaults to 5.\n",
    "        nsrdb_file (str, optional): Path to the NSRDB file. Defaults to NSRDB_FILE.\n",
    "\n",
    "    Returns:\n",
    "        list: List of location IDs.\n",
    "    \"\"\"\n",
    "    with NSRDBX(nsrdb_file, hsds=True) as f:\n",
    "        meta = f.meta\n",
    "        region_data = meta[meta[region_col] == region]\n",
    "\n",
    "        if region_data.empty:\n",
    "            print(f\"No data found for {region_col}={region}\")\n",
    "            return []\n",
    "\n",
    "        # Get n random samples or all if fewer than n\n",
    "        if len(region_data) <= n:\n",
    "            sample_ids = region_data.index.tolist()\n",
    "        else:\n",
    "            sample_ids = region_data.sample(n).index.tolist()\n",
    "\n",
    "        return sample_ids\n",
    "\n",
    "# Get 5 sample location IDs from Hồ Chí Minh\n",
    "hcm_location_ids = get_sample_location_ids(\"Hồ Chí Minh\", n=5)\n",
    "print(f\"Sample location IDs from Hồ Chí Minh: {hcm_location_ids}\")\n",
    "\n",
    "# Plot time series for the first sample location ID for a few days\n",
    "if hcm_location_ids:\n",
    "    plot_ghi_timeseries(hcm_location_ids[0], '2018-10-15', '2018-10-18')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa08822f",
   "metadata": {},
   "source": [
    "# 2. Download the data for a specific state (Hồ Chí Minh)\n",
    "Download Himawari 7 from 2011 to 2020 for Hồ Chí Minh region\n",
    "\n",
    "Highly recommended running this on GitHub Codespaces (see the introduction above).\n",
    "If you run it locally, the API key will be rate limited and you will not be able to download the data.\n",
    "If it being rate limited, you can wait for the rate limit to reset (1 hour) and try again and downlaod each year sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe39e8a",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from rex import NSRDBX\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "\n",
    "def extract_and_save_nsrdb_data(nsrdb_files, country=None, state=None, output_dir=None):\n",
    "    \"\"\"Extract data from NSRDB files based on geographic filters using rex's save_region.\n",
    "\n",
    "    Args:\n",
    "        nsrdb_files (list or str): List of NSRDB file paths or single file path.\n",
    "        country (str, optional): Filter by country name (None means no filter). Defaults to None.\n",
    "        state (str, optional): Filter by state name (None means no filter). Defaults to None.\n",
    "        output_dir (str, optional): Directory to save output files (defaults to current directory). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: List of output file paths created.\n",
    "    \"\"\"\n",
    "    # Handle single file input\n",
    "    if isinstance(nsrdb_files, str):\n",
    "        nsrdb_files = [nsrdb_files]\n",
    "\n",
    "    # Set default output directory\n",
    "    if output_dir is None:\n",
    "        output_dir = os.getcwd()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Determine which filter to use (prioritize more specific filters)\n",
    "    region_value = None\n",
    "    region_col = None\n",
    "\n",
    "    if state is not None:\n",
    "        region_value = state\n",
    "        region_col = \"state\"\n",
    "    elif country is not None:\n",
    "        region_value = country\n",
    "        region_col = \"country\"\n",
    "\n",
    "    if region_value is None:\n",
    "        print(\"No region filters specified. Please provide at least one filter.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Create filter string for filename\n",
    "    filter_parts = []\n",
    "    if country:\n",
    "        filter_parts.append(country.replace(\" \", \"-\"))\n",
    "    if state:\n",
    "        filter_parts.append(state.replace(\" \", \"-\"))\n",
    "    filter_str = \"_\".join(filter_parts)\n",
    "\n",
    "    output_files = []\n",
    "\n",
    "    # Process each input file\n",
    "    for nsrdb_file in nsrdb_files:\n",
    "        try:\n",
    "            print(f\"Processing {nsrdb_file}...\")\n",
    "\n",
    "            # Create output filename\n",
    "            base_name = Path(nsrdb_file).stem\n",
    "            output_file = f\"{base_name}_{filter_str}.h5\"\n",
    "            output_path = os.path.join(output_dir, output_file)\n",
    "\n",
    "            # Delete existing file if it exists to avoid conflicts\n",
    "            if os.path.exists(output_path):\n",
    "                os.remove(output_path)\n",
    "                print(f\"Removed existing file: {output_path}\")\n",
    "\n",
    "            with NSRDBX(nsrdb_file, hsds=True) as source:\n",
    "                print(f\"Extracting region: {region_value} from column: {region_col}\")\n",
    "\n",
    "                # Use save_region to extract and save the data in a single operation\n",
    "                source.save_region(output_path, region_value, region_col=region_col)\n",
    "\n",
    "                print(f\"Successfully created: {output_path}\")\n",
    "                output_files.append(output_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing {nsrdb_file}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(\"Terminating process.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "    print(f\"\\nSuccessfully created {len(output_files)} output files:\")\n",
    "    for output_file in output_files:\n",
    "        print(f\"  - {output_file}\")\n",
    "\n",
    "    return output_files\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Example: Extract data for Vietnam from multiple NSRDB files\n",
    "nsrdb_files = [\n",
    "    '/nrel/nsrdb/himawari/himawari7/himawari7_2011.h5',\n",
    "    '/nrel/nsrdb/himawari/himawari7/himawari7_2012.h5',\n",
    "    '/nrel/nsrdb/himawari/himawari7/himawari7_2013.h5',\n",
    "    '/nrel/nsrdb/himawari/himawari7/himawari7_2014.h5',\n",
    "    '/nrel/nsrdb/himawari/himawari7/himawari7_2015.h5',\n",
    "    '/nrel/nsrdb/himawari/himawari7/himawari7_2016.h5',\n",
    "    '/nrel/nsrdb/himawari/himawari7/himawari7_2017.h5',\n",
    "    '/nrel/nsrdb/himawari/himawari7/himawari7_2018.h5',\n",
    "    '/nrel/nsrdb/himawari/himawari7/himawari7_2019.h5',\n",
    "    '/nrel/nsrdb/himawari/himawari7/himawari7_2020.h5',\n",
    "]\n",
    "\n",
    "# Filter by country, state, county and save to new h5 files\n",
    "output_files = extract_and_save_nsrdb_data(\n",
    "    nsrdb_files,\n",
    "    output_dir=\"data/processed/\",\n",
    "    # country=\"Vietnam\",\n",
    "    state=\"Hồ Chí Minh\",\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
