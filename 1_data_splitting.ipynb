{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Solar Radiation Dataset\n",
    "\n",
    "This notebook preprocesses the NSRDB (National Solar Radiation Database) H5 data and implements two splitting strategies:\n",
    "1. Time-Based Split - Divides data chronologically\n",
    "2. Spatial Split - Divides data by geographic locations\n",
    "\n",
    "Both strategies allow for customization of train/validation/test ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Dict, List, Tuple, Union, Optional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up configuration parameters for data splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data file\n",
    "INPUT_FILE = \"data/NSRDB/vietnam_2016.h5\"\n",
    "\n",
    "# Output directory\n",
    "OUTPUT_DIR = \"data/processed\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Default split ratios\n",
    "DEFAULT_TRAIN_RATIO = 0.7\n",
    "DEFAULT_VAL_RATIO = 0.15\n",
    "DEFAULT_TEST_RATIO = 0.15\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Explore Data\n",
    "\n",
    "Load the H5 file and explore its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets in the file:\n",
      "/air_temperature (8784, 75361): int8\n",
      "/coordinates (75361, 2): float32\n",
      "/dhi (8784, 75361): int16\n",
      "/dni (8784, 75361): int16\n",
      "/ghi (8784, 75361): int16\n",
      "/meta (75361,): [('gid', '<i4'), ('latitude', '<f4'), ('longitude', '<f4'), ('country', 'S8'), ('timezone', '<i2'), ('elevation', '<f4')]\n",
      "/time_index (8784,): |S25\n",
      "/wind_speed (8784, 75361): int16\n"
     ]
    }
   ],
   "source": [
    "def load_h5_file(file_path: str) -> h5py.File:\n",
    "    \"\"\"Load an H5 file and return the file object.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the H5 file\n",
    "\n",
    "    Returns:\n",
    "        h5py.File: Loaded H5 file object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return h5py.File(file_path, 'r')\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file {file_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Load the H5 file\n",
    "h5_file = load_h5_file(INPUT_FILE)\n",
    "\n",
    "# List datasets in the file\n",
    "print(\"Datasets in the file:\")\n",
    "for key in h5_file:\n",
    "    dataset = h5_file[key]\n",
    "    print(f\"/{key} {dataset.shape}: {dataset.dtype}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 timestamps: [b'2016-01-01 00:00:00+00:00' b'2016-01-01 01:00:00+00:00'\n",
      " b'2016-01-01 02:00:00+00:00' b'2016-01-01 03:00:00+00:00'\n",
      " b'2016-01-01 04:00:00+00:00']\n",
      "Last 5 timestamps: [b'2016-12-31 19:00:00+00:00' b'2016-12-31 20:00:00+00:00'\n",
      " b'2016-12-31 21:00:00+00:00' b'2016-12-31 22:00:00+00:00'\n",
      " b'2016-12-31 23:00:00+00:00']\n",
      "Total time steps: 8784\n"
     ]
    }
   ],
   "source": [
    "# Examine time_index\n",
    "time_index = h5_file['time_index'][:]\n",
    "print(f\"First 5 timestamps: {time_index[:5]}\")\n",
    "print(f\"Last 5 timestamps: {time_index[-5:]}\")\n",
    "print(f\"Total time steps: {len(time_index)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample coordinates (first 5 rows):\n",
      "[[ 24.   100.  ]\n",
      " [ 23.95 100.  ]\n",
      " [ 23.9  100.  ]\n",
      " [ 23.85 100.  ]\n",
      " [ 23.8  100.  ]]\n",
      "Total locations: 75361\n"
     ]
    }
   ],
   "source": [
    "# Examine coordinates\n",
    "coordinates = h5_file['coordinates'][:]\n",
    "print(f\"Sample coordinates (first 5 rows):\\n{coordinates[:5]}\")\n",
    "print(f\"Total locations: {len(coordinates)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting Strategies\n",
    "\n",
    "Implement two different data splitting strategies:\n",
    "1. Time-Based Split\n",
    "2. Spatial Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_ratios(train_ratio: float, val_ratio: float, test_ratio: float) -> bool:\n",
    "    \"\"\"Validate that the ratios sum to approximately 1.0.\n",
    "\n",
    "    Args:\n",
    "        train_ratio: Ratio for training set\n",
    "        val_ratio: Ratio for validation set\n",
    "        test_ratio: Ratio for test set\n",
    "\n",
    "    Returns:\n",
    "        bool: True if ratios are valid\n",
    "    \"\"\"\n",
    "    total = train_ratio + val_ratio + test_ratio\n",
    "    if abs(total - 1.0) > 1e-10:\n",
    "        print(f\"Warning: Ratios sum to {total}, not 1.0. Adjusting ratios.\")\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Time-Based Split\n",
    "\n",
    "Split the data chronologically (by time index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_split(\n",
    "    h5_file: h5py.File,\n",
    "    train_ratio: float = DEFAULT_TRAIN_RATIO,\n",
    "    val_ratio: float = DEFAULT_VAL_RATIO,\n",
    "    test_ratio: float = DEFAULT_TEST_RATIO\n",
    ") -> Dict[str, Tuple[int, int]]:\n",
    "    \"\"\"Split data chronologically by time.\n",
    "\n",
    "    Args:\n",
    "        h5_file: H5 file object\n",
    "        train_ratio: Proportion of data for training\n",
    "        val_ratio: Proportion of data for validation\n",
    "        test_ratio: Proportion of data for testing\n",
    "\n",
    "    Returns:\n",
    "        Dict with dataset splits information\n",
    "    \"\"\"\n",
    "    # Validate ratios\n",
    "    if not validate_ratios(train_ratio, val_ratio, test_ratio):\n",
    "        # Normalize ratios\n",
    "        total = train_ratio + val_ratio + test_ratio\n",
    "        train_ratio /= total\n",
    "        val_ratio /= total\n",
    "        test_ratio /= total\n",
    "\n",
    "    num_timesteps = h5_file['time_index'].shape[0]\n",
    "\n",
    "    # Calculate split indices\n",
    "    train_end = int(num_timesteps * train_ratio)\n",
    "    val_end = train_end + int(num_timesteps * val_ratio)\n",
    "\n",
    "    # Define splits as (start_idx, end_idx) tuples\n",
    "    splits = {\n",
    "        'train': (0, train_end),\n",
    "        'val': (train_end, val_end),\n",
    "        'test': (val_end, num_timesteps)\n",
    "    }\n",
    "\n",
    "    # Print split information\n",
    "    print(f\"Time-based split:\")\n",
    "    print(f\"  Train: {splits['train'][0]} to {splits['train'][1]-1} ({splits['train'][1] - splits['train'][0]} samples)\")\n",
    "    print(f\"  Val:   {splits['val'][0]} to {splits['val'][1]-1} ({splits['val'][1] - splits['val'][0]} samples)\")\n",
    "    print(f\"  Test:  {splits['test'][0]} to {splits['test'][1]-1} ({splits['test'][1] - splits['test'][0]} samples)\")\n",
    "\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spatial Split\n",
    "\n",
    "Split the data by locations (geographically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_split(\n",
    "    h5_file: h5py.File,\n",
    "    train_ratio: float = DEFAULT_TRAIN_RATIO,\n",
    "    val_ratio: float = DEFAULT_VAL_RATIO,\n",
    "    test_ratio: float = DEFAULT_TEST_RATIO,\n",
    "    random_seed: int = 42\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Split data by geographic locations.\n",
    "\n",
    "    Args:\n",
    "        h5_file: H5 file object\n",
    "        train_ratio: Proportion of locations for training\n",
    "        val_ratio: Proportion of locations for validation\n",
    "        test_ratio: Proportion of locations for testing\n",
    "        random_seed: Random seed for reproducibility\n",
    "\n",
    "    Returns:\n",
    "        Dict with location indices for each split\n",
    "    \"\"\"\n",
    "    # Validate ratios\n",
    "    if not validate_ratios(train_ratio, val_ratio, test_ratio):\n",
    "        # Normalize ratios\n",
    "        total = train_ratio + val_ratio + test_ratio\n",
    "        train_ratio /= total\n",
    "        val_ratio /= total\n",
    "        test_ratio /= total\n",
    "\n",
    "    num_locations = h5_file['coordinates'].shape[0]\n",
    "\n",
    "    # Create shuffled indices for locations\n",
    "    np.random.seed(random_seed)\n",
    "    location_indices = np.arange(num_locations)\n",
    "    np.random.shuffle(location_indices)\n",
    "\n",
    "    # Calculate split sizes\n",
    "    train_size = int(num_locations * train_ratio)\n",
    "    val_size = int(num_locations * val_ratio)\n",
    "\n",
    "    # Split location indices\n",
    "    train_indices = location_indices[:train_size]\n",
    "    val_indices = location_indices[train_size:train_size + val_size]\n",
    "    test_indices = location_indices[train_size + val_size:]\n",
    "\n",
    "    # Sort indices for h5py compatibility (h5py requires indices to be in increasing order)\n",
    "    train_indices = np.sort(train_indices)\n",
    "    val_indices = np.sort(val_indices)\n",
    "    test_indices = np.sort(test_indices)\n",
    "\n",
    "    splits = {\n",
    "        'train': train_indices,\n",
    "        'val': val_indices,\n",
    "        'test': test_indices\n",
    "    }\n",
    "\n",
    "    # Print split information\n",
    "    print(f\"Spatial split:\")\n",
    "    print(f\"  Train: {len(train_indices)} locations ({len(train_indices)/num_locations:.2%})\")\n",
    "    print(f\"  Val:   {len(val_indices)} locations ({len(val_indices)/num_locations:.2%})\")\n",
    "    print(f\"  Test:  {len(test_indices)} locations ({len(test_indices)/num_locations:.2%})\")\n",
    "\n",
    "    return splits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Splits to H5 Files\n",
    "\n",
    "Functions to save the generated splits to separate H5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_time_based_split(\n",
    "    h5_file: h5py.File,\n",
    "    splits: Dict[str, Tuple[int, int]],\n",
    "    output_dir: str\n",
    ") -> None:\n",
    "    \"\"\"Save time-based splits to separate H5 files.\n",
    "\n",
    "    Args:\n",
    "        h5_file: Source H5 file\n",
    "        splits: Dictionary with time split information\n",
    "        output_dir: Directory to save output files\n",
    "    \"\"\"\n",
    "    for split_name, (start_idx, end_idx) in tqdm(splits.items(), desc=\"Saving time-based splits\"):\n",
    "        output_file = f\"{output_dir}/time_based_{split_name}.h5\"\n",
    "\n",
    "        with h5py.File(output_file, 'w') as out_file:\n",
    "            # Copy time-sliced data for each dataset\n",
    "            for key in h5_file:\n",
    "                dataset = h5_file[key]\n",
    "\n",
    "                # Handle time-indexed datasets vs. metadata\n",
    "                if key == 'time_index':\n",
    "                    # Copy time slice\n",
    "                    out_file.create_dataset(key, data=dataset[start_idx:end_idx])\n",
    "\n",
    "                elif len(dataset.shape) == 2 and dataset.shape[0] == len(h5_file['time_index']):\n",
    "                    # Time-indexed dataset (e.g., ghi, dni, air_temperature)\n",
    "                    out_file.create_dataset(key, data=dataset[start_idx:end_idx, :])\n",
    "\n",
    "                else:\n",
    "                    # Copy other datasets as-is (coordinates, meta)\n",
    "                    out_file.create_dataset(key, data=dataset[:])\n",
    "\n",
    "        print(f\"Saved {split_name} split to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_spatial_split(\n",
    "    h5_file: h5py.File,\n",
    "    splits: Dict[str, np.ndarray],\n",
    "    output_dir: str\n",
    ") -> None:\n",
    "    \"\"\"Save spatial splits to separate H5 files.\n",
    "\n",
    "    Args:\n",
    "        h5_file: Source H5 file\n",
    "        splits: Dictionary with location indices for each split\n",
    "        output_dir: Directory to save output files\n",
    "    \"\"\"\n",
    "    for split_name, location_indices in tqdm(splits.items(), desc=\"Saving spatial splits\"):\n",
    "        output_file = f\"{output_dir}/spatial_{split_name}.h5\"\n",
    "\n",
    "        with h5py.File(output_file, 'w') as out_file:\n",
    "            # Copy location-sliced data for each dataset\n",
    "            for key in h5_file:\n",
    "                dataset = h5_file[key]\n",
    "\n",
    "                # Handle different dataset types\n",
    "                if key == 'coordinates':\n",
    "                    # Location coordinates\n",
    "                    out_file.create_dataset(key, data=dataset[location_indices])\n",
    "\n",
    "                elif key == 'meta':\n",
    "                    # Location metadata\n",
    "                    out_file.create_dataset(key, data=dataset[location_indices])\n",
    "\n",
    "                elif key == 'time_index':\n",
    "                    # Copy time index as-is\n",
    "                    out_file.create_dataset(key, data=dataset[:])\n",
    "\n",
    "                elif len(dataset.shape) == 2 and dataset.shape[1] == len(h5_file['coordinates']):\n",
    "                    # Location-indexed data (e.g., ghi, dni, air_temperature)\n",
    "                    out_file.create_dataset(key, data=dataset[:, location_indices])\n",
    "\n",
    "        print(f\"Saved {split_name} split to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interface\n",
    "\n",
    "Allow users to choose splitting strategy and customize ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(\n",
    "    strategy: str = 'time',  # 'time' or 'spatial'\n",
    "    train_ratio: float = DEFAULT_TRAIN_RATIO,\n",
    "    val_ratio: float = DEFAULT_VAL_RATIO,\n",
    "    test_ratio: float = DEFAULT_TEST_RATIO,\n",
    "    random_seed: int = 42\n",
    ") -> None:\n",
    "    \"\"\"Process data using the specified splitting strategy.\n",
    "\n",
    "    Args:\n",
    "        strategy: Splitting strategy ('time' or 'spatial')\n",
    "        train_ratio: Proportion for training set\n",
    "        val_ratio: Proportion for validation set\n",
    "        test_ratio: Proportion for testing set\n",
    "        random_seed: Random seed for spatial split\n",
    "    \"\"\"\n",
    "    if strategy not in ['time', 'spatial']:\n",
    "        raise ValueError(f\"Invalid strategy: {strategy}. Must be 'time' or 'spatial'.\")\n",
    "\n",
    "    print(f\"Processing data using {strategy}-based split strategy\")\n",
    "    print(f\"Ratios - Train: {train_ratio:.2f}, Val: {val_ratio:.2f}, Test: {test_ratio:.2f}\")\n",
    "\n",
    "    # Load the H5 file\n",
    "    h5_file = load_h5_file(INPUT_FILE)\n",
    "\n",
    "    try:\n",
    "        if strategy == 'time':\n",
    "            # Time-based split\n",
    "            splits = time_based_split(h5_file, train_ratio, val_ratio, test_ratio)\n",
    "            save_time_based_split(h5_file, splits, OUTPUT_DIR)\n",
    "        else:\n",
    "            # Spatial split\n",
    "            splits = spatial_split(h5_file, train_ratio, val_ratio, test_ratio, random_seed)\n",
    "            save_spatial_split(h5_file, splits, OUTPUT_DIR)\n",
    "\n",
    "        print(f\"Data processing complete. Files saved to {OUTPUT_DIR}\")\n",
    "    finally:\n",
    "        h5_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Data Processing\n",
    "\n",
    "Execute data processing with desired settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data using time-based split strategy\n",
      "Ratios - Train: 0.70, Val: 0.15, Test: 0.15\n",
      "Time-based split:\n",
      "  Train: 0 to 6147 (6148 samples)\n",
      "  Val:   6148 to 7464 (1317 samples)\n",
      "  Test:  7465 to 8783 (1319 samples)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9bc34d2ffe64ce4ae91638976831258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving time-based splits:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train split to data/processed/time_based_train.h5\n",
      "Saved val split to data/processed/time_based_val.h5\n",
      "Saved test split to data/processed/time_based_test.h5\n",
      "Data processing complete. Files saved to data/processed\n"
     ]
    }
   ],
   "source": [
    "# Splitting 1: Time-based split with default ratios\n",
    "process_data(strategy='time')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data using spatial-based split strategy\n",
      "Ratios - Train: 0.70, Val: 0.15, Test: 0.15\n",
      "Spatial split:\n",
      "  Train: 52752 locations (70.00%)\n",
      "  Val:   11304 locations (15.00%)\n",
      "  Test:  11305 locations (15.00%)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f309704aab4e47c89cdd96e3828fa73a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving spatial splits:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Splitting 2: Spatial split with default ratios\n",
    "process_data(strategy='spatial')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Split Results\n",
    "\n",
    "This section lets you examine the generated split files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_split_file(file_path: str) -> None:\n",
    "    \"\"\"Check the contents of a split file.\n",
    "\n",
    "    Args:\n",
    "        file_path: Path to the split file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as f:\n",
    "            print(f\"\\nFile: {file_path}\")\n",
    "            for key in f:\n",
    "                dataset = f[key]\n",
    "                print(f\"  /{key} {dataset.shape}: {dataset.dtype}\")\n",
    "\n",
    "            # Show sample data\n",
    "            if 'time_index' in f:\n",
    "                print(f\"  Time range: {f['time_index'][0]} to {f['time_index'][-1]}\")\n",
    "\n",
    "            if 'coordinates' in f:\n",
    "                print(f\"  Number of locations: {len(f['coordinates'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking file {file_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all H5 files in the output directory\n",
    "output_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('.h5')]\n",
    "print(f\"Split files generated: {output_files}\")\n",
    "\n",
    "# Check each file\n",
    "for file_name in output_files:\n",
    "    check_split_file(os.path.join(OUTPUT_DIR, file_name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
