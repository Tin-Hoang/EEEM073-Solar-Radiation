{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e03ddec3",
   "metadata": {},
   "source": [
    "# GHI Forecasting using PyTorch Models\n",
    "\n",
    "This notebook implements deep learning models to forecast Global Horizontal Irradiance (GHI) based on various input features. The notebook is structured as follows:\n",
    "\n",
    "1. **Data Loading and Exploration**: Load the pre-split train/validation/test datasets and explore their structure\n",
    "2. **Data Preprocessing**: Prepare the data for model training (normalization, feature engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46423a36",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9baff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1c734e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data loading parameters\n",
    "\n",
    "# Find all data files\n",
    "train_files = [\n",
    "    \"data/raw/himawari7_2011_Hồ-Chí-Minh.h5\",\n",
    "    # \"data/raw/himawari7_2012_Hồ-Chí-Minh.h5\",\n",
    "    # \"data/raw/himawari7_2013_Hồ-Chí-Minh.h5\",\n",
    "    # \"data/raw/himawari7_2014_Hồ-Chí-Minh.h5\",\n",
    "    # \"data/raw/himawari7_2015_Hồ-Chí-Minh.h5\",\n",
    "    # \"data/raw/himawari7_2016_Hồ-Chí-Minh.h5\",\n",
    "    # \"data/raw/himawari7_2017_Hồ-Chí-Minh.h5\",\n",
    "    # \"data/raw/himawari7_2018_Hồ-Chí-Minh.h5\",\n",
    "]\n",
    "val_files = [\n",
    "    \"data/raw/himawari7_2019_Hồ-Chí-Minh.h5\",\n",
    "]\n",
    "test_files = [\n",
    "    \"data/raw/himawari7_2020_Hồ-Chí-Minh.h5\",\n",
    "]\n",
    "\n",
    "print(f\"Found {len(train_files)} training files: {[Path(f).name for f in train_files]}\")\n",
    "print(f\"Found {len(val_files)} validation files: {[Path(f).name for f in val_files]}\")\n",
    "print(f\"Found {len(test_files)} test files: {[Path(f).name for f in test_files]}\")\n",
    "\n",
    "# Check if the files exist\n",
    "if not (train_files and val_files and test_files):\n",
    "    print(\"Warning: Some data files are missing!\")\n",
    "\n",
    "# List of features to use\n",
    "AVAILABLE_FEATURES = [\n",
    "    'ghi',                     # Target variable\n",
    "    'air_temperature',         # Weather features\n",
    "    'wind_speed',\n",
    "    'relative_humidity',\n",
    "    'dew_point',\n",
    "    'surface_pressure',\n",
    "    'total_precipitable_water',\n",
    "    'cloud_type',              # Cloud features\n",
    "    'cloud_fill_flag',\n",
    "    'cld_opd_dcomp',\n",
    "    'cld_press_acha',\n",
    "    'cld_reff_dcomp',\n",
    "    'clearsky_ghi',            # Clear sky estimates\n",
    "    'clearsky_dni',\n",
    "    'clearsky_dhi',\n",
    "    'solar_zenith_angle',      # Solar geometry\n",
    "    'surface_albedo',          # Surface properties\n",
    "    'ozone',                   # Atmospheric properties\n",
    "    'aod',\n",
    "    'ssa',\n",
    "    'asymmetry',\n",
    "    'alpha'\n",
    "]\n",
    "\n",
    "# Choose features which could be potentially useful for the modeling\n",
    "# This is to reduce the number of features to be processed\n",
    "# by excluding the features which are unrelevant to the target variable\n",
    "# In the later modelling part, we will continue to select only subset of those features to input into the model\n",
    "SELECTED_FEATURES = [\n",
    "    'air_temperature',\n",
    "    'wind_speed',\n",
    "    'relative_humidity',\n",
    "    'cloud_type',\n",
    "    'cld_opd_dcomp',\n",
    "    'cld_press_acha',\n",
    "    'cld_reff_dcomp',\n",
    "    'solar_zenith_angle',\n",
    "    'clearsky_ghi',\n",
    "    'total_precipitable_water',\n",
    "    'surface_albedo',\n",
    "    'aod'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "TARGET_VARIABLE = 'ghi'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fdef07",
   "metadata": {},
   "source": [
    "### 1.1 Exploring the Data Structure\n",
    "\n",
    "Let's examine the structure of our H5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69372c2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def explore_h5_structure(h5_path):\n",
    "    try:\n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            print(f\"\\nExploring {h5_path}\")\n",
    "            print(\"\\nDatasets:\")\n",
    "            for key in f.keys():\n",
    "                dataset = f[key]\n",
    "                print(f\"  /{key} {dataset.shape}: {dataset.dtype}\")\n",
    "                if key == 'meta':\n",
    "                    # Explore metadata structure\n",
    "                    print(\"  Metadata fields:\")\n",
    "                    for field in dataset.dtype.names:\n",
    "                        values = dataset[field]\n",
    "                        unique_count = len(np.unique(values))\n",
    "                        sample = np.unique(values)[:5]\n",
    "                        print(f\"    {field}: {unique_count} unique values, examples: {sample}\")\n",
    "                elif len(dataset) > 0 and dataset.ndim > 0:\n",
    "                    # For numerical data, show some statistics\n",
    "                    try:\n",
    "                        if dataset.dtype in [np.float32, np.float64, np.int16, np.int32, np.uint8, np.uint16]:\n",
    "                            # Take a small sample to calculate stats\n",
    "                            sample = dataset[:10, :100] if dataset.ndim > 1 else dataset[:10]\n",
    "                            stats = {\n",
    "                                'min': np.min(sample),\n",
    "                                'max': np.max(sample),\n",
    "                                'mean': np.mean(sample),\n",
    "                                'has_nan': np.isnan(sample).any()\n",
    "                            }\n",
    "                            print(f\"    Sample stats: {stats}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"    Error computing stats: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring {h5_path}: {e}\")\n",
    "\n",
    "# Explore first file from each set\n",
    "if train_files:\n",
    "    explore_h5_structure(train_files[0])\n",
    "if val_files:\n",
    "    explore_h5_structure(val_files[0])\n",
    "if test_files:\n",
    "    explore_h5_structure(test_files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e958ac",
   "metadata": {},
   "source": [
    "### 1.2 Raw Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9800783",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.data_loading_utils import load_dataset\n",
    "\n",
    "# Load data with selected features\n",
    "print(\"Loading training data...\")\n",
    "train_data = load_dataset(train_files, n_sites=None, features=SELECTED_FEATURES, target_variable=\"ghi\", apply_scaling=True, time_interval=\"1h\")\n",
    "print(\"\\nLoading validation data...\")\n",
    "val_data = load_dataset(val_files, n_sites=None, features=SELECTED_FEATURES, target_variable=\"ghi\", apply_scaling=True, time_interval=\"1h\")\n",
    "print(\"\\nLoading test data...\")\n",
    "test_data = load_dataset(test_files, n_sites=None, features=SELECTED_FEATURES, target_variable=\"ghi\", apply_scaling=True, time_interval=\"1h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23bd6a",
   "metadata": {},
   "source": [
    "### 1.3 Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9670820b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_time_series\n",
    "\n",
    "# Plot data for a few locations\n",
    "fig = plot_time_series(train_data,\n",
    "                       features=SELECTED_FEATURES + ['nighttime_mask', \"ghi\"],\n",
    "                       location_idx=0,\n",
    "                       start_idx=0,\n",
    "                       n_steps=48)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11b624e",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20987a4c",
   "metadata": {},
   "source": [
    "### 2.1 Time Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a7962f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.normalize_utils import create_time_features\n",
    "\n",
    "# Create time features\n",
    "train_time_features_dict = create_time_features(train_data['timestamps'])\n",
    "val_time_features_dict = create_time_features(val_data['timestamps'])\n",
    "test_time_features_dict = create_time_features(test_data['timestamps'])\n",
    "\n",
    "# Add each time feature to the respective data dictionaries\n",
    "print(\"Time features:\")\n",
    "for key, value in train_time_features_dict.items():\n",
    "    train_data[key] = value\n",
    "    val_data[key] = val_time_features_dict[key]\n",
    "    test_data[key] = test_time_features_dict[key]\n",
    "    print(f\"  - {key}: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba4323",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_time_features\n",
    "\n",
    "_ = plot_time_features(train_data['timestamps'], train_time_features_dict, n_days=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27007c2e",
   "metadata": {},
   "source": [
    "### 2.2 Create nighttime mask feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b87e69",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.features_utils import compute_nighttime_mask\n",
    "\n",
    "# Compute nighttime mask using solar zenith angle if available\n",
    "train_nighttime_mask = compute_nighttime_mask(\n",
    "    train_data['timestamps'], train_data['latitude'], train_data['longitude'], train_data.get('solar_zenith_angle')\n",
    ")\n",
    "train_data['nighttime_mask'] = train_nighttime_mask\n",
    "# Do the same for validation\n",
    "val_nighttime_mask = compute_nighttime_mask(\n",
    "    val_data['timestamps'], val_data['latitude'], val_data['longitude'], val_data.get('solar_zenith_angle')\n",
    ")\n",
    "val_data['nighttime_mask'] = val_nighttime_mask\n",
    "# Do the same for test data\n",
    "test_nighttime_mask = compute_nighttime_mask(\n",
    "    test_data['timestamps'], test_data['latitude'], test_data['longitude'], test_data.get('solar_zenith_angle')\n",
    ")\n",
    "test_data['nighttime_mask'] = test_nighttime_mask\n",
    "print(f\"  Computed nighttime_mask with {np.sum(train_nighttime_mask)} night hours out of {train_nighttime_mask.size} total hours\")\n",
    "\n",
    "# Add nighttime mask to selected features if not already present\n",
    "if \"nighttime_mask\" not in SELECTED_FEATURES:\n",
    "    SELECTED_FEATURES += [\"nighttime_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e3329c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_solar_day_night\n",
    "\n",
    "_ = plot_solar_day_night(train_data, location_idx=0, n_steps=48, start_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ad38ac",
   "metadata": {},
   "source": [
    "### 2.3 Create Clear Sky model (if not provided)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d87010f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.features_utils import compute_clearsky_ghi\n",
    "\n",
    "# Use pre-computed clear sky GHI if available, otherwise compute it\n",
    "if 'clearsky_ghi' not in train_data:\n",
    "    clear_sky_ghi = compute_clearsky_ghi(\n",
    "        train_data['timestamps'], train_data['latitude'], train_data['longitude'], train_data.get('solar_zenith_angle')\n",
    "    )\n",
    "    train_data['clearsky_ghi'] = clear_sky_ghi\n",
    "\n",
    "if 'clearsky_ghi' not in val_data:\n",
    "    clear_sky_ghi = compute_clearsky_ghi(\n",
    "        val_data['timestamps'], val_data['latitude'], val_data['longitude'], val_data.get('solar_zenith_angle')\n",
    "    )\n",
    "    val_data['clearsky_ghi'] = clear_sky_ghi\n",
    "\n",
    "if 'clearsky_ghi' not in test_data:\n",
    "    clear_sky_ghi = compute_clearsky_ghi(\n",
    "        test_data['timestamps'], test_data['latitude'], test_data['longitude'], test_data.get('solar_zenith_angle')\n",
    "    )\n",
    "    test_data['clearsky_ghi'] = clear_sky_ghi\n",
    "\n",
    "if \"clearsky_ghi\" not in SELECTED_FEATURES:\n",
    "    SELECTED_FEATURES += [\"clearsky_ghi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3055850e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.plot_utils import plot_time_series\n",
    "\n",
    "# Plot data for a few locations\n",
    "fig = plot_time_series(train_data,\n",
    "                       features=[\"clearsky_ghi\", \"ghi\"],\n",
    "                       location_idx=0,\n",
    "                       start_idx=0,\n",
    "                       n_steps=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6cc1f",
   "metadata": {},
   "source": [
    "### 2.4 Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eacdee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.normalize_utils import normalize_data\n",
    "\n",
    "# Normalize data\n",
    "norm_train_data, scalers = normalize_data(train_data, SELECTED_FEATURES, TARGET_VARIABLE, scalers=None, fit_scalers=True)\n",
    "norm_val_data, _ = normalize_data(val_data, SELECTED_FEATURES, TARGET_VARIABLE, scalers=scalers, fit_scalers=False)\n",
    "norm_test_data, _ = normalize_data(test_data, SELECTED_FEATURES, TARGET_VARIABLE, scalers=scalers, fit_scalers=False)\n",
    "\n",
    "# Print normalized data shapes\n",
    "print(\"Normalized data shapes:\")\n",
    "for key in norm_train_data:\n",
    "    if isinstance(norm_train_data[key], np.ndarray):\n",
    "        print(f\"  {key}: {norm_train_data[key].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32007296",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.data_persistence import save_normalized_data, save_scalers\n",
    "\n",
    "# Create metadata\n",
    "train_metadata = {\n",
    "    \"selected_features\": SELECTED_FEATURES,\n",
    "    \"target_variable\": TARGET_VARIABLE,\n",
    "    \"selected_feature_org_shape\": train_data[SELECTED_FEATURES[0]].shape,\n",
    "    \"target_org_shape\": train_data[TARGET_VARIABLE].shape,\n",
    "    \"raw_files\": train_files,\n",
    "}\n",
    "val_metadata = {\n",
    "    \"selected_features\": SELECTED_FEATURES,\n",
    "    \"target_variable\": TARGET_VARIABLE,\n",
    "    \"selected_feature_org_shape\": val_data[SELECTED_FEATURES[0]].shape,\n",
    "    \"target_org_shape\": val_data[TARGET_VARIABLE].shape,\n",
    "    \"raw_files\": val_files,\n",
    "}\n",
    "test_metadata = {\n",
    "    \"selected_features\": SELECTED_FEATURES,\n",
    "    \"target_variable\": TARGET_VARIABLE,\n",
    "    \"selected_feature_org_shape\": test_data[SELECTED_FEATURES[0]].shape,\n",
    "    \"target_org_shape\": test_data[TARGET_VARIABLE].shape,\n",
    "    \"raw_files\": test_files,\n",
    "}\n",
    "\n",
    "# Save normalized data\n",
    "save_normalized_data(norm_train_data, output_dir=\"data/processed\", prefix=\"train\", add_timestamp=True)\n",
    "save_normalized_data(norm_val_data, output_dir=\"data/processed\", prefix=\"val\", add_timestamp=True)\n",
    "save_normalized_data(norm_test_data, output_dir=\"data/processed\", prefix=\"test\", add_timestamp=True)\n",
    "\n",
    "# Save scalers\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "save_scalers(scalers, filepath=f\"data/processed/scalers_{timestamp}.pkl\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
