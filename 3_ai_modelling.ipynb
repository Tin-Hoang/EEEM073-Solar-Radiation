{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create PyTorch Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import os\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "\n",
    "from utils.data_persistence import load_scalers\n",
    "from utils.training_utils import plot_training_history, plot_predictions\n",
    "from utils.wandb_utils import setup_wandb\n",
    "from utils.training_utils import train_model, evaluate_model\n",
    "from utils.wandb_utils import setup_wandb\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "# List of features to use\n",
    "AVAILABLE_FEATURES = [\n",
    "    'ghi',                     # Target variable\n",
    "    'air_temperature',         # Weather features\n",
    "    'wind_speed',\n",
    "    'relative_humidity',\n",
    "    'dew_point',\n",
    "    'surface_pressure',\n",
    "    'total_precipitable_water',\n",
    "    'cloud_type',              # Cloud features\n",
    "    'cloud_fill_flag',\n",
    "    'cld_opd_dcomp',\n",
    "    'cld_press_acha',\n",
    "    'cld_reff_dcomp',\n",
    "    'clearsky_ghi',            # Clear sky estimates\n",
    "    'clearsky_dni',\n",
    "    'clearsky_dhi',\n",
    "    'solar_zenith_angle',      # Solar geometry\n",
    "    'surface_albedo',          # Surface properties\n",
    "    'ozone',                   # Atmospheric properties\n",
    "    'aod',\n",
    "    'ssa',\n",
    "    'asymmetry',\n",
    "    'alpha'\n",
    "]\n",
    "\n",
    "# Choose features to use in modeling\n",
    "SELECTED_FEATURES = [\n",
    "    'air_temperature',\n",
    "    'wind_speed',\n",
    "    'relative_humidity',\n",
    "    'cloud_type',\n",
    "    'solar_zenith_angle',\n",
    "    'clearsky_ghi',\n",
    "    'total_precipitable_water',\n",
    "    'surface_albedo'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "TARGET_VARIABLE = 'ghi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded normalized data from data/processed/train_normalized.h5\n",
      "Loaded 12 scalers from data/processed/model_scalers.pkl\n",
      "Train set | Metadata: {'created_time': '2025-04-25 05:36:18'}\n",
      "Train set | Created time: 2025-04-25 05:36:18\n",
      "Train set | Raw files: No raw files\n",
      "Train set | Data structure:\n",
      "  air_temperature shape: (8760, 105)\n",
      "  clearsky_ghi shape: (8760, 105)\n",
      "  cloud_type shape: (8760, 105)\n",
      "  coordinates shape: (105, 2)\n",
      "  elevation shape: (105,)\n",
      "  ghi shape: (8760, 105)\n",
      "  nighttime_mask shape: (8760, 105)\n",
      "  relative_humidity shape: (8760, 105)\n",
      "  solar_zenith_angle shape: (8760, 105)\n",
      "  surface_albedo shape: (8760, 105)\n",
      "  time_features shape: (8760, 8)\n",
      "  total_precipitable_water shape: (8760, 105)\n",
      "  wind_speed shape: (8760, 105)\n"
     ]
    }
   ],
   "source": [
    "from utils.data_persistence import load_normalized_data\n",
    "\n",
    "train_preprocessed_data_path = \"data/processed/train_normalized.h5\"\n",
    "val_preprocessed_data_path = \"data/processed/val_normalized.h5\"\n",
    "test_preprocessed_data_path = \"data/processed/test_normalized.h5\"\n",
    "\n",
    "# Load sequences\n",
    "train_data, metadata = load_normalized_data(train_preprocessed_data_path)\n",
    "\n",
    "scaler_path = \"data/processed/model_scalers.pkl\"\n",
    "scalers = load_scalers(scaler_path)\n",
    "\n",
    "# Print metadata\n",
    "print(f\"Train set | Metadata: {metadata}\")\n",
    "# Print created time\n",
    "print(f\"Train set | Created time: {metadata['created_time'] if 'created_time' in metadata else 'No created time'}\")\n",
    "# Print raw files\n",
    "print(f\"Train set | Raw files: {metadata['raw_files'] if 'raw_files' in metadata else 'No raw files'}\")\n",
    "\n",
    "# Print data structure and shape\n",
    "print(f\"Train set | Data structure:\")\n",
    "for key, value in train_data.items():\n",
    "    print(f\"  {key} shape: {value.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded normalized data from data/processed/train_normalized.h5\n",
      "Loaded normalized data file (1/1): data/processed/train_normalized.h5\n",
      "Loaded data with 13 features\n",
      "Temporal features: ['time_features']\n",
      "Static features: ['coordinates', 'elevation']\n",
      "Time series features: ['air_temperature', 'clearsky_ghi', 'cloud_type', 'ghi', 'nighttime_mask', 'relative_humidity', 'solar_zenith_angle', 'surface_albedo', 'total_precipitable_water', 'wind_speed']\n",
      "Dataset dimensions: 8760 timesteps, 105 locations\n",
      "Dataset contains 917280 possible samples\n",
      "Loaded normalized data from data/processed/val_normalized.h5\n",
      "Loaded normalized data file (1/1): data/processed/val_normalized.h5\n",
      "Loaded data with 13 features\n",
      "Temporal features: ['time_features']\n",
      "Static features: ['coordinates', 'elevation']\n",
      "Time series features: ['air_temperature', 'clearsky_ghi', 'cloud_type', 'ghi', 'nighttime_mask', 'relative_humidity', 'solar_zenith_angle', 'surface_albedo', 'total_precipitable_water', 'wind_speed']\n",
      "Dataset dimensions: 8760 timesteps, 105 locations\n",
      "Dataset contains 917280 possible samples\n",
      "Loaded normalized data from data/processed/test_normalized.h5\n",
      "Loaded normalized data file (1/1): data/processed/test_normalized.h5\n",
      "Loaded data with 13 features\n",
      "Temporal features: ['time_features']\n",
      "Static features: ['coordinates', 'elevation']\n",
      "Time series features: ['air_temperature', 'clearsky_ghi', 'cloud_type', 'ghi', 'nighttime_mask', 'relative_humidity', 'solar_zenith_angle', 'surface_albedo', 'total_precipitable_water', 'wind_speed']\n",
      "Dataset dimensions: 8784 timesteps, 105 locations\n",
      "Dataset contains 919800 possible samples\n",
      "static_features shape: torch.Size([64, 2])\n",
      "target shape: torch.Size([64])\n",
      "temporal_features shape: torch.Size([64, 24, 8])\n",
      "air_temperature shape: torch.Size([64, 24])\n",
      "clearsky_ghi shape: torch.Size([64, 24])\n",
      "cloud_type shape: torch.Size([64, 24])\n",
      "ghi shape: torch.Size([64, 24])\n",
      "nighttime_mask shape: torch.Size([64, 24])\n",
      "relative_humidity shape: torch.Size([64, 24])\n",
      "solar_zenith_angle shape: torch.Size([64, 24])\n",
      "surface_albedo shape: torch.Size([64, 24])\n",
      "total_precipitable_water shape: torch.Size([64, 24])\n",
      "wind_speed shape: torch.Size([64, 24])\n"
     ]
    }
   ],
   "source": [
    "from utils.timeseriesdataset import TimeSeriesDataset\n",
    "\n",
    "LOOKBACK = 24\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TimeSeriesDataset(train_preprocessed_data_path, lookback=LOOKBACK)\n",
    "val_dataset = TimeSeriesDataset(val_preprocessed_data_path, lookback=LOOKBACK)\n",
    "test_dataset = TimeSeriesDataset(test_preprocessed_data_path, lookback=LOOKBACK)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Check sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"{key} shape: {value.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Input dimensions determined from batch:\n",
      "  - Temporal dimension: 8\n",
      "  - Static dimension: 2\n"
     ]
    }
   ],
   "source": [
    "# Get a batch to determine input dimensions\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Method 1: Extract dimensions from a batch (more reliable)\n",
    "temporal_features = batch['temporal_features']\n",
    "static_features = batch['static_features']\n",
    "\n",
    "# Check if we have 3D temporal features (batch, seq_len, features)\n",
    "if len(temporal_features.shape) == 3:\n",
    "    temporal_dim = temporal_features.shape[2]\n",
    "else:\n",
    "    # Handle 2D temporal features (batch, features)\n",
    "    temporal_dim = temporal_features.shape[1]\n",
    "\n",
    "static_dim = static_features.shape[1]\n",
    "\n",
    "print(f\"  Input dimensions determined from batch:\")\n",
    "print(f\"  - Temporal dimension: {temporal_dim}\")\n",
    "print(f\"  - Static dimension: {static_dim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(8, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (bn_lstm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (static_proj): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=160, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.lstm import LSTMModel\n",
    "\n",
    "# Create LSTM model\n",
    "lstm_model = LSTMModel(\n",
    "    input_dim=temporal_dim,\n",
    "    static_dim=static_dim,\n",
    "    hidden_dim=128,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(lstm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNLSTMModel(\n",
      "  (cnn): Sequential(\n",
      "    (0): Conv1d(8, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (lstm): LSTM(128, 128, num_layers=2, batch_first=True, dropout=0.3)\n",
      "  (bn_lstm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (static_proj): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=160, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.cnn_lstm import CNNLSTMModel\n",
    "\n",
    "# Create CNN-LSTM model\n",
    "cnn_lstm_model = CNNLSTMModel(\n",
    "    input_dim=temporal_dim,\n",
    "    static_dim=static_dim,\n",
    "    hidden_dim=128,\n",
    "    num_filters=64,\n",
    "    kernel_size=3,\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ").to(device)\n",
    "\n",
    "print(cnn_lstm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multi-Layer Perceptron (MLP) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPModel(\n",
      "  (mlp): Sequential(\n",
      "    (0): Linear(in_features=192, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=512, bias=True)\n",
      "    (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (9): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU()\n",
      "    (11): Dropout(p=0.3, inplace=False)\n",
      "    (12): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (13): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (14): ReLU()\n",
      "    (15): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (static_proj): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "    (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=160, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from models.mlp import MLPModel\n",
    "\n",
    "# Create MLP model\n",
    "mlp_model = MLPModel(\n",
    "    input_dim=temporal_dim,\n",
    "    static_dim=static_dim,\n",
    "    hidden_dims=[256, 512, 256, 128],\n",
    "    dropout=0.3,\n",
    "    lookback=LOOKBACK\n",
    ").to(device)\n",
    "\n",
    "print(mlp_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights & Biases tracking enabled with username 'tin-hoang' and project 'EEEM073-Solar-Radiation'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.training_utils import train_model, evaluate_model\n",
    "from utils.wandb_utils import setup_wandb\n",
    "\n",
    "# Default settings\n",
    "USE_WANDB = True\n",
    "WANDB_USERNAME = \"tin-hoang\"\n",
    "WANDB_PROJECT = \"EEEM073-Solar-Radiation\"\n",
    "setup_wandb(WANDB_USERNAME, WANDB_PROJECT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train and Evaluate LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtin-hoang\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/Surrey/AI_and_Sustainability/EEEM073-Solar-Radiation/wandb/run-20250425_054641-9o8gqy0n</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tin-hoang/EEEM073-Solar-Radiation/runs/9o8gqy0n' target=\"_blank\">LSTM-20250425-054640</a></strong> to <a href='https://wandb.ai/tin-hoang/EEEM073-Solar-Radiation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tin-hoang/EEEM073-Solar-Radiation' target=\"_blank\">https://wandb.ai/tin-hoang/EEEM073-Solar-Radiation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tin-hoang/EEEM073-Solar-Radiation/runs/9o8gqy0n' target=\"_blank\">https://wandb.ai/tin-hoang/EEEM073-Solar-Radiation/runs/9o8gqy0n</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training LSTM:   0%|                                                      | 0/30 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "print(\"Training LSTM model...\")\n",
    "lstm_history = train_model(\n",
    "    lstm_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model_name=\"LSTM\",\n",
    "    epochs=30,\n",
    "    patience=5,\n",
    "    lr=0.001\n",
    ")\n",
    "plot_training_history(lstm_history, model_name=\"LSTM\")\n",
    "\n",
    "print(\"Evaluating LSTM model on validation set...\")\n",
    "lstm_val_metrics = evaluate_model(\n",
    "    lstm_model,\n",
    "    val_loader,\n",
    "    scalers[f'{TARGET_VARIABLE}_scaler'],\n",
    "    model_name=\"LSTM - Validation\"\n",
    ")\n",
    "plot_predictions(lstm_val_metrics, model_name='LSTM - Validation')\n",
    "\n",
    "print(\"\\nEvaluating LSTM model on test set...\")\n",
    "lstm_test_metrics = evaluate_model(\n",
    "    lstm_model,\n",
    "    test_loader,\n",
    "    scalers[f'{TARGET_VARIABLE}_scaler'],\n",
    "    model_name=\"LSTM - Test\"\n",
    ")\n",
    "plot_predictions(lstm_test_metrics, model_name='LSTM - Test')\n",
    "\n",
    "torch.save(lstm_model.state_dict(), 'lstm_ghi_forecasting_model_v2.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train and Evaluate CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training CNN-LSTM model...\")\n",
    "cnn_lstm_history = train_model(\n",
    "    cnn_lstm_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model_name=\"CNN-LSTM\",\n",
    "    epochs=30,\n",
    "    patience=5,\n",
    "    lr=0.001\n",
    ")\n",
    "plot_training_history(cnn_lstm_history, model_name=\"CNN-LSTM\")\n",
    "\n",
    "print(\"Evaluating CNN-LSTM model on validation set...\")\n",
    "cnn_lstm_val_metrics = evaluate_model(\n",
    "    cnn_lstm_model,\n",
    "    val_loader,\n",
    "    scalers[f'{TARGET_VARIABLE}_scaler'],\n",
    "    model_name=\"CNN-LSTM - Validation\"\n",
    ")\n",
    "plot_predictions(cnn_lstm_val_metrics, model_name='CNN-LSTM - Validation')\n",
    "\n",
    "print(\"\\nEvaluating CNN-LSTM model on test set...\")\n",
    "cnn_lstm_test_metrics = evaluate_model(\n",
    "    cnn_lstm_model,\n",
    "    test_loader,\n",
    "    scalers[f'{TARGET_VARIABLE}_scaler'],\n",
    "    model_name=\"CNN-LSTM - Test\"\n",
    ")\n",
    "plot_predictions(cnn_lstm_test_metrics, model_name='CNN-LSTM - Test')\n",
    "\n",
    "torch.save(cnn_lstm_model.state_dict(), 'cnn_lstm_ghi_forecasting_model_v2.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train and Evaluate MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MLP model...\")\n",
    "mlp_history = train_model(\n",
    "    mlp_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    model_name=\"MLP\",\n",
    "    epochs=30,\n",
    "    patience=5,\n",
    "    lr=0.001\n",
    ")\n",
    "plot_training_history(mlp_history, model_name=\"MLP\")\n",
    "\n",
    "print(\"Evaluating MLP model on validation set...\")\n",
    "mlp_val_metrics = evaluate_model(\n",
    "    mlp_model,\n",
    "    val_loader,\n",
    "    scalers[f'{TARGET_VARIABLE}_scaler'],\n",
    "    model_name=\"MLP - Validation\"\n",
    ")\n",
    "plot_predictions(mlp_val_metrics, model_name='MLP - Validation')\n",
    "\n",
    "print(\"\\nEvaluating MLP model on test set...\")\n",
    "mlp_test_metrics = evaluate_model(\n",
    "    mlp_model,\n",
    "    test_loader,\n",
    "    scalers[f'{TARGET_VARIABLE}_scaler'],\n",
    "    model_name=\"MLP - Test\"\n",
    ")\n",
    "plot_predictions(mlp_test_metrics, model_name='MLP - Test')\n",
    "\n",
    "torch.save(mlp_model.state_dict(), 'mlp_ghi_forecasting_model_v2.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(metrics_list, model_names, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Compare performance metrics across models\n",
    "\n",
    "    Args:\n",
    "        metrics_list: List of metrics dictionaries\n",
    "        model_names: List of model names\n",
    "        dataset_name: Name of the dataset (train/val/test)\n",
    "    \"\"\"\n",
    "    metrics = ['rmse', 'mae', 'r2', 'day_rmse', 'day_mae', 'day_r2', 'night_rmse', 'night_mae']\n",
    "    metric_labels = ['RMSE', 'MAE', 'R²', 'Day RMSE', 'Day MAE', 'Day R²', 'Night RMSE', 'Night MAE']\n",
    "\n",
    "    comparison = pd.DataFrame(index=metric_labels, columns=model_names)\n",
    "\n",
    "    for i, model_metrics in enumerate(metrics_list):\n",
    "        for j, metric in enumerate(metrics):\n",
    "            if metric in model_metrics:\n",
    "                comparison.iloc[j, i] = model_metrics[metric]\n",
    "            else:\n",
    "                comparison.iloc[j, i] = np.nan\n",
    "\n",
    "    print(f\"\\nModel Comparison - {dataset_name} Set:\")\n",
    "    print(comparison)\n",
    "\n",
    "    # Create comparison visualization\n",
    "    fig_comparison = create_comparison_plots(metrics_list, model_names, dataset_name)\n",
    "    plt.show()\n",
    "\n",
    "    # Log comparison to wandb\n",
    "    if USE_WANDB:\n",
    "        # Create a comparison table\n",
    "        comparison_table = wandb.Table(\n",
    "            columns=[\"Metric\"] + model_names,\n",
    "            data=[[metric_labels[i]] + [comparison.iloc[i, j] for j in range(len(model_names))]\n",
    "                  for i in range(len(metric_labels))]\n",
    "        )\n",
    "\n",
    "        # Log to wandb\n",
    "        wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            entity=WANDB_USERNAME,\n",
    "            name=f\"Model-Comparison-{dataset_name}\",\n",
    "            config={\"dataset\": dataset_name}\n",
    "        )\n",
    "\n",
    "        wandb.log({\n",
    "            f\"comparison_table_{dataset_name}\": comparison_table,\n",
    "            f\"comparison_plot_{dataset_name}\": wandb.Image(fig_comparison)\n",
    "        })\n",
    "\n",
    "        # Create individual metric plots for clearer visualization\n",
    "        for j, metric in enumerate(metrics[:6]):  # Skip night metrics which might be NaN\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            values = [metrics_dict[metric] if not np.isnan(metrics_dict[metric]) else 0 for metrics_dict in metrics_list]\n",
    "            bars = plt.bar(model_names, values)\n",
    "\n",
    "            # Add values on top of bars\n",
    "            for bar, value in zip(bars, values):\n",
    "                plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                        f\"{value:.3f}\" if metric.startswith('r2') else f\"{value:.2f}\",\n",
    "                        ha='center', va='bottom')\n",
    "\n",
    "            plt.title(f'{metric_labels[j]} Comparison - {dataset_name} Set')\n",
    "            plt.ylabel(metric_labels[j])\n",
    "            plt.grid(axis='y')\n",
    "            plt.tight_layout()\n",
    "\n",
    "            # Log to wandb\n",
    "            wandb.log({f\"{metric}_{dataset_name}_comparison\": wandb.Image(plt)})\n",
    "            plt.close()\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "    return comparison\n",
    "\n",
    "def create_comparison_plots(metrics_list, model_names, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Create comparison visualizations for multiple models\n",
    "\n",
    "    Args:\n",
    "        metrics_list: List of metrics dictionaries\n",
    "        model_names: List of model names\n",
    "        dataset_name: Name of the dataset (train/val/test)\n",
    "\n",
    "    Returns:\n",
    "        fig: Matplotlib figure\n",
    "    \"\"\"\n",
    "    # Create grouped bar charts for key metrics\n",
    "    # RMSE and MAE for overall, day, night\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    # Overall metrics\n",
    "    x = np.arange(len(model_names))\n",
    "    width = 0.35\n",
    "\n",
    "    # Plot RMSE\n",
    "    rmse_values = [metrics_dict['rmse'] for metrics_dict in metrics_list]\n",
    "    bars1 = axes[0].bar(x - width/2, rmse_values, width, label='RMSE')\n",
    "    # Plot MAE\n",
    "    mae_values = [metrics_dict['mae'] for metrics_dict in metrics_list]\n",
    "    bars2 = axes[0].bar(x + width/2, mae_values, width, label='MAE')\n",
    "\n",
    "    axes[0].set_title('Overall Error Metrics')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(model_names)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(axis='y')\n",
    "\n",
    "    # Add data labels\n",
    "    for i, v in enumerate(rmse_values):\n",
    "        axes[0].text(i - width/2, v + 0.1, f\"{v:.1f}\", ha='center')\n",
    "    for i, v in enumerate(mae_values):\n",
    "        axes[0].text(i + width/2, v + 0.1, f\"{v:.1f}\", ha='center')\n",
    "\n",
    "    # Daytime metrics\n",
    "    day_rmse_values = [metrics_dict['day_rmse'] for metrics_dict in metrics_list]\n",
    "    bars3 = axes[1].bar(x - width/2, day_rmse_values, width, label='Day RMSE')\n",
    "    day_mae_values = [metrics_dict['day_mae'] for metrics_dict in metrics_list]\n",
    "    bars4 = axes[1].bar(x + width/2, day_mae_values, width, label='Day MAE')\n",
    "\n",
    "    axes[1].set_title('Daytime Error Metrics')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(model_names)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(axis='y')\n",
    "\n",
    "    # Add data labels\n",
    "    for i, v in enumerate(day_rmse_values):\n",
    "        axes[1].text(i - width/2, v + 0.1, f\"{v:.1f}\", ha='center')\n",
    "    for i, v in enumerate(day_mae_values):\n",
    "        axes[1].text(i + width/2, v + 0.1, f\"{v:.1f}\", ha='center')\n",
    "\n",
    "    # Nighttime metrics\n",
    "    night_rmse_values = [metrics_dict['night_rmse'] for metrics_dict in metrics_list]\n",
    "    bars5 = axes[2].bar(x - width/2, night_rmse_values, width, label='Night RMSE')\n",
    "    night_mae_values = [metrics_dict['night_mae'] for metrics_dict in metrics_list]\n",
    "    bars6 = axes[2].bar(x + width/2, night_mae_values, width, label='Night MAE')\n",
    "\n",
    "    axes[2].set_title('Nighttime Error Metrics')\n",
    "    axes[2].set_xticks(x)\n",
    "    axes[2].set_xticklabels(model_names)\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(axis='y')\n",
    "\n",
    "    # Add data labels\n",
    "    for i, v in enumerate(night_rmse_values):\n",
    "        if not np.isnan(v):\n",
    "            axes[2].text(i - width/2, v + 0.1, f\"{v:.1f}\", ha='center')\n",
    "    for i, v in enumerate(night_mae_values):\n",
    "        if not np.isnan(v):\n",
    "            axes[2].text(i + width/2, v + 0.1, f\"{v:.1f}\", ha='center')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f'Model Comparison - {dataset_name} Set', fontsize=16)\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Compare model performance on validation set\n",
    "print(\"Validation Set Comparison:\")\n",
    "compare_models(\n",
    "    [lstm_val_metrics, cnn_lstm_val_metrics, mlp_val_metrics],\n",
    "    ['LSTM', 'CNN-LSTM', 'MLP', 'PINN-MLP'],\n",
    "    dataset_name=\"Validation\"\n",
    ")\n",
    "\n",
    "# Compare model performance on test set\n",
    "print(\"\\nTest Set Comparison:\")\n",
    "compare_models(\n",
    "    [lstm_test_metrics, cnn_lstm_test_metrics, mlp_test_metrics],\n",
    "    ['LSTM', 'CNN-LSTM', 'MLP', 'PINN-MLP'],\n",
    "    dataset_name=\"Test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Time Series Predictions\n",
    "\n",
    "Visualize predictions over time, including the PINN-MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_over_time(models, model_names, data_loader, target_scaler, num_samples=200, start_idx=0):\n",
    "    \"\"\"\n",
    "    Plot time series predictions for multiple models\n",
    "\n",
    "    Args:\n",
    "        models: List of PyTorch models\n",
    "        model_names: List of model names\n",
    "        data_loader: Data loader\n",
    "        target_scaler: Scaler for the target variable\n",
    "        num_samples: Number of consecutive time steps to plot\n",
    "        start_idx: Starting index in the dataset\n",
    "    \"\"\"\n",
    "    # Collect data samples\n",
    "    all_batches = []\n",
    "    for batch in data_loader:\n",
    "        all_batches.append(batch)\n",
    "        if len(all_batches) * batch['target'].shape[0] > start_idx + num_samples:\n",
    "            break\n",
    "\n",
    "    # Combine batches into a single dataset\n",
    "    all_temporal = []\n",
    "    all_static = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in all_batches:\n",
    "        all_temporal.append(batch['temporal_features'])\n",
    "        all_static.append(batch['static_features'])\n",
    "        all_targets.append(batch['target'])\n",
    "\n",
    "    all_temporal = torch.cat(all_temporal, dim=0)\n",
    "    all_static = torch.cat(all_static, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    # Get the subset for visualization\n",
    "    temporal = all_temporal[start_idx:start_idx+num_samples].to(device)\n",
    "    static = all_static[start_idx:start_idx+num_samples].to(device)\n",
    "    targets = all_targets[start_idx:start_idx+num_samples].cpu().numpy()\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(temporal, static).cpu().numpy()\n",
    "            predictions.append(outputs)\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_true_orig = target_scaler.inverse_transform(targets)\n",
    "    y_pred_orig_list = [target_scaler.inverse_transform(pred) for pred in predictions]\n",
    "\n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "\n",
    "    # Plot predictions\n",
    "    plt.plot(y_true_orig, 'k-', label='Actual GHI', linewidth=2)\n",
    "\n",
    "    colors = ['b-', 'r-', 'g-', 'm-']\n",
    "    for i, (pred, name) in enumerate(zip(y_pred_orig_list, model_names)):\n",
    "        plt.plot(pred, colors[i], label=f'{name} Predicted', alpha=0.7)\n",
    "\n",
    "    plt.title('GHI Predictions Over Time')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('GHI (W/m²)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Log the plot to wandb\n",
    "    if USE_WANDB:\n",
    "        wandb.init(\n",
    "            project=WANDB_PROJECT,\n",
    "            entity=WANDB_USERNAME,\n",
    "            name=\"Time-Series-Predictions\",\n",
    "            config={\"num_samples\": num_samples}\n",
    "        )\n",
    "\n",
    "        # Log the matplotlib figure\n",
    "        wandb.log({\"time_series_predictions\": wandb.Image(fig)})\n",
    "\n",
    "        # Create an interactive line chart\n",
    "        time_steps = list(range(num_samples))\n",
    "        data = [[step] + [float(y_true_orig[step][0])] + [float(pred[step][0]) for pred in y_pred_orig_list]\n",
    "                for step in range(num_samples)]\n",
    "\n",
    "        columns = [\"Time Step\", \"Actual\"] + model_names\n",
    "        table = wandb.Table(columns=columns, data=data)\n",
    "\n",
    "        wandb.log({\"predictions_table\": table})\n",
    "\n",
    "        # Log prediction error over time\n",
    "        error_data = []\n",
    "        for step in range(num_samples):\n",
    "            actual = float(y_true_orig[step][0])\n",
    "            errors = [abs(float(pred[step][0]) - actual) for pred in y_pred_orig_list]\n",
    "            error_data.append([step, actual] + errors)\n",
    "\n",
    "        error_columns = [\"Time Step\", \"Actual\"] + [f\"{name}_error\" for name in model_names]\n",
    "        error_table = wandb.Table(columns=error_columns, data=error_data)\n",
    "\n",
    "        wandb.log({\"prediction_errors\": error_table})\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Plot time series predictions\n",
    "plot_predictions_over_time(\n",
    "    models=[lstm_model, cnn_lstm_model, mlp_model],\n",
    "    model_names=['LSTM', 'CNN-LSTM', 'MLP', 'PINN-MLP'],\n",
    "    data_loader=test_loader,\n",
    "    target_scaler=scalers[f'{TARGET_VARIABLE}_scaler'],\n",
    "    num_samples=200,\n",
    "    start_idx=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This notebook implemented four deep learning models for GHI forecasting:\n",
    "\n",
    "1. **LSTM Model**: Captures long-term temporal dependencies in the enhanced meteorological dataset.\n",
    "2. **CNN-LSTM Model**: Combines local pattern extraction via CNNs with temporal modeling via LSTMs.\n",
    "3. **MLP Model**: Processes flattened time series with many more features than the original model.\n",
    "4. **PINN-MLP Model**: Enhances MLP with physics-informed constraints to enforce zero GHI at night.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- The models effectively handle the expanded feature set from the new dataset.\n",
    "- Physics-informed constraints improve model performance, especially during nighttime.\n",
    "- Performance varies across models, with trade-offs in complexity and accuracy.\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "1. Incorporate more domain-specific features:\n",
    "   - Cloud types and coverage\n",
    "   - Aerosol optical depth\n",
    "   - Solar zenith angle\n",
    "\n",
    "2. Experiment with additional physical constraints:\n",
    "   - Maximum GHI constraints based on clear sky models\n",
    "   - More sophisticated atmospheric transmission models\n",
    "\n",
    "3. Explore additional model architectures:\n",
    "   - Transformer-based models for capturing long-range dependencies\n",
    "   - Graph neural networks for spatial correlations\n",
    "   - Ensemble methods combining multiple model types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models\n",
    "def save_models_with_metadata(models, model_names, metrics_list, log_to_wandb=True):\n",
    "    \"\"\"\n",
    "    Save trained models with performance metadata\n",
    "\n",
    "    Args:\n",
    "        models: List of PyTorch models\n",
    "        model_names: List of model names\n",
    "        metrics_list: List of metrics dictionaries for each model\n",
    "        log_to_wandb: Whether to log models as wandb artifacts\n",
    "    \"\"\"\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    save_dir = f\"models_{timestamp}\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Save each model with its metadata\n",
    "    for model, name, metrics in zip(models, model_names, metrics_list):\n",
    "        # Create unique model filename\n",
    "        model_filename = f\"{name.lower().replace('-', '_')}_model.pt\"\n",
    "        model_path = os.path.join(save_dir, model_filename)\n",
    "\n",
    "        # Create metadata for the model\n",
    "        metadata = {\n",
    "            \"model_name\": name,\n",
    "            \"saved_date\": timestamp,\n",
    "            \"performance\": {\n",
    "                \"rmse\": float(metrics['rmse']),\n",
    "                \"mae\": float(metrics['mae']),\n",
    "                \"r2\": float(metrics['r2']),\n",
    "                \"day_rmse\": float(metrics['day_rmse']),\n",
    "                \"day_mae\": float(metrics['day_mae']),\n",
    "                \"day_r2\": float(metrics['day_r2'])\n",
    "            },\n",
    "            \"training_config\": {\n",
    "                \"lookback\": LOOKBACK,\n",
    "                \"features\": SELECTED_FEATURES,\n",
    "                \"device\": str(device)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save the model\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'metadata': metadata\n",
    "        }, model_path)\n",
    "\n",
    "        # Create metadata JSON file\n",
    "        metadata_path = os.path.join(save_dir, f\"{name.lower().replace('-', '_')}_metadata.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            import json\n",
    "            json.dump(metadata, f, indent=2)\n",
    "\n",
    "        print(f\"Model {name} saved to {model_path}\")\n",
    "\n",
    "        # Log as wandb artifact if enabled\n",
    "        if USE_WANDB and log_to_wandb:\n",
    "            wandb.init(\n",
    "                project=WANDB_PROJECT,\n",
    "                entity=WANDB_USERNAME,\n",
    "                name=f\"{name}-Model-Save\",\n",
    "                config=metadata\n",
    "            )\n",
    "\n",
    "            # Create artifact\n",
    "            artifact = wandb.Artifact(\n",
    "                name=f\"{name.lower().replace('-', '_')}_model\",\n",
    "                type=\"model\",\n",
    "                description=f\"Trained {name} model for GHI forecasting\"\n",
    "            )\n",
    "\n",
    "            # Add model file to artifact\n",
    "            artifact.add_file(model_path)\n",
    "\n",
    "            # Add metadata file to artifact\n",
    "            artifact.add_file(metadata_path)\n",
    "\n",
    "            # Log artifact\n",
    "            wandb.log_artifact(artifact)\n",
    "\n",
    "            # Finish wandb run\n",
    "            wandb.finish()\n",
    "\n",
    "    print(f\"All models saved successfully to {save_dir}\")\n",
    "    return save_dir\n",
    "\n",
    "# Save all models with their performance metrics\n",
    "models = [lstm_model, cnn_lstm_model, mlp_model]\n",
    "model_names = ['LSTM', 'CNN-LSTM', 'MLP']\n",
    "metrics_list = [lstm_test_metrics, cnn_lstm_test_metrics, mlp_test_metrics]\n",
    "\n",
    "saved_models_dir = save_models_with_metadata(models, model_names, metrics_list)\n",
    "print(f\"Models saved to {saved_models_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ending Weights & Biases Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all wandb runs are properly closed\n",
    "if 'wandb' in globals() and wandb.run is not None:\n",
    "    wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
