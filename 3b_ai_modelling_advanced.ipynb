{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f07bf13",
   "metadata": {},
   "source": [
    "# 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd1f8f",
   "metadata": {},
   "source": [
    "### 1.1 Create PyTorch Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bb7359",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Load autoreload extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload to mode 1\n",
    "%autoreload 2\n",
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import wandb\n",
    "from datetime import datetime # Import datetime for timestamp\n",
    "\n",
    "# Local modules\n",
    "from utils.data_persistence import load_scalers\n",
    "from utils.plot_utils import plot_training_history, plot_evaluation_metrics\n",
    "from utils.training_utils import train_model, evaluate_model\n",
    "from utils.wandb_utils import is_wandb_enabled, set_wandb_flag, set_keep_run_open\n",
    "from utils.model_utils import print_model_info, save_model, load_model\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# ========== Model training hyperparameters =========\n",
    "PATIENCE = 10\n",
    "LR = 0.0001\n",
    "# Debug mode to test code. Set to False for actual training\n",
    "DEBUG_MODE = True\n",
    "\n",
    "if DEBUG_MODE:\n",
    "    # Local debug settings (to check if the code is working)\n",
    "    # Will only run 10 batches/epoch for 10 epochs\n",
    "    N_EPOCHS = 10\n",
    "    BATCH_SIZE = 64\n",
    "    NUM_WORKERS = 4\n",
    "else:\n",
    "    # Remote server settings (to train the model, recommend using Otter lab machine)\n",
    "    N_EPOCHS = 30\n",
    "    BATCH_SIZE = 2 ** 13   # = 8192 samples\n",
    "    NUM_WORKERS = 16\n",
    "\n",
    "# ================= Wandb settings =============\n",
    "USE_WANDB = False\n",
    "WANDB_USERNAME = \"tin-hoang\"\n",
    "WANDB_PROJECT = \"EEEM073-Solar-Radiation\"\n",
    "\n",
    "# =========== Time series hyperparameters ===========\n",
    "# Number of timesteps to look back when creating sequences\n",
    "LOOKBACK = 24\n",
    "\n",
    "# Choose features to use in modeling\n",
    "TIME_KEY = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "            'month_sin', 'month_cos', 'dow_sin', 'dow_cos']\n",
    "SELECTED_FEATURES = [\n",
    "    'air_temperature',\n",
    "    'wind_speed',\n",
    "    'relative_humidity',\n",
    "    'cloud_type',\n",
    "    'solar_zenith_angle',\n",
    "    'clearsky_ghi',\n",
    "    'total_precipitable_water',\n",
    "    'surface_albedo',\n",
    "    'nighttime_mask',  # New field from preprocess_data\n",
    "    'cld_opd_dcomp',\n",
    "    'aod'\n",
    "]\n",
    "\n",
    "# Target variable\n",
    "TARGET_VARIABLE = 'ghi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b853c263",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.data_persistence import load_normalized_data\n",
    "\n",
    "TRAIN_PREPROCESSED_DATA_PATH = \"data/processed/train_normalized_20250430_050932.h5\"\n",
    "VAL_PREPROCESSED_DATA_PATH = \"data/processed/val_normalized_20250430_050940.h5\"\n",
    "TEST_PREPROCESSED_DATA_PATH = \"data/processed/test_normalized_20250430_050941.h5\"\n",
    "\n",
    "# Load sequences\n",
    "train_data, metadata = load_normalized_data(TRAIN_PREPROCESSED_DATA_PATH)\n",
    "\n",
    "SCALER_PATH = \"data/processed/scalers_20250430_050941.pkl\"\n",
    "scalers = load_scalers(SCALER_PATH)\n",
    "\n",
    "# Print metadata\n",
    "print(f\"Train set | Metadata: {metadata}\")\n",
    "# Print created time\n",
    "print(f\"Train set | Created time: {metadata['created_time'] if 'created_time' in metadata else 'No created time'}\")\n",
    "# Print raw files\n",
    "print(f\"Train set | Raw files: {metadata['raw_files'] if 'raw_files' in metadata else 'No raw files'}\")\n",
    "\n",
    "# Print data structure and shape\n",
    "print(f\"Train set | Data structure:\")\n",
    "for key, value in train_data.items():\n",
    "    print(f\"  {key} shape: {value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888600e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.timeseriesdataset import TimeSeriesDataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TimeSeriesDataset(TRAIN_PREPROCESSED_DATA_PATH, lookback=LOOKBACK, target_field=TARGET_VARIABLE, selected_features=SELECTED_FEATURES, include_target_history=False)\n",
    "val_dataset = TimeSeriesDataset(VAL_PREPROCESSED_DATA_PATH, lookback=LOOKBACK, target_field=TARGET_VARIABLE, selected_features=SELECTED_FEATURES, include_target_history=False)\n",
    "test_dataset = TimeSeriesDataset(TEST_PREPROCESSED_DATA_PATH, lookback=LOOKBACK, target_field=TARGET_VARIABLE, selected_features=SELECTED_FEATURES, include_target_history=False)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c6ff7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Get a batch to determine input dimensions\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Check sample batch\n",
    "sample_batch = next(iter(train_loader))\n",
    "for key, value in sample_batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"{key} shape: {value.shape}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"{key} length: {len(value)}\")\n",
    "\n",
    "# Extract dimensions from a batch (more reliable)\n",
    "temporal_features = batch['temporal_features']\n",
    "static_features = batch['static_features']\n",
    "TEMPORAL_FEATURES_SHAPE = list(temporal_features.shape)\n",
    "STATIC_FEATURES_SHAPE = list(static_features.shape)\n",
    "\n",
    "# Check if we have 3D temporal features (batch, seq_len, features)\n",
    "if len(temporal_features.shape) == 3:\n",
    "    temporal_dim = temporal_features.shape[2]\n",
    "else:\n",
    "    # Handle 2D temporal features (batch, features)\n",
    "    temporal_dim = temporal_features.shape[1]\n",
    "\n",
    "static_dim = static_features.shape[1]\n",
    "\n",
    "print(f\"  Input dimensions determined from batch:\")\n",
    "print(f\"  - Batch temporal_features shape: {TEMPORAL_FEATURES_SHAPE}\")\n",
    "print(f\"  - Batch static_features shape: {STATIC_FEATURES_SHAPE}\")\n",
    "print(f\"  - Temporal dimension: {temporal_dim}\")\n",
    "print(f\"  - Static dimension: {static_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfde4692",
   "metadata": {},
   "source": [
    "## 2. Model Training Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1a861",
   "metadata": {},
   "source": [
    "## 2.1 Import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc1267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "from utils.training_utils import train_model, evaluate_model\n",
    "from utils.wandb_utils import is_wandb_enabled, set_wandb_flag, set_keep_run_open\n",
    "from utils.model_utils import print_model_info\n",
    "\n",
    "# Enable wandb tracking\n",
    "set_wandb_flag(USE_WANDB)\n",
    "# Keep the wandb run open after training to continue logging evaluation plots\n",
    "set_keep_run_open(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daed848",
   "metadata": {},
   "source": [
    "## 2.2 Define Experiment Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b367ee1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_experiment_pipeline(model, train_loader, val_loader, test_loader, model_name, epochs=30, patience=5, lr=0.001):\n",
    "    \"\"\"\n",
    "    Run the experiment pipeline for a given model.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        train_loader: The training data loader.\n",
    "        val_loader: The validation data loader.\n",
    "        test_loader: The test data loader.\n",
    "        model_name: The name of the model.\n",
    "        epochs: The number of epochs to train the model.\n",
    "        patience: The number of epochs to wait before early stopping.\n",
    "        lr: The learning rate for the model.\n",
    "    \"\"\"\n",
    "    history, val_metrics, test_metrics = None, None, None\n",
    "\n",
    "    # Get the current config\n",
    "    CONFIG = {}\n",
    "    cur_globals = globals().copy()\n",
    "    for x in cur_globals:\n",
    "        # Only get the variables that are uppercase and not digits\n",
    "        if x.upper() == x and not x.startswith('_') and not x == \"CONFIG\":\n",
    "            CONFIG[x] = cur_globals[x]\n",
    "\n",
    "    try:\n",
    "        print(f\"Training {model_name} model...\")\n",
    "        history = train_model(\n",
    "            model,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            model_name=model_name,\n",
    "            epochs=epochs,\n",
    "            patience=patience,\n",
    "            lr=lr,\n",
    "            target_scaler=scalers[f'{TARGET_VARIABLE}_scaler'],\n",
    "            config=CONFIG,\n",
    "            debug_mode=DEBUG_MODE,\n",
    "        )\n",
    "        training_plot = plot_training_history(history, model_name=model_name)\n",
    "\n",
    "        print(f\"\\nEvaluating {model_name} model on test set...\")\n",
    "        test_metrics = evaluate_model(\n",
    "            model,\n",
    "            test_loader,\n",
    "            scalers[f'{TARGET_VARIABLE}_scaler'],\n",
    "            model_name=f\"{model_name} - Test\"\n",
    "        )\n",
    "        test_plot = plot_evaluation_metrics(test_metrics, model_name=f\"{model_name} - Test\")\n",
    "\n",
    "        # ========== Save Best Model Checkpoint ===========\n",
    "        checkpoint_dir = \"checkpoints\"\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        # Generate timestamp\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        # Construct filename with timestamp and directory\n",
    "        model_filename = f\"{model_name}_best_{timestamp}.pt\"\n",
    "        model_path = os.path.join(checkpoint_dir, model_filename)\n",
    "\n",
    "        # Save the model with metadata using the new save_model function\n",
    "        save_model(\n",
    "            model=model,\n",
    "            filepath=model_path,\n",
    "            metadata={\n",
    "                \"model_name\": model_name,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"train_metrics\": {\n",
    "                    \"final_train_loss\": history[\"train_loss\"][-1] if history and \"train_loss\" in history else None,\n",
    "                    \"final_train_mae\": history[\"train_mae\"][-1] if history and \"train_mae\" in history else None,\n",
    "                    \"final_val_loss\": history[\"val_loss\"][-1] if history and \"val_loss\" in history else None,\n",
    "                    \"final_val_mae\": history[\"val_mae\"][-1] if history and \"val_mae\" in history else None,\n",
    "                },\n",
    "                \"test_metrics\": {\n",
    "                    \"mse\": test_metrics[\"mse\"] if test_metrics else None,\n",
    "                    \"rmse\": test_metrics[\"rmse\"] if test_metrics else None,\n",
    "                    \"mae\": test_metrics[\"mae\"] if test_metrics else None,\n",
    "                    \"r2\": test_metrics[\"r2\"] if test_metrics else None,\n",
    "                }\n",
    "            },\n",
    "            temporal_features=SELECTED_FEATURES,\n",
    "            static_features=['coordinates', 'elevation'],\n",
    "            target_field=TARGET_VARIABLE,\n",
    "            time_feature_keys=TIME_KEY,\n",
    "            config=CONFIG\n",
    "        )\n",
    "\n",
    "        print(f\"Best model saved to {model_path}\")\n",
    "\n",
    "        # Log saved model path to wandb if enabled\n",
    "        if is_wandb_enabled():\n",
    "            wandb.save(model_path)\n",
    "            print(f\"Saved model checkpoint logged to wandb: {model_path}\")\n",
    "            wandb.log({\"plots/history_plot\": wandb.Image(training_plot)})\n",
    "            wandb.log({\"plots/predictions_plot\": wandb.Image(test_plot)})\n",
    "\n",
    "    finally:\n",
    "        # Finish wandb run if it's still open\n",
    "        if is_wandb_enabled():\n",
    "            wandb.finish()\n",
    "\n",
    "        # Clear GPU memory\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return history, val_metrics, test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e7f52",
   "metadata": {},
   "source": [
    "# 3. Model Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8f8d3",
   "metadata": {},
   "source": [
    "### 3.1 Temporal Convolutional Networks (TCN) Model\n",
    "\n",
    "TCNs are specialized convolutional architectures for sequence modeling that combine the best of CNNs and RNNs. The key features include:\n",
    "\n",
    "- **Causal Convolutions**: Each output only depends on current and past inputs.\n",
    "- **Dilated Convolutions**: Captures larger effective history with fewer parameters.\n",
    "- **Residual Connections**: Helps with training deep networks and information flow.\n",
    "\n",
    "TCNs can capture long-range patterns in time series data efficiently, making them suitable for solar radiation forecasting where both short-term weather fluctuations and longer-term patterns matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb23e0d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from models.tcn import TCNModel\n",
    "\n",
    "# Create TCN model\n",
    "tcn_model = TCNModel(\n",
    "    input_dim=temporal_dim,\n",
    "    static_dim=static_dim,\n",
    "    num_channels=[64, 128, 128, 64],  # Number of channels in each layer\n",
    "    kernel_size=3,                    # Size of the convolutional kernel\n",
    "    dropout=0.2,                      # Dropout rate\n",
    ").to(device)\n",
    "\n",
    "# Print the model\n",
    "print_model_info(tcn_model, temporal_features.shape, static_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba4cae0",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model_name = \"TCN\"\n",
    "\n",
    "# Train the TCN model\n",
    "tcn_history, tcn_val_metrics, tcn_test_metrics = run_experiment_pipeline(\n",
    "    tcn_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    model_name=model_name,\n",
    "    epochs=N_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    lr=LR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5457ae",
   "metadata": {},
   "source": [
    "### 3.2 Transformer Model\n",
    "\n",
    "Transformers revolutionized natural language processing and have been adapted for time series forecasting with impressive results. Their key advantages include:\n",
    "\n",
    "- **Self-Attention Mechanism**: Allows the model to weight the importance of different input time steps dynamically.\n",
    "- **Parallelization**: Can process the entire sequence in parallel, unlike RNNs.\n",
    "- **Long-range Dependencies**: Captures dependencies at arbitrary distances in the sequence.\n",
    "\n",
    "For solar forecasting, Transformers can identify complex temporal patterns across different time scales and account for both short-term and long-term relationships in the weather and solar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3ff309",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from models.transformer import TransformerModel\n",
    "\n",
    "# Create Transformer model\n",
    "transformer_model = TransformerModel(\n",
    "    input_dim=temporal_dim,           # Dimension of input features\n",
    "    static_dim=static_dim,            # Dimension of static features\n",
    "    d_model=128,                      # Transformer model dimension\n",
    "    nhead=8,                          # Number of attention heads\n",
    "    num_layers=4,                     # Number of transformer layers\n",
    "    dim_feedforward=512,              # Dimension of feedforward network\n",
    "    dropout=0.2,                      # Dropout rate\n",
    ").to(device)\n",
    "\n",
    "# Print the model\n",
    "print_model_info(transformer_model, temporal_features.shape, static_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f8fef2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model_name = \"Transformer\"\n",
    "\n",
    "# Train the Transformer model\n",
    "transformer_history, transformer_val_metrics, transformer_test_metrics = run_experiment_pipeline(\n",
    "    transformer_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    model_name=model_name,\n",
    "    epochs=N_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    lr=LR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2aa67",
   "metadata": {},
   "source": [
    "### 3.3 Informer Model\n",
    "\n",
    "The Informer model is a recent advancement in time series forecasting that addresses the limitations of standard Transformer models for long sequence prediction. Key innovations include:\n",
    "\n",
    "- **ProbSparse Self-attention**: Reduces complexity from O(L²) to O(L log L) where L is sequence length.\n",
    "- **Self-attention Distilling**: Progressive downsampling of hidden states along the encoder.\n",
    "- **Generative Decoder**: Enables long sequence prediction with minimal compute.\n",
    "\n",
    "For solar radiation forecasting, Informer can efficiently capture daily, weekly, and seasonal patterns while focusing computational resources on the most informative timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea3cc5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from models.informer import InformerModel\n",
    "\n",
    "# Create Informer model\n",
    "informer_model = InformerModel(\n",
    "    input_dim=temporal_dim,           # Dimension of input features\n",
    "    static_dim=static_dim,            # Dimension of static features\n",
    "    d_model=128,                      # Model dimension\n",
    "    n_heads=8,                        # Number of attention heads\n",
    "    e_layers=3,                       # Number of encoder layers\n",
    "    d_ff=256,                         # Dimension of feedforward network\n",
    "    dropout=0.1,                      # Dropout rate\n",
    "    activation='gelu'                 # Activation function\n",
    ").to(device)\n",
    "\n",
    "# Print the model\n",
    "print_model_info(informer_model, temporal_features.shape, static_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea04c4",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "model_name = \"Informer\"\n",
    "\n",
    "# Train the Informer model with a lower learning rate\n",
    "informer_history, informer_val_metrics, informer_test_metrics = run_experiment_pipeline(\n",
    "    informer_model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    model_name=model_name,\n",
    "    epochs=N_EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    lr=LR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1514852c",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2562309f",
   "metadata": {},
   "source": [
    "## 4.1 Compare Models' Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4de6fe",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from utils.plot_utils import compare_models\n",
    "\n",
    "# Create a dictionary of model metrics\n",
    "model_metrics = {\n",
    "    'TCN': tcn_test_metrics,\n",
    "    'Transformer': transformer_test_metrics,\n",
    "    'Informer': informer_test_metrics\n",
    "}\n",
    "\n",
    "# Compare model performance on test set\n",
    "print(\"\\nTest Set Comparison:\")\n",
    "compare_models(model_metrics, dataset_name='Test Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59e1677",
   "metadata": {},
   "source": [
    "## 4.2 Model Comparison on Daytime/Nighttime/Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca5ed33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plot_utils import compare_models_daytime_nighttime\n",
    "\n",
    "# Create a dictionary of model metrics\n",
    "model_metrics = {\n",
    "    'TCN': tcn_test_metrics,\n",
    "    'Transformer': transformer_test_metrics,\n",
    "    'Informer': informer_test_metrics\n",
    "}\n",
    "\n",
    "# Generate the comparison plot\n",
    "comparison_fig = compare_models_daytime_nighttime(model_metrics, dataset_name='Test Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73d535e",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da9018c",
   "metadata": {},
   "source": [
    "### 5.2 Time Series Predictions\n",
    "\n",
    "Visualize predictions over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bbd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_over_time(models, model_names, data_loader, target_scaler, num_samples=200, start_idx=0):\n",
    "    \"\"\"\n",
    "    Plot time series predictions for multiple models with nighttime shading if available\n",
    "\n",
    "    Args:\n",
    "        models: List of PyTorch models\n",
    "        model_names: List of model names\n",
    "        data_loader: Data loader\n",
    "        target_scaler: Scaler for the target variable\n",
    "        num_samples: Number of consecutive time steps to plot\n",
    "        start_idx: Starting index in the dataset\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import os\n",
    "    from matplotlib.patches import Patch\n",
    "    import matplotlib.dates as mdates\n",
    "    from datetime import datetime\n",
    "\n",
    "    # Get device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Collect data samples\n",
    "    all_batches = []\n",
    "    for batch in data_loader:\n",
    "        all_batches.append(batch)\n",
    "        if len(all_batches) * batch['target'].shape[0] > start_idx + num_samples:\n",
    "            break\n",
    "\n",
    "    # Combine batches into a single dataset\n",
    "    all_temporal = []\n",
    "    all_static = []\n",
    "    all_targets = []\n",
    "    all_nighttime = []\n",
    "    all_time_index_local = []\n",
    "    has_nighttime = False\n",
    "    has_time_index_local = False\n",
    "\n",
    "    for batch in all_batches:\n",
    "        all_temporal.append(batch['temporal_features'])\n",
    "        all_static.append(batch['static_features'])\n",
    "        all_targets.append(batch['target'])\n",
    "        # Check if nighttime data is available\n",
    "        if 'nighttime_mask' in batch:\n",
    "            has_nighttime = True\n",
    "            all_nighttime.append(batch['nighttime_mask'])\n",
    "        # Check if time_index_local is available\n",
    "        if 'time_index_local' in batch:\n",
    "            has_time_index_local = True\n",
    "            # Store the time index values as they are\n",
    "            if isinstance(batch['time_index_local'], list):\n",
    "                all_time_index_local.extend(batch['time_index_local'])\n",
    "            else:\n",
    "                all_time_index_local.append(batch['time_index_local'])\n",
    "\n",
    "    all_temporal = torch.cat(all_temporal, dim=0)\n",
    "    all_static = torch.cat(all_static, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    if has_nighttime:\n",
    "        all_nighttime = torch.cat(all_nighttime, dim=0)\n",
    "\n",
    "    # Get the subset for visualization\n",
    "    temporal = all_temporal[start_idx:start_idx+num_samples].to(device)\n",
    "    static = all_static[start_idx:start_idx+num_samples].to(device)\n",
    "    targets = all_targets[start_idx:start_idx+num_samples].cpu().numpy()\n",
    "\n",
    "    if has_nighttime:\n",
    "        nighttime = all_nighttime[start_idx:start_idx+num_samples].cpu().numpy()\n",
    "        # Ensure nighttime is a 1D array\n",
    "        if len(nighttime.shape) > 1:\n",
    "            nighttime = nighttime.flatten() if nighttime.shape[1] == 1 else nighttime[:,0]\n",
    "\n",
    "    # Get time index for x-axis if available\n",
    "    x_values = None\n",
    "    if has_time_index_local and len(all_time_index_local) >= start_idx + num_samples:\n",
    "        # Extract the time values for the plotting window\n",
    "        x_values = all_time_index_local[start_idx:start_idx+num_samples]\n",
    "\n",
    "        # Try to convert to datetime objects if they are strings\n",
    "        if isinstance(x_values[0], str):\n",
    "            try:\n",
    "                # Try different datetime formats\n",
    "                date_formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%Y-%m-%dT%H:%M:%S', '%Y%m%d%H%M%S']\n",
    "                for date_format in date_formats:\n",
    "                    try:\n",
    "                        x_values = [datetime.strptime(t, date_format) for t in x_values]\n",
    "                        print(f\"Successfully parsed dates with format: {date_format}\")\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "                # If we couldn't parse with any format, notify and use indices\n",
    "                if isinstance(x_values[0], str):\n",
    "                    print(f\"Could not parse date format: {x_values[0]}, using indices instead\")\n",
    "                    x_values = None\n",
    "\n",
    "            except (ValueError, TypeError) as e:\n",
    "                # If conversion fails, fall back to using indices\n",
    "                print(f\"Error converting time_index_local to datetime: {e}, using indices instead\")\n",
    "                x_values = None\n",
    "\n",
    "    # Generate predictions\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(temporal, static).cpu().numpy()\n",
    "            predictions.append(outputs)\n",
    "\n",
    "    # Inverse transform to original scale\n",
    "    y_true_orig = target_scaler.inverse_transform(targets.reshape(-1, 1)).flatten()\n",
    "    y_pred_orig_list = [target_scaler.inverse_transform(pred.reshape(-1, 1)).flatten() for pred in predictions]\n",
    "\n",
    "    # Create visualization\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    # Define colors and line styles for predictions\n",
    "    colors = ['blue', 'red', 'green', 'magenta', 'cyan', 'orange']\n",
    "    line_styles = ['--', ':', '-.', '--', ':', '--']\n",
    "\n",
    "    # Set x-axis values based on availability of time_index_local\n",
    "    if x_values:\n",
    "        # Plot actual values with time index\n",
    "        actual_line, = plt.plot(x_values, y_true_orig, 'k-', label='Actual GHI', linewidth=2)\n",
    "\n",
    "        # Plot predictions with time index\n",
    "        pred_lines = []\n",
    "        handles = [actual_line]\n",
    "        labels = ['Actual GHI']\n",
    "\n",
    "        for i, (pred, name) in enumerate(zip(y_pred_orig_list, model_names)):\n",
    "            color = colors[i % len(colors)]\n",
    "            style = line_styles[i % len(line_styles)]\n",
    "            line, = plt.plot(x_values, pred, color=color, linestyle=style, label=f'{name} Predicted', alpha=0.7)\n",
    "            pred_lines.append(line)\n",
    "            handles.append(line)\n",
    "            labels.append(f'{name} Predicted')\n",
    "\n",
    "        # Format the x-axis to show dates properly\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "        plt.xticks(rotation=0)  # Make labels horizontal\n",
    "        fig.subplots_adjust(bottom=0.15)  # Adjust bottom margin for horizontal labels\n",
    "\n",
    "        # If we have nighttime data, shade those regions\n",
    "        if has_nighttime:\n",
    "            # Modify nighttime shading to work with datetime x-axis\n",
    "            nighttime_bool = (nighttime > 0.5)\n",
    "            night_regions = []\n",
    "            start = None\n",
    "            for i, is_night in enumerate(nighttime_bool):\n",
    "                if is_night and start is None:\n",
    "                    start = i\n",
    "                elif not is_night and start is not None:\n",
    "                    night_regions.append((start, i))\n",
    "                    start = None\n",
    "            if start is not None:\n",
    "                night_regions.append((start, len(nighttime_bool)))\n",
    "\n",
    "            for start, end in night_regions:\n",
    "                if start < len(x_values) and end <= len(x_values):\n",
    "                    ax.axvspan(x_values[start], x_values[min(end, len(x_values)-1)],\n",
    "                              alpha=0.2, color='gray', label='_nolegend_')\n",
    "    else:\n",
    "        # Use default integer indices for x-axis\n",
    "        actual_line, = plt.plot(y_true_orig, 'k-', label='Actual GHI', linewidth=2)\n",
    "\n",
    "        # Plot predictions and collect handles/labels\n",
    "        pred_lines = []\n",
    "        handles = [actual_line]\n",
    "        labels = ['Actual GHI']\n",
    "\n",
    "        for i, (pred, name) in enumerate(zip(y_pred_orig_list, model_names)):\n",
    "            color = colors[i % len(colors)]\n",
    "            style = line_styles[i % len(line_styles)]\n",
    "            line, = plt.plot(pred, color=color, linestyle=style, label=f'{name} Predicted', alpha=0.7)\n",
    "            pred_lines.append(line)\n",
    "            handles.append(line)\n",
    "            labels.append(f'{name} Predicted')\n",
    "\n",
    "        # If we have nighttime data, shade those regions\n",
    "        if has_nighttime:\n",
    "            nighttime_bool = (nighttime > 0.5)\n",
    "            night_regions = []\n",
    "            start = None\n",
    "            for i, is_night in enumerate(nighttime_bool):\n",
    "                if is_night and start is None:\n",
    "                    start = i\n",
    "                elif not is_night and start is not None:\n",
    "                    night_regions.append((start, i))\n",
    "                    start = None\n",
    "            if start is not None:\n",
    "                night_regions.append((start, len(nighttime_bool)))\n",
    "\n",
    "            for start, end in night_regions:\n",
    "                ax.axvspan(start, end, alpha=0.2, color='gray', label='_nolegend_')\n",
    "\n",
    "    # Add nighttime legend if applicable\n",
    "    if has_nighttime and len(night_regions) > 0:\n",
    "        night_patch = Patch(facecolor='gray', alpha=0.2, label='Nighttime')\n",
    "        handles.append(night_patch)\n",
    "        labels.append('Nighttime')\n",
    "\n",
    "    # Calculate and display error metrics for the visualization window\n",
    "    for i, (pred, name) in enumerate(zip(y_pred_orig_list, model_names)):\n",
    "        rmse = np.sqrt(np.mean((y_true_orig - pred) ** 2))\n",
    "        mae = np.mean(np.abs(y_true_orig - pred))\n",
    "        plt.annotate(f\"{name}: RMSE={rmse:.2f}, MAE={mae:.2f}\",\n",
    "                     xy=(0.02, 0.97 - 0.03*i),\n",
    "                     xycoords='axes fraction',\n",
    "                     fontsize=9,\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", alpha=0.8))\n",
    "\n",
    "    plt.title('GHI Predictions Over Time')\n",
    "    plt.xlabel('Time' if x_values else 'Time Step')\n",
    "    plt.ylabel('GHI (W/m²)')\n",
    "\n",
    "    # Set the legend with the correct handles and labels\n",
    "    plt.legend(handles, labels, loc='upper right')\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # Save the figure\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plt.savefig(f'plots/predictions_over_time_{timestamp}.png')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb51776",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Plot time series predictions\n",
    "plot_predictions_over_time(\n",
    "    models=[tcn_model, transformer_model, informer_model],\n",
    "    model_names=['TCN', 'Transformer', 'Informer'],\n",
    "    data_loader=test_loader,\n",
    "    target_scaler=scalers[f'{TARGET_VARIABLE}_scaler'],\n",
    "    num_samples=72,\n",
    "    start_idx=40\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
