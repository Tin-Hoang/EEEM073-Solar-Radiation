{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ddff781",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Model Efficiency Experiment (example on: Transformer)\n",
    "\n",
    "This notebook implements various model efficiency techniques for the Transformer model used in solar radiation forecasting:\n",
    "\n",
    "1. **Quantization**: Reducing model precision to decrease size and improve inference speed\n",
    "2. **Structured Pruning**: Removing less important components to reduce parameters\n",
    "3. **Knowledge Distillation**: Training a smaller model to mimic the larger model's behavior\n",
    "\n",
    "These techniques help make models more energy-efficient and computationally efficient, which is crucial for sustainability in AI applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ae05d",
   "metadata": {},
   "source": [
    "## Debug Mode\n",
    "\n",
    "**IMPORTANT**: Set to True for code debugging mode and False for actual training.\n",
    "In debug mode, the code will only run 10 batches/epoch for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c6947c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug mode to test code. Set to False for actual training\n",
    "DEBUG_MODE = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c1e089",
   "metadata": {},
   "source": [
    "## 0. Setup and Load Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6371016b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load autoreload extension\n",
    "%load_ext autoreload\n",
    "# Set autoreload to mode 2\n",
    "%autoreload 2\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import wandb\n",
    "\n",
    "# Import project utilities\n",
    "from utils.model_utils import load_model, save_model, print_model_info\n",
    "from utils.data_persistence import load_scalers\n",
    "from utils.plot_utils import plot_predictions_over_time\n",
    "from utils.timeseriesdataset import TimeSeriesDataset\n",
    "from utils.training_utils import evaluate_model\n",
    "from utils.wandb_utils import track_experiment, is_wandb_enabled\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Model efficiency experiment configuration\n",
    "\n",
    "# Data settings\n",
    "TRAIN_PREPROCESSED_DATA_PATH = \"data/processed/train_normalized_20250430_145157.h5\"\n",
    "VAL_PREPROCESSED_DATA_PATH = \"data/processed/val_normalized_20250430_145205.h5\"\n",
    "TEST_PREPROCESSED_DATA_PATH = \"data/processed/test_normalized_20250430_145205.h5\"\n",
    "SCALER_PATH = \"data/processed/scalers_20250430_145206.pkl\"\n",
    "# Choose the model checkpoint from the previous experiment\n",
    "# You can choose other model checkpoints as well, here we pick Transformer as an example\n",
    "PRETRAINED_MODEL_PATH = \"checkpoints/Transformer_best_20250506_213328.pt\"\n",
    "\n",
    "# Dataset settings\n",
    "LOOKBACK = 24\n",
    "TARGET_VARIABLE = \"ghi\"\n",
    "SELECTED_FEATURES = [\n",
    "    'air_temperature', 'wind_speed', 'relative_humidity', 'cloud_type',\n",
    "    'solar_zenith_angle', 'clearsky_ghi', 'total_precipitable_water',\n",
    "    'surface_albedo', 'nighttime_mask', 'cld_opd_dcomp', 'aod'\n",
    "]\n",
    "STATIC_FEATURES = ['latitude', 'longitude', 'elevation']\n",
    "TIME_FEATURES = ['hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "            'month_sin', 'month_cos', 'dow_sin', 'dow_cos']\n",
    "if DEBUG_MODE:\n",
    "    BATCH_SIZE = 2**10\n",
    "    NUM_WORKERS = 4\n",
    "else:\n",
    "    BATCH_SIZE = 2**13\n",
    "    NUM_WORKERS = 16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4ba277",
   "metadata": {},
   "source": [
    "## Helper Functions for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c202a772",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_model_size(model):\n",
    "    \"\"\"Get model size in MB.\"\"\"\n",
    "    torch_model_size = 0\n",
    "    for param in model.parameters():\n",
    "        torch_model_size += param.nelement() * param.element_size()\n",
    "    torch_model_size_mb = torch_model_size / (1024 * 1024)\n",
    "    return torch_model_size_mb\n",
    "\n",
    "def print_model_report(model_name, model, test_loader, scalers, device=device):\n",
    "    \"\"\"Print a comprehensive report about the model.\"\"\"\n",
    "    model_size = get_model_size(model)\n",
    "\n",
    "    # Use the project's evaluate_model function instead of custom evaluation logic\n",
    "    eval_metrics = evaluate_model(\n",
    "        model=model,\n",
    "        data_loader=test_loader,\n",
    "        target_scaler=scalers.get('ghi_scaler', None),\n",
    "        model_name=model_name,\n",
    "        log_to_wandb=False,\n",
    "        device=device,\n",
    "        debug_mode=DEBUG_MODE\n",
    "    )\n",
    "    # Get inference time from the evaluation metrics\n",
    "    inference_time = eval_metrics['total_inference_time']\n",
    "\n",
    "    # Extract test loss (MSE) from the evaluation metrics\n",
    "    test_loss = eval_metrics['mse']\n",
    "\n",
    "    print(f\"=== {model_name} ===\")\n",
    "    print(f\"Model Size: {model_size:.2f} MB\")\n",
    "    print(f\"Inference Time: {inference_time*1000:.2f} ms\")\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    if 'mae' in eval_metrics:\n",
    "        print(f\"Test MAE: {eval_metrics['mae']:.4f}\")\n",
    "    if 'r2' in eval_metrics:\n",
    "        print(f\"Test RÂ²: {eval_metrics['r2']:.4f}\")\n",
    "    print()\n",
    "\n",
    "    return {\n",
    "        'name': model_name,\n",
    "        'size': model_size,\n",
    "        'inference_time': inference_time,\n",
    "        'test_loss': test_loss,\n",
    "        'metrics': eval_metrics\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529fc258",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13808d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scalers\n",
    "from utils.timeseriesdataset import TimeSeriesDataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TimeSeriesDataset(TRAIN_PREPROCESSED_DATA_PATH, lookback=LOOKBACK, target_field=TARGET_VARIABLE,\n",
    "                                 selected_features=SELECTED_FEATURES, include_target_history=False,\n",
    "                                 static_features=STATIC_FEATURES)\n",
    "val_dataset = TimeSeriesDataset(VAL_PREPROCESSED_DATA_PATH, lookback=LOOKBACK, target_field=TARGET_VARIABLE,\n",
    "                               selected_features=SELECTED_FEATURES, include_target_history=False,\n",
    "                               static_features=STATIC_FEATURES)\n",
    "test_dataset = TimeSeriesDataset(TEST_PREPROCESSED_DATA_PATH, lookback=LOOKBACK, target_field=TARGET_VARIABLE,\n",
    "                                selected_features=SELECTED_FEATURES, include_target_history=False,\n",
    "                                static_features=STATIC_FEATURES)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "scalers = load_scalers(SCALER_PATH)\n",
    "\n",
    "# Get sample input for inference time measurement\n",
    "sample_batch = next(iter(test_loader))\n",
    "sample_temporal = sample_batch['temporal_features'][0:1].to(device)\n",
    "sample_static = sample_batch['static_features'][0:1].to(device)\n",
    "sample_input = (sample_temporal, sample_static)\n",
    "\n",
    "print(\"Data loading successful.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04768d7d",
   "metadata": {},
   "source": [
    "## Load Original Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821a85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"Loading model from {PRETRAINED_MODEL_PATH}\")\n",
    "original_model, metadata = load_model(PRETRAINED_MODEL_PATH, device=device)\n",
    "base_checkpoint_name = os.path.splitext(os.path.basename(PRETRAINED_MODEL_PATH))[0]\n",
    "model_name = metadata['model_name']\n",
    "# Print model information\n",
    "print_model_info(original_model,\n",
    "                temporal_shape=sample_input[0].shape,\n",
    "                static_shape=sample_input[1].shape)\n",
    "\n",
    "# Warm up the model to avoid first batch overhead\n",
    "print(\"Warming up model...\")\n",
    "n_warmup_batches = 10\n",
    "original_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(n_warmup_batches):\n",
    "        batch = next(iter(test_loader))\n",
    "        temporal_features = batch['temporal_features'].to(device)\n",
    "        static_features = batch['static_features'].to(device)\n",
    "        original_model(temporal_features, static_features)\n",
    "print(\"Model warmed up.\")\n",
    "\n",
    "# Evaluate original model\n",
    "original_metrics = print_model_report(model_name, original_model, test_loader, scalers, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519f4ce1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Visualize Model Predictions Over Time\n",
    "Check if the model is loaded correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36730b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define target scaler from the data_metadata\n",
    "target_scaler = scalers.get(f'{TARGET_VARIABLE}_scaler')\n",
    "if target_scaler is None:\n",
    "    print(f\"Warning: No scaler found for target field '{TARGET_VARIABLE}'. Visualization may show scaled values.\")\n",
    "\n",
    "# Visualize the loaded model's predictions\n",
    "print(\"Generating predictions visualization...\")\n",
    "original_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Create the visualization using the imported function\n",
    "viz_fig = plot_predictions_over_time(\n",
    "    models=[original_model],\n",
    "    model_names=[model_name],\n",
    "    data_loader=test_loader,\n",
    "    target_scaler=target_scaler,\n",
    "    num_samples=72,  # Adjust as needed\n",
    "    start_idx=40,\n",
    "    device=device       # Adjust as needed\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba7e9ea",
   "metadata": {},
   "source": [
    "## Technique 1: Quantization\n",
    "\n",
    "Quantization reduces model precision from float32 to int8 to decrease model size and improve inference speed.\n",
    "\n",
    "The quantization in this section is done with ONNX Runtime on CPU (section 1a and 1b) and GPU (section 1c).\n",
    "\n",
    "Require libraries:\n",
    "- onnx\n",
    "- onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8225e3e8",
   "metadata": {},
   "source": [
    "## Technique 1a: Int8 Quantization with ONNX (CPU)\n",
    "\n",
    "This section demonstrates exporting the PyTorch model to ONNX format and applying ONNX dynamic quantization for model efficiency.\n",
    "\n",
    "Steps:\n",
    "1. Export the original PyTorch model to ONNX\n",
    "2. Quantize the ONNX model with ONNX Runtime\n",
    "3. Evaluate the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d506f89d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Define an ONNX wrapper to preserve batch dimension in the output\n",
    "import onnx\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import onnxruntime as ort\n",
    "\n",
    "class OnnxModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(OnnxModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, temporal_features, static_features):\n",
    "        outputs = self.model(temporal_features, static_features)\n",
    "        return outputs\n",
    "\n",
    "# Define device (use CPU for 'fbgemm')\n",
    "cpu_device = torch.device('cpu')\n",
    "\n",
    "# Prepare wrapper and CPU sample input\n",
    "wrapper = OnnxModelWrapper(original_model).eval().to(cpu_device)\n",
    "sample_input_cpu = (sample_input[0].cpu(), sample_input[1].cpu())\n",
    "\n",
    "# Export the original model to ONNX\n",
    "onnx_model_path = f\"checkpoints/{base_checkpoint_name}_original.onnx\"\n",
    "print(f\"Exporting original model to ONNX at {onnx_model_path}\")\n",
    "torch.onnx.export(\n",
    "    wrapper,\n",
    "    sample_input_cpu,\n",
    "    onnx_model_path,\n",
    "    input_names=[\"temporal_features\", \"static_features\"],\n",
    "    output_names=[\"output\"],\n",
    "    opset_version=20,\n",
    "    dynamic_axes={\n",
    "        \"temporal_features\": {0: \"batch\"},\n",
    "        \"static_features\": {0: \"batch\"},\n",
    "        \"output\": {0: \"batch\"},\n",
    "    },\n",
    ")\n",
    "# File size before quantization\n",
    "orig_onnx_size = os.path.getsize(onnx_model_path) / (1024 * 1024)\n",
    "print(f\"Original ONNX model size: {orig_onnx_size:.2f} MB\")\n",
    "\n",
    "# Apply dynamic quantization with ONNX Runtime\n",
    "quantized_onnx_model_path = f\"checkpoints/{base_checkpoint_name}_quantized_int8.onnx\"\n",
    "print(f\"Quantizing ONNX model to {quantized_onnx_model_path}\")\n",
    "quantize_dynamic(\n",
    "    model_input=onnx_model_path,\n",
    "    model_output=quantized_onnx_model_path,\n",
    "    weight_type=QuantType.QUInt8\n",
    ")\n",
    "\n",
    "# Run ONNX shape inference to populate output shape info and avoid mismatches\n",
    "print(\"Running ONNX shape inference for original and quantized models...\")\n",
    "# Original ONNX model shape inference\n",
    "model_proto = onnx.load(onnx_model_path)\n",
    "inferred_model = onnx.shape_inference.infer_shapes(model_proto)\n",
    "onnx.save(inferred_model, onnx_model_path)\n",
    "# Quantized ONNX model shape inference\n",
    "model_q_proto = onnx.load(quantized_onnx_model_path)\n",
    "inferred_q_model = onnx.shape_inference.infer_shapes(model_q_proto)\n",
    "onnx.save(inferred_q_model, quantized_onnx_model_path)\n",
    "\n",
    "# Only annotate batch dimension if missing\n",
    "for model_path in [onnx_model_path, quantized_onnx_model_path]:\n",
    "    m = onnx.load(model_path)\n",
    "    for output in m.graph.output:\n",
    "        shape = output.type.tensor_type.shape\n",
    "        # Ensure dynamic batch dimension exists\n",
    "        if len(shape.dim) == 0:\n",
    "            dim0 = shape.dim.add()\n",
    "            dim0.dim_param = \"batch\"\n",
    "    onnx.save(m, model_path)\n",
    "\n",
    "quant_onnx_size = os.path.getsize(quantized_onnx_model_path) / (1024 * 1024)\n",
    "print(f\"Quantized ONNX model size: {quant_onnx_size:.2f} MB\")\n",
    "print()\n",
    "\n",
    "# Helper function to evaluate ONNX models over the full test set\n",
    "def evaluate_onnx_model(model_path):\n",
    "    sess = ort.InferenceSession(model_path, providers=[\"CPUExecutionProvider\"])\n",
    "    input_name1 = sess.get_inputs()[0].name\n",
    "    input_name2 = sess.get_inputs()[1].name\n",
    "    # Warm-up on the first batch to initialize optimizations\n",
    "    first_batch = next(iter(test_loader))\n",
    "    warm_inp1 = first_batch['temporal_features'].numpy()\n",
    "    warm_inp2 = first_batch['static_features'].numpy()\n",
    "    for _ in range(5):\n",
    "        sess.run(None, {input_name1: warm_inp1, input_name2: warm_inp2})\n",
    "    # Measure inference time over all batches and collect outputs for MAE\n",
    "    total_time = 0.0\n",
    "    num_batches = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    for batch in tqdm(test_loader, desc=f\"ONNX inference ({os.path.basename(model_path)})\"):\n",
    "        inp1 = batch['temporal_features'].numpy()\n",
    "        inp2 = batch['static_features'].numpy()\n",
    "        targets = batch['target'].numpy()\n",
    "        start = time.time()\n",
    "        outputs = sess.run(None, {input_name1: inp1, input_name2: inp2})[0]\n",
    "        elapsed = time.time() - start\n",
    "        total_time += elapsed\n",
    "        num_batches += 1\n",
    "        all_preds.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "    avg_time = total_time / num_batches if num_batches > 0 else 0.0\n",
    "    # Concatenate and inverse-transform\n",
    "    preds = np.concatenate(all_preds, axis=0)\n",
    "    targets_arr = np.concatenate(all_targets, axis=0)\n",
    "    # Inverse scale if available\n",
    "    scaler = scalers.get(f\"{TARGET_VARIABLE}_scaler\", None)\n",
    "    if scaler is not None:\n",
    "        preds = scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "        targets_arr = scaler.inverse_transform(targets_arr.reshape(-1, 1)).flatten()\n",
    "    # Compute MAE via sklearn\n",
    "    mae_value = mean_absolute_error(targets_arr, preds)\n",
    "    print(f\"Inference time for {os.path.basename(model_path)}: {avg_time*1000:.2f} ms per batch over {num_batches} batches\")\n",
    "    print(f\"MAE for {os.path.basename(model_path)}: {mae_value:.4f}\")\n",
    "    return {'size': os.path.getsize(model_path)/(1024*1024), 'inference_time': avg_time, 'mae': mae_value}\n",
    "\n",
    "# Technique 1a: ONNX CPU Quantization Results\n",
    "print(\"\\n===== Technique 1a: ONNX CPU Quantization Results =====\")\n",
    "onnx_orig_metrics = evaluate_onnx_model(onnx_model_path)\n",
    "onnx_int8_metrics = evaluate_onnx_model(quantized_onnx_model_path)\n",
    "print(f\"{'Model':<30}{'Size (MB)':<12}{'Latency(ms)':<15}{'MAE':<10}\")\n",
    "print('-'*67)\n",
    "print(f\"{'Original ONNX CPU':<30}{onnx_orig_metrics['size']:<12.2f}{onnx_orig_metrics['inference_time']*1000:<15.2f}{onnx_orig_metrics['mae']:<10.4f}\")\n",
    "print(f\"{'Int8 ONNX CPU':<30}{onnx_int8_metrics['size']:<12.2f}{onnx_int8_metrics['inference_time']*1000:<15.2f}{onnx_int8_metrics['mae']:<10.4f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b33cc23",
   "metadata": {},
   "source": [
    "## Technique 1b: Int4 Quantization with ONNX (CPU)\n",
    "\n",
    "Int4 quantization is an extreme form of quantization that reduces model weights to 4-bit integers.\n",
    "This technique significantly reduces model size with some potential impact on accuracy.\n",
    "It's especially effective for large transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e231bba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "print(\"\\n===== Technique 1b: ONNX Int4 Quantization =====\")\n",
    "\n",
    "from onnxruntime.quantization import (\n",
    "    matmul_4bits_quantizer,\n",
    "    quant_utils,\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "# Define input/output paths\n",
    "model_fp32_path = onnx_model_path  # Re-use the original FP32 ONNX model\n",
    "model_int4_path = f\"checkpoints/{base_checkpoint_name}_quantized_int4.onnx\"\n",
    "\n",
    "print(f\"Creating Int4 quantized model at {model_int4_path}\")\n",
    "\n",
    "# Configure Int4 quantization settings\n",
    "quant_config = matmul_4bits_quantizer.DefaultWeightOnlyQuantConfig(\n",
    "    block_size=128,  # 2's exponential and >= 16\n",
    "    is_symmetric=True,  # if true, quantize to Int4, otherwise, quantize to uint4\n",
    "    accuracy_level=4,  # used by MatMulNbits\n",
    "    quant_format=quant_utils.QuantFormat.QOperator,\n",
    "    op_types_to_quantize=(\"MatMul\", \"Gather\"),  # specify which op types to quantize\n",
    "    quant_axes=((\"MatMul\", 0), (\"Gather\", 1),)  # specify which axis to quantize for an op type\n",
    ")\n",
    "\n",
    "# Load the model with shape inference\n",
    "model = quant_utils.load_model_with_shape_infer(Path(model_fp32_path))\n",
    "\n",
    "# Create the quantizer\n",
    "quant = matmul_4bits_quantizer.MatMul4BitsQuantizer(\n",
    "    model,\n",
    "    nodes_to_exclude=None,  # specify a list of nodes to exclude from quantization\n",
    "    nodes_to_include=None,  # specify a list of nodes to force include from quantization\n",
    "    algo_config=quant_config,\n",
    ")\n",
    "\n",
    "# Apply quantization\n",
    "quant.process()\n",
    "\n",
    "# Save the quantized model\n",
    "quant.model.save_model_to_file(\n",
    "    model_int4_path,\n",
    "    True  # save data to external file\n",
    ")\n",
    "\n",
    "quant_int4_size = os.path.getsize(model_int4_path) / (1024 * 1024)\n",
    "print(f\"Int4 Quantized ONNX model size: {quant_int4_size:.2f} MB\")\n",
    "print(f\"Size reduction: {(orig_onnx_size - quant_int4_size) / orig_onnx_size * 100:.2f}%\")\n",
    "\n",
    "# Evaluate Int4 model\n",
    "print(\"\\nEvaluating Int4 quantized model on CPU...\")\n",
    "onnx_int4_metrics = evaluate_onnx_model(model_int4_path)\n",
    "\n",
    "# Add to our comparison table\n",
    "print(\"\\n===== Int4 Quantization Results =====\")\n",
    "print(f\"{'Model':<30}{'Size (MB)':<12}{'Latency(ms)':<15}{'MAE':<10}\")\n",
    "print('-'*67)\n",
    "print(f\"{'Original ONNX CPU':<30}{onnx_orig_metrics['size']:<12.2f}{onnx_orig_metrics['inference_time']*1000:<15.2f}{onnx_orig_metrics['mae']:<10.4f}\")\n",
    "print(f\"{'Int8 ONNX CPU':<30}{onnx_int8_metrics['size']:<12.2f}{onnx_int8_metrics['inference_time']*1000:<15.2f}{onnx_int8_metrics['mae']:<10.4f}\")\n",
    "print(f\"{'Int4 ONNX CPU':<30}{onnx_int4_metrics['size']:<12.2f}{onnx_int4_metrics['inference_time']*1000:<15.2f}{onnx_int4_metrics['mae']:<10.4f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a31290e",
   "metadata": {},
   "source": [
    "## Technique 1c: FP16 Quantization (Requires CUDA GPU)\n",
    "\n",
    "FP16 quantization is a technique that converts the model to FP16 precision to reduce memory usage and improve inference speed.\n",
    "\n",
    "Require libraries:\n",
    "- onnxconverter-common\n",
    "- onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8aa81",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Helper function to evaluate ONNX models on GPU\n",
    "def evaluate_onnx_model_gpu(model_path, provider='CUDAExecutionProvider', fp16_mode=False):\n",
    "    # Configure session options to optimize for GPU\n",
    "    sess_options = ort.SessionOptions()\n",
    "    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "\n",
    "    # Silence provider assignment warnings - these are expected and normal\n",
    "    sess_options.add_session_config_entry(\"session.log.severity\", \"2\")  # Set log level to WARNING (2) or ERROR (3)\n",
    "    sess_options.log_severity_level = 2\n",
    "\n",
    "    # Create inference session with CUDA provider\n",
    "    # Add CPU provider as a fallback because not all operations are supported on GPU\n",
    "    providers = [provider, 'CPUExecutionProvider']\n",
    "    session = ort.InferenceSession(\n",
    "        model_path,\n",
    "        providers=providers,\n",
    "        sess_options=sess_options\n",
    "    )\n",
    "\n",
    "    input_name1 = session.get_inputs()[0].name\n",
    "    input_name2 = session.get_inputs()[1].name\n",
    "\n",
    "    # Get a small batch for warm-up\n",
    "    first_batch = next(iter(test_loader))\n",
    "    warm_inp1 = first_batch['temporal_features'].numpy()\n",
    "    warm_inp2 = first_batch['static_features'].numpy()\n",
    "\n",
    "    # Convert inputs to FP16 if running in FP16 mode\n",
    "    if fp16_mode:\n",
    "        warm_inp1 = warm_inp1.astype(np.float16)\n",
    "        warm_inp2 = warm_inp2.astype(np.float16)\n",
    "\n",
    "    # Warm up with a few iterations\n",
    "    print(f\"Warming up GPU ONNX model with provider: {provider}...\")\n",
    "    for _ in range(10):  # More warm-up iterations for GPU\n",
    "        session.run(None, {input_name1: warm_inp1, input_name2: warm_inp2})\n",
    "\n",
    "    # Measure inference time and MAE\n",
    "    total_time = 0.0\n",
    "    num_batches = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in tqdm(test_loader, desc=f\"GPU ONNX inference ({os.path.basename(model_path)})\"):\n",
    "        inp1 = batch['temporal_features'].numpy()\n",
    "        inp2 = batch['static_features'].numpy()\n",
    "        targets = batch['target'].numpy()\n",
    "\n",
    "        # Convert inputs to FP16 if running in FP16 mode\n",
    "        if fp16_mode:\n",
    "            inp1 = inp1.astype(np.float16)\n",
    "            inp2 = inp2.astype(np.float16)\n",
    "\n",
    "        # Sync CUDA before timing\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        start = time.time()\n",
    "        outputs = session.run(None, {input_name1: inp1, input_name2: inp2})[0]\n",
    "\n",
    "        # Sync CUDA after inference for accurate timing\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "        total_time += elapsed\n",
    "        num_batches += 1\n",
    "\n",
    "        all_preds.append(outputs)\n",
    "        all_targets.append(targets)\n",
    "\n",
    "    avg_time = total_time / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "    # Calculate MAE (same as before)\n",
    "    preds = np.concatenate(all_preds, axis=0)\n",
    "    targets_arr = np.concatenate(all_targets, axis=0)\n",
    "\n",
    "    scaler = scalers.get(f\"{TARGET_VARIABLE}_scaler\", None)\n",
    "    if scaler is not None:\n",
    "        preds = scaler.inverse_transform(preds.reshape(-1, 1)).flatten()\n",
    "        targets_arr = scaler.inverse_transform(targets_arr.reshape(-1, 1)).flatten()\n",
    "\n",
    "    mae_value = mean_absolute_error(targets_arr, preds)\n",
    "\n",
    "    print(f\"GPU Inference time for {os.path.basename(model_path)}: {avg_time*1000:.2f} ms per batch over {num_batches} batches\")\n",
    "    print(f\"MAE for {os.path.basename(model_path)}: {mae_value:.4f}\")\n",
    "\n",
    "    return {\n",
    "        'size': os.path.getsize(model_path)/(1024*1024),\n",
    "        'inference_time': avg_time,\n",
    "        'mae': mae_value,\n",
    "        'provider': provider\n",
    "    }\n",
    "\n",
    "# Create an FP16 quantized model specifically for GPU\n",
    "quantized_fp16_path = f\"checkpoints/{base_checkpoint_name}_quantized_fp16.onnx\"\n",
    "print(f\"Creating FP16 quantized model for GPU at {quantized_fp16_path}\")\n",
    "\n",
    "# Use the convert_float_to_float16 utility from ONNX\n",
    "from onnxconverter_common import float16\n",
    "model_fp16 = float16.convert_float_to_float16(\n",
    "    model=model_proto,\n",
    "    min_positive_val=1e-7,\n",
    "    max_finite_val=1e4,\n",
    ")\n",
    "for output in model_fp16.graph.output:\n",
    "    shape = output.type.tensor_type.shape\n",
    "    # Ensure dynamic batch dimension exists\n",
    "    if len(shape.dim) == 0:\n",
    "        dim0 = shape.dim.add()\n",
    "        dim0.dim_param = \"batch\"\n",
    "onnx.save(model_fp16, quantized_fp16_path)\n",
    "\n",
    "# First evaluate the original model on GPU for baseline comparison\n",
    "print(\"\\nEvaluating original (FP32) ONNX model on GPU...\")\n",
    "onnx_gpu_metrics = evaluate_onnx_model_gpu(onnx_model_path, fp16_mode=False)\n",
    "\n",
    "# Evaluate the FP16 model on GPU\n",
    "print(\"\\nEvaluating FP16 quantized model on GPU...\")\n",
    "onnx_fp16_metrics = evaluate_onnx_model_gpu(quantized_fp16_path, fp16_mode=True)\n",
    "\n",
    "# Technique 1b: ONNX GPU Quantization Results (FP32 vs FP16 vs INT8)\n",
    "print(\"\\n===== Technique 1b: ONNX GPU Quantization Results =====\")\n",
    "print(f\"{'Model':<30}{'Size (MB)':<12}{'Latency(ms)':<15}{'MAE':<10}\")\n",
    "print('-'*67)\n",
    "print(f\"{'Original ONNX GPU (FP32)':<30}{onnx_gpu_metrics['size']:<12.2f}{onnx_gpu_metrics['inference_time']*1000:<15.2f}{onnx_gpu_metrics['mae']:<10.4f}\")\n",
    "print(f\"{'FP16 ONNX GPU':<30}{onnx_fp16_metrics['size']:<12.2f}{onnx_fp16_metrics['inference_time']*1000:<15.2f}{onnx_fp16_metrics['mae']:<10.4f}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e494f",
   "metadata": {},
   "source": [
    "## Technique 2: Structured Pruning\n",
    "\n",
    "Structured pruning removes less important components to reduce the number of parameters.\n",
    "\n",
    "Steps:\n",
    "1. Apply different levels of structured pruning to the transformer model using PyTorch's pruning utilities\n",
    "2. Evaluate the pruned model with different levels of sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2fad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Technique 2: Structured Pruning =====\")\n",
    "\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Apply structured pruning to the transformer model using PyTorch's pruning utilities\n",
    "def apply_structured_pruning(model, amount=0.3):\n",
    "    \"\"\"\n",
    "    Apply structured pruning to a model using PyTorch's pruning utilities.\n",
    "\n",
    "    Args:\n",
    "        model: The model to prune\n",
    "        amount: The proportion of weights to prune (0-1)\n",
    "\n",
    "    Returns:\n",
    "        Pruned model\n",
    "    \"\"\"\n",
    "    # Create a copy of the model to avoid modifying the original\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "\n",
    "    # Track which layers were pruned\n",
    "    pruned_layers = []\n",
    "\n",
    "    # Apply structured pruning (channel-wise) to convolutional layers\n",
    "    for name, module in pruned_model.named_modules():\n",
    "        # Prune Linear layers by channels (structured pruning)\n",
    "        if isinstance(module, nn.Linear) and module.out_features > 1:\n",
    "            # Only prune if output dimension is > 1 to avoid errors\n",
    "            prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)\n",
    "            prune.remove(module, 'weight')  # Make pruning permanent\n",
    "            pruned_layers.append(name)\n",
    "\n",
    "        # Prune the attention weights (more targeted pruning for transformers)\n",
    "        # This requires identifying the specific attention matrices in your model\n",
    "        if 'query' in name or 'key' in name or 'value' in name:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                prune.ln_structured(module, name='weight', amount=amount, n=2, dim=0)\n",
    "                prune.remove(module, 'weight')\n",
    "                pruned_layers.append(name)\n",
    "\n",
    "    print(f\"Applied structured pruning to {len(pruned_layers)} layers with amount={amount}\")\n",
    "\n",
    "    return pruned_model\n",
    "\n",
    "# Define a function to measure sparsity\n",
    "def measure_sparsity(model):\n",
    "    \"\"\"Measure the sparsity of a model (percentage of zeros)\"\"\"\n",
    "    total_params = 0\n",
    "    zero_params = 0\n",
    "\n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            total_params += param.numel()\n",
    "            zero_params += (param == 0).sum().item()\n",
    "\n",
    "    sparsity = 100.0 * zero_params / total_params if total_params > 0 else 0\n",
    "    return sparsity\n",
    "\n",
    "# Print sparsity of original model\n",
    "print(f\"Original model sparsity: {measure_sparsity(original_model):.2f}%\")\n",
    "\n",
    "# Apply different levels of pruning and evaluate\n",
    "pruning_levels = [0.1, 0.3, 0.5]\n",
    "pruning_results = []\n",
    "\n",
    "for prune_amount in pruning_levels:\n",
    "    print(f\"\\nApplying {prune_amount:.1%} structured pruning...\")\n",
    "\n",
    "    # Apply pruning\n",
    "    pruned_model = apply_structured_pruning(original_model, amount=prune_amount)\n",
    "    pruned_model = pruned_model.to(device)\n",
    "\n",
    "    # Measure sparsity\n",
    "    sparsity = measure_sparsity(pruned_model)\n",
    "    print(f\"Pruned model sparsity: {sparsity:.2f}%\")\n",
    "\n",
    "    # Evaluate pruned model\n",
    "    pruned_metrics = print_model_report(f\"{model_name}_pruned_{prune_amount:.1f}\",\n",
    "                                        pruned_model, test_loader, scalers, device=device)\n",
    "\n",
    "    # Store results\n",
    "    pruning_results.append({\n",
    "        'prune_amount': prune_amount,\n",
    "        'sparsity': sparsity,\n",
    "        'metrics': pruned_metrics\n",
    "    })\n",
    "\n",
    "# Compare the models with different pruning levels\n",
    "print(\"\\n===== Structured Pruning Results =====\")\n",
    "print(f\"{'Model':<30}{'Size (MB)':<12}{'Sparsity':<10}{'Latency(ms)':<15}{'MAE':<10}\")\n",
    "print('-'*77)\n",
    "print(f\"{'Original Model':<30}{original_metrics['size']:<12.2f}{measure_sparsity(original_model):<10.2f}{original_metrics['inference_time']*1000:<15.2f}{original_metrics['metrics']['mae']:<10.4f}\")\n",
    "\n",
    "for result in pruning_results:\n",
    "    metrics = result['metrics']\n",
    "    name = f\"Pruned ({result['prune_amount']:.1%})\"\n",
    "    print(f\"{name:<30}{metrics['size']:<12.2f}{result['sparsity']:<10.2f}{metrics['inference_time']*1000:<15.2f}{metrics['metrics']['mae']:<10.4f}\")\n",
    "\n",
    "# Find the best pruned model based on inference time and accuracy tradeoff\n",
    "# Simple metric: normalize both factors and sum\n",
    "best_model_idx = 0\n",
    "best_score = -float('inf')\n",
    "\n",
    "for i, result in enumerate(pruning_results):\n",
    "    # Lower MAE is better, higher speed improvement is better\n",
    "    mae_score = original_metrics['metrics']['mae'] / result['metrics']['metrics']['mae']\n",
    "    speed_score = original_metrics['inference_time'] / result['metrics']['inference_time']\n",
    "\n",
    "    # Combined score (you could adjust weights as needed)\n",
    "    score = mae_score + speed_score\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_model_idx = i\n",
    "\n",
    "# Use the best pruned model for overall comparison\n",
    "best_pruned_model = pruning_results[best_model_idx]['metrics']\n",
    "print(f\"\\nBest pruning level: {pruning_results[best_model_idx]['prune_amount']:.1%}\")\n",
    "\n",
    "# Add the best pruned model to the overall comparison\n",
    "all_models = [original_metrics, best_pruned_model]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe7857f",
   "metadata": {},
   "source": [
    "## Technique 3: Knowledge Distillation\n",
    "\n",
    "Knowledge distillation trains a smaller \"student\" model to mimic the behavior of the larger \"teacher\" model,\n",
    "preserving most of the accuracy while using fewer parameters.\n",
    "\n",
    "Steps:\n",
    "1. Create a smaller student model based on the original model architecture\n",
    "2. Train the student model with knowledge distillation from the teacher model\n",
    "3. Evaluate the student model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced34b7",
   "metadata": {},
   "source": [
    "### Settings for Distillation Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c03c860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Distillation Training settings =========\n",
    "from utils.wandb_utils import set_wandb_flag, set_keep_run_open\n",
    "\n",
    "PATIENCE = 5  # Early stopping patience\n",
    "LR = 0.0001\n",
    "if DEBUG_MODE:\n",
    "    BATCH_SIZE = 2**10\n",
    "    NUM_WORKERS = 4\n",
    "    EPOCHS = 10\n",
    "    USE_WANDB = False\n",
    "else:\n",
    "    BATCH_SIZE = 2**13\n",
    "    NUM_WORKERS = 16\n",
    "    EPOCHS = 30\n",
    "    USE_WANDB = True\n",
    "# Enable wandb tracking\n",
    "set_wandb_flag(USE_WANDB)\n",
    "# Keep the wandb run open after training to continue logging evaluation plots\n",
    "set_keep_run_open(True)\n",
    "if is_wandb_enabled():\n",
    "    wandb.finish()\n",
    "# Transformer student model settings (should be lower than the original model)\n",
    "STUDENT_D_MODEL = 128\n",
    "STUDENT_N_HEADS = 2\n",
    "STUDENT_E_LAYERS = 1\n",
    "STUDENT_D_FF = 128\n",
    "# ======================================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2236ef",
   "metadata": {},
   "source": [
    "### Training with Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d79c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n===== Technique 3: Knowledge Distillation =====\")\n",
    "# Get the current config\n",
    "CONFIG = {}\n",
    "cur_globals = globals().copy()\n",
    "for x in cur_globals:\n",
    "    # Only get the variables that are uppercase and not digits\n",
    "    if x.upper() == x and not x.startswith('_') and not x == \"CONFIG\":\n",
    "        CONFIG[x] = cur_globals[x]\n",
    "metadata = {\n",
    "    \"model_name\": model_name,\n",
    "}\n",
    "\n",
    "# Define distillation loss - combines task loss with matching teacher outputs\n",
    "class DistillationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, temperature=2.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            alpha: Weight for the distillation loss (0-1)\n",
    "            temperature: Temperature for softening the teacher's predictions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, student_outputs, teacher_outputs, targets):\n",
    "        # Task loss - direct prediction loss\n",
    "        task_loss = self.mse_loss(student_outputs, targets)\n",
    "\n",
    "        # Distillation loss - match the teacher's predictions\n",
    "        # For regression task, we use MSE between student and teacher outputs\n",
    "        distill_loss = self.mse_loss(student_outputs, teacher_outputs)\n",
    "\n",
    "        # Combined loss\n",
    "        loss = (1 - self.alpha) * task_loss + self.alpha * distill_loss\n",
    "\n",
    "        return loss, task_loss, distill_loss\n",
    "\n",
    "# Create smaller student model based on the original model architecture\n",
    "def create_student_model(original_model, d_model=STUDENT_D_MODEL, n_heads=STUDENT_N_HEADS, e_layers=STUDENT_E_LAYERS, d_ff=STUDENT_D_FF):\n",
    "    \"\"\"\n",
    "    Creates a smaller student model with reduced parameters\n",
    "    \"\"\"\n",
    "    from models.transformer import TransformerModel\n",
    "\n",
    "    print(f\"Creating student model with d_model={d_model}, n_heads={n_heads}, e_layers={e_layers}\")\n",
    "\n",
    "    # Use the TransformerModel with smaller parameters\n",
    "    return TransformerModel(\n",
    "        input_dim=original_model.input_dim,\n",
    "        static_dim=original_model.static_dim,\n",
    "        d_model=d_model,\n",
    "        n_heads=n_heads,\n",
    "        e_layers=e_layers,\n",
    "        d_ff=d_ff,\n",
    "        dropout=0.1,\n",
    "    )\n",
    "\n",
    "# Training function for distillation\n",
    "@track_experiment\n",
    "def train_with_distillation(teacher_model, student_model, train_loader, val_loader,\n",
    "                          criterion, optimizer, scheduler=None,\n",
    "                          epochs=EPOCHS, device=device, patience=PATIENCE,\n",
    "                          debug_mode=DEBUG_MODE, model_name=\"DistilledModel\",\n",
    "                          target_scaler=None, config=None):\n",
    "    \"\"\"\n",
    "    Train the student model with knowledge distillation from the teacher\n",
    "\n",
    "    Args:\n",
    "        teacher_model: Teacher model for knowledge distillation\n",
    "        student_model: Student model to be trained\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        criterion: Loss function for distillation\n",
    "        optimizer: Optimizer for training\n",
    "        scheduler: Learning rate scheduler (optional)\n",
    "        epochs: Maximum number of epochs\n",
    "        device: Device to train on\n",
    "        patience: Early stopping patience\n",
    "        debug_mode: Whether to run in debug mode (limit batches)\n",
    "        model_name: Name of the model for logging\n",
    "        target_scaler: Scaler for the target variable\n",
    "        config: Configuration dictionary\n",
    "\n",
    "    Returns:\n",
    "        student_model: Trained student model\n",
    "        best_val_loss: Best validation loss achieved\n",
    "    \"\"\"\n",
    "    # Ensure models are on the correct device\n",
    "    teacher_model = teacher_model.to(device)\n",
    "    student_model = student_model.to(device)\n",
    "\n",
    "    teacher_model.eval()  # Teacher model is only used for inference\n",
    "    student_model.train()\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        student_model.train()\n",
    "        train_losses = []\n",
    "        task_losses = []\n",
    "        distill_losses = []\n",
    "        train_samples = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} [Train]\")\n",
    "        for i, batch in enumerate(pbar):\n",
    "            if debug_mode and i >= 10:\n",
    "                break\n",
    "\n",
    "            # Move data to device\n",
    "            temporal_features = batch['temporal_features'].to(device)\n",
    "            static_features = batch['static_features'].to(device)\n",
    "            targets = batch['target'].to(device)\n",
    "            train_samples += targets.size(0)\n",
    "\n",
    "            # Forward pass through teacher model (no grad needed)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = teacher_model(temporal_features, static_features)\n",
    "\n",
    "            # Forward pass through student model\n",
    "            optimizer.zero_grad()\n",
    "            student_outputs = student_model(temporal_features, static_features)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss, task_loss, distill_loss = criterion(student_outputs, teacher_outputs, targets)\n",
    "\n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Log losses\n",
    "            train_losses.append(loss.item())\n",
    "            task_losses.append(task_loss.item())\n",
    "            distill_losses.append(distill_loss.item())\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': f\"{loss.item():.4f}\",\n",
    "                'task_loss': f\"{task_loss.item():.4f}\",\n",
    "                'distill_loss': f\"{distill_loss.item():.4f}\"\n",
    "            })\n",
    "\n",
    "        # Validation phase\n",
    "        student_model.eval()\n",
    "        val_losses = []\n",
    "        val_task_losses = []\n",
    "        val_distill_losses = []\n",
    "        val_samples = 0\n",
    "        student_preds = []\n",
    "        val_targets_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} [Val]\")):\n",
    "                if debug_mode and i >= 10:\n",
    "                    break\n",
    "\n",
    "                # Move data to device\n",
    "                temporal_features = batch['temporal_features'].to(device)\n",
    "                static_features = batch['static_features'].to(device)\n",
    "                targets = batch['target'].to(device)\n",
    "                val_samples += targets.size(0)\n",
    "\n",
    "                # Forward pass\n",
    "                teacher_outputs = teacher_model(temporal_features, static_features)\n",
    "                student_outputs = student_model(temporal_features, static_features)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss, task_loss, distill_loss = criterion(student_outputs, teacher_outputs, targets)\n",
    "                val_losses.append(loss.item())\n",
    "                val_task_losses.append(task_loss.item())\n",
    "                val_distill_losses.append(distill_loss.item())\n",
    "\n",
    "                # Store predictions and targets for MAE calculation - reshape to ensure consistent dimensions\n",
    "                student_preds.append(student_outputs.cpu().numpy().flatten())\n",
    "                val_targets_list.append(targets.cpu().numpy().flatten())\n",
    "\n",
    "        # Calculate average losses\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses) if train_losses else 0\n",
    "        avg_task_loss = sum(task_losses) / len(task_losses) if task_losses else 0\n",
    "        avg_distill_loss = sum(distill_losses) / len(distill_losses) if distill_losses else 0\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses) if val_losses else 0\n",
    "        avg_val_task_loss = sum(val_task_losses) / len(val_task_losses) if val_task_losses else 0\n",
    "        avg_val_distill_loss = sum(val_distill_losses) / len(val_distill_losses) if val_distill_losses else 0\n",
    "\n",
    "        # Calculate MAE if we have target_scaler\n",
    "        student_preds_concat = np.concatenate(student_preds)\n",
    "        val_targets_concat = np.concatenate(val_targets_list)\n",
    "\n",
    "        if target_scaler is not None:\n",
    "            student_preds_orig = target_scaler.inverse_transform(student_preds_concat.reshape(-1, 1)).flatten()\n",
    "            val_targets_orig = target_scaler.inverse_transform(val_targets_concat.reshape(-1, 1)).flatten()\n",
    "            val_mae = mean_absolute_error(val_targets_orig, student_preds_orig)\n",
    "        else:\n",
    "            val_mae = mean_absolute_error(val_targets_concat, student_preds_concat)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f} \"\n",
    "              f\"(Task: {avg_task_loss:.4f}, Distill: {avg_distill_loss:.4f}) - \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f} \"\n",
    "              f\"(Task: {avg_val_task_loss:.4f}, Distill: {avg_val_distill_loss:.4f}) - \"\n",
    "              f\"Val MAE: {val_mae:.4f}\")\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "        # Log metrics to wandb\n",
    "        if is_wandb_enabled():\n",
    "            log_dict = {\n",
    "                'train/epoch': epoch,\n",
    "                'train/loss': avg_train_loss,\n",
    "                'train/task_loss': avg_task_loss,\n",
    "                'train/distill_loss': avg_distill_loss,\n",
    "                'train/learning_rate': optimizer.param_groups[0]['lr'],\n",
    "                'val/epoch': epoch,\n",
    "                'val/loss': avg_val_loss,\n",
    "                'val/task_loss': avg_val_task_loss,\n",
    "                'val/distill_loss': avg_val_distill_loss,\n",
    "                'val/mae': val_mae,\n",
    "            }\n",
    "            wandb.log(log_dict)\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model state\n",
    "            best_model_state = copy.deepcopy(student_model.state_dict())\n",
    "            print(f\"Improved validation loss! New best: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping after {epoch+1} epochs!\")\n",
    "                break\n",
    "\n",
    "    # Load best model state\n",
    "    if best_model_state is not None:\n",
    "        student_model.load_state_dict(best_model_state)\n",
    "\n",
    "    return student_model, best_val_loss\n",
    "\n",
    "# Create student model\n",
    "student_model = create_student_model(original_model)\n",
    "student_model = student_model.to(device)\n",
    "\n",
    "# Ensure both models are on the same device\n",
    "original_model = original_model.to(device)\n",
    "\n",
    "# Print model info\n",
    "print(\"\\nTeacher model:\")\n",
    "print_model_info(original_model, temporal_shape=sample_input[0].shape,\n",
    "                static_shape=sample_input[1].shape)\n",
    "print(\"\\nStudent model:\")\n",
    "print_model_info(student_model, temporal_shape=sample_input[0].shape,\n",
    "                static_shape=sample_input[1].shape)\n",
    "\n",
    "# Configure distillation training\n",
    "distillation_loss = DistillationLoss(alpha=0.5, temperature=2.0)\n",
    "optimizer = torch.optim.Adam(student_model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n",
    ")\n",
    "\n",
    "# Train student model with distillation\n",
    "print(\"\\nTraining student model with knowledge distillation...\")\n",
    "\n",
    "# Train with distillation\n",
    "student_model, best_val_loss = train_with_distillation(\n",
    "    teacher_model=original_model,\n",
    "    student_model=student_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=distillation_loss,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epochs=EPOCHS,\n",
    "    patience=PATIENCE,\n",
    "    debug_mode=DEBUG_MODE,\n",
    "    model_name=f\"{model_name}_student\",\n",
    "    target_scaler=target_scaler,\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "# Save the trained student model\n",
    "student_model_path = f\"checkpoints/{base_checkpoint_name}_student.pt\"\n",
    "save_model(student_model, student_model_path, temporal_features=SELECTED_FEATURES, static_features=STATIC_FEATURES,\n",
    "           target_field=TARGET_VARIABLE, config=CONFIG, time_feature_keys=TIME_FEATURES)\n",
    "print(f\"Saved student model to {student_model_path}\")\n",
    "\n",
    "# Evaluate student model\n",
    "student_metrics = print_model_report(f\"{model_name}_student\", student_model, test_loader, scalers, device=device)\n",
    "\n",
    "# Compare original and student models\n",
    "print(\"\\n===== Knowledge Distillation Results =====\")\n",
    "print(f\"{'Model':<30}{'Size (MB)':<12}{'Latency(ms)':<15}{'MAE':<10}\")\n",
    "print('-'*67)\n",
    "print(f\"{'Original Model':<30}{original_metrics['size']:<12.2f}{original_metrics['inference_time']*1000:<15.2f}{original_metrics['metrics']['mae']:<10.4f}\")\n",
    "print(f\"{'Student Model':<30}{student_metrics['size']:<12.2f}{student_metrics['inference_time']*1000:<15.2f}{student_metrics['metrics']['mae']:<10.4f}\")\n",
    "\n",
    "# Calculate and display improvement percentages\n",
    "size_reduction = (original_metrics['size'] - student_metrics['size']) / original_metrics['size'] * 100\n",
    "speed_improvement = (original_metrics['inference_time'] - student_metrics['inference_time']) / original_metrics['inference_time'] * 100\n",
    "accuracy_change = (original_metrics['metrics']['mae'] - student_metrics['metrics']['mae']) / original_metrics['metrics']['mae'] * 100\n",
    "\n",
    "print(f\"\\nSize reduction: {size_reduction:.2f}%\")\n",
    "print(f\"Inference speed improvement: {speed_improvement:.2f}%\")\n",
    "print(f\"Accuracy change: {accuracy_change:.2f}%\")\n",
    "\n",
    "# Visualize predictions of teacher and student models\n",
    "print(\"\\nGenerating predictions visualization for teacher and student models...\")\n",
    "viz_fig = plot_predictions_over_time(\n",
    "    models=[original_model, student_model],\n",
    "    model_names=[\"Teacher\", \"Student\"],\n",
    "    data_loader=test_loader,\n",
    "    target_scaler=target_scaler,\n",
    "    num_samples=72,\n",
    "    start_idx=40,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "plt.show()\n",
    "if is_wandb_enabled():\n",
    "    wandb.finish()\n",
    "\n",
    "# Add student model to overall comparison\n",
    "all_models = [original_metrics, best_pruned_model, student_metrics]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f0843e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Compare teacher and student model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d23a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model\n",
    "from utils.plot_utils import compare_models\n",
    "\n",
    "\n",
    "# Create a dictionary of model metrics\n",
    "model_metrics = {\n",
    "    'Original Model': original_metrics['metrics'],\n",
    "    'Student Model': student_metrics['metrics']\n",
    "}\n",
    "# Drop the 'y_pred' and 'y_true' keys from the model metrics\n",
    "for model in model_metrics:\n",
    "    model_metrics[model].pop('y_pred', None)\n",
    "    model_metrics[model].pop('y_true', None)\n",
    "    model_metrics[model].pop('nighttime_mask', None)\n",
    "\n",
    "# Save model metrics to a json file for later use\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "json_file_path = f'plots/compression_model_metrics_{timestamp}.json'\n",
    "# Fix TypeError: Object of type float32 is not JSON serializable\n",
    "for model in model_metrics:\n",
    "    for key, value in model_metrics[model].items():\n",
    "        if isinstance(value, np.float32):\n",
    "            model_metrics[model][key] = float(value)\n",
    "with open(json_file_path, 'w') as f:\n",
    "    json.dump(model_metrics, f)\n",
    "\n",
    "# Compare model performance on test set\n",
    "fig = compare_models(model_metrics, dataset_name='Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8975c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### Compare predictions of teacher and student models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c63c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot time series predictions for advanced models\n",
    "_ = plot_predictions_over_time(\n",
    "    models=[original_model, student_model],\n",
    "    model_names=['Teacher', 'Student'],\n",
    "    data_loader=test_loader,\n",
    "    target_scaler=target_scaler,\n",
    "    num_samples=72,\n",
    "    start_idx=40\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
