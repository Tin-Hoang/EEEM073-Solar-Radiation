{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GHI Forecasting using PyTorch Models\n",
    "\n",
    "This notebook implements deep learning models to forecast Global Horizontal Irradiance (GHI) based on various input features. The notebook is structured as follows:\n",
    "\n",
    "1. **Data Loading and Exploration**: Load the pre-split train/validation/test datasets and explore their structure\n",
    "2. **Data Preprocessing**: Prepare the data for model training (normalization, feature engineering)\n",
    "3. **Model 1: LSTM Network**: Implement a Long Short-Term Memory network for time series forecasting\n",
    "4. **Model 2: CNN-LSTM**: Implement a hybrid Convolutional-LSTM network\n",
    "5. **Model 3: Multi-Layer Perceptron**: Implement a deep neural network with multiple linear layers\n",
    "6. **Model 4: Physics-Informed MLP**: Implement a PINN-based MLP that enforces physical constraints\n",
    "7. **Model Evaluation**: Compare model performance on validation and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, let's import the necessary libraries and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm.notebook import tqdm\n",
    "import pytz\n",
    "\n",
    "# PyTorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Vietnam timezone (UTC+7)\n",
    "VIETNAM_TZ = pytz.timezone('Asia/Ho_Chi_Minh')\n",
    "\n",
    "# Function to compute nighttime mask and clear-sky GHI\n",
    "def compute_physical_constraints(timestamps, coordinates):\n",
    "    n_times = len(timestamps)\n",
    "    n_locations = coordinates.shape[0]\n",
    "    nighttime_mask = np.zeros((n_times, n_locations), dtype=np.float32)\n",
    "    clear_sky_ghi = np.zeros((n_times, n_locations), dtype=np.float32)\n",
    "    solar_constant = 1366.1  # W/m²\n",
    "\n",
    "    for t, timestamp in enumerate(timestamps):\n",
    "        # Convert UTC+0 to UTC+7\n",
    "        doy = timestamp.timetuple().tm_yday\n",
    "        decl = 23.45 * np.sin(2 * np.pi * (doy - 81) / 365)\n",
    "        decl_rad = np.deg2rad(decl)\n",
    "        hour = timestamp.hour + timestamp.minute / 60\n",
    "        ha = (hour - 12) * 15  # Hour angle in degrees\n",
    "        ha_rad = np.deg2rad(ha)\n",
    "\n",
    "        for loc in range(n_locations):\n",
    "            lat, lon = coordinates[loc]\n",
    "            lat_rad = np.deg2rad(lat)\n",
    "            cos_theta = np.sin(lat_rad) * np.sin(decl_rad) + np.cos(lat_rad) * np.cos(decl_rad) * np.cos(ha_rad)\n",
    "            if cos_theta < 0:\n",
    "                nighttime_mask[t, loc] = 1\n",
    "            else:\n",
    "                # Simplified clear-sky GHI (Ineichen-Perez model approximation)\n",
    "                clear_sky_ghi[t, loc] = solar_constant * cos_theta * 0.7  # Atmospheric transmittance factor\n",
    "\n",
    "    return nighttime_mask, clear_sky_ghi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths to the train, validation and test datasets\n",
    "train_path = 'data/processed/vietnam2016_time_based_train.h5'\n",
    "val_path = 'data/processed/vietnam2016_time_based_val.h5'\n",
    "test_path = 'data/processed/vietnam2016_time_based_test.h5'\n",
    "\n",
    "# Check if the files exist\n",
    "for path in [train_path, val_path, test_path]:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: {path} does not exist!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Exploring the Data Structure\n",
    "\n",
    "Let's examine the structure of our H5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_h5_structure(h5_path):\n",
    "    try:\n",
    "        with h5py.File(h5_path, 'r') as f:\n",
    "            print(f\"\\nExploring {h5_path}\")\n",
    "            print(\"\\nDatasets:\")\n",
    "            for key in f.keys():\n",
    "                dataset = f[key]\n",
    "                print(f\"  /{key} {dataset.shape}: {dataset.dtype}\")\n",
    "                if len(dataset) > 0 and dataset.ndim > 0:\n",
    "                    if dataset.dtype in [np.float32, np.float64]:\n",
    "                        stats = {\n",
    "                            'min': np.min(dataset[:10]),\n",
    "                            'max': np.max(dataset[:10]),\n",
    "                            'mean': np.mean(dataset[:10]),\n",
    "                            'stddev': np.std(dataset[:10]),\n",
    "                            'has_nan': np.isnan(dataset[:10]).any()\n",
    "                        }\n",
    "                        print(f\"    Sample stats: {stats}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error exploring {h5_path}: {e}\")\n",
    "\n",
    "# Explore data structures\n",
    "explore_h5_structure(train_path)\n",
    "explore_h5_structure(val_path)\n",
    "explore_h5_structure(test_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Loading Function\n",
    "\n",
    "We'll load the data and include the nighttime mask for the PINN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path, n_samples=1000, random_state=42):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        total_locations = f['coordinates'].shape[0]\n",
    "        np.random.seed(random_state)\n",
    "        sampled_indices = np.random.choice(total_locations, min(n_samples, total_locations), replace=False)\n",
    "        sampled_indices = np.sort(sampled_indices)\n",
    "\n",
    "        air_temp = f['air_temperature'][:, sampled_indices]\n",
    "        wind_speed = f['wind_speed'][:, sampled_indices]\n",
    "        coordinates = f['coordinates'][sampled_indices]\n",
    "        time_index = f['time_index'][:]\n",
    "        # Parse the actual timestamps from the file (they are in UTC)\n",
    "        # First convert bytes to strings\n",
    "        time_strings = [t.decode('utf-8') for t in time_index]\n",
    "        # Parse with pandas to preserve timezone information (UTC)\n",
    "        utc_timestamps = pd.to_datetime(time_strings)\n",
    "        # Convert UTC to Vietnam local time\n",
    "        timestamps = [utc_dt.astimezone(VIETNAM_TZ) for utc_dt in utc_timestamps]\n",
    "\n",
    "        ghi = f['ghi'][:, sampled_indices]\n",
    "\n",
    "        # Compute nighttime mask for PINN\n",
    "        nighttime_mask, clear_sky_ghi = compute_physical_constraints(timestamps, coordinates)\n",
    "\n",
    "        return {\n",
    "            'air_temperature': air_temp,\n",
    "            'wind_speed': wind_speed,\n",
    "            'coordinates': coordinates,\n",
    "            'time_index': time_index,\n",
    "            'timestamps': timestamps,\n",
    "            'ghi': ghi,\n",
    "            'nighttime_mask': nighttime_mask,\n",
    "            'clear_sky_ghi': clear_sky_ghi\n",
    "        }\n",
    "\n",
    "# Load data\n",
    "print(\"Loading training data...\")\n",
    "train_data = load_data(train_path, n_samples=500)\n",
    "print(\"Loading validation data...\")\n",
    "val_data = load_data(val_path, n_samples=500)\n",
    "print(\"Loading test data...\")\n",
    "test_data = load_data(test_path, n_samples=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series(data, location_idx=0, title=None):\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(15, 12), sharex=True)\n",
    "\n",
    "    axes[0].plot(data['timestamps'], data['ghi'][:, location_idx], 'b-', label='GHI')\n",
    "    axes[0].set_ylabel('GHI (W/m²)')\n",
    "    axes[0].set_title(title or f'Time Series for Location {location_idx}')\n",
    "    axes[0].grid(True)\n",
    "    axes[0].legend()\n",
    "\n",
    "    axes[1].plot(data['timestamps'], data['air_temperature'][:, location_idx], 'r-', label='Air Temp')\n",
    "    axes[1].set_ylabel('Air Temperature (°C)')\n",
    "    axes[1].grid(True)\n",
    "    axes[1].legend()\n",
    "\n",
    "    axes[2].plot(data['timestamps'], data['wind_speed'][:, location_idx], 'g-', label='Wind Speed')\n",
    "    axes[2].set_ylabel('Wind Speed (m/s)')\n",
    "    axes[2].set_xlabel('Time')\n",
    "    axes[2].grid(True)\n",
    "    axes[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    lat, lon = data['coordinates'][location_idx]\n",
    "    print(f\"Location coordinates: Latitude {lat:.4f}, Longitude {lon:.4f}\")\n",
    "\n",
    "# Plot data for a few locations\n",
    "for i in range(4):\n",
    "    plot_time_series(train_data, location_idx=i, title=f'Training Data - Location {i}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Time Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_time_features(timestamps):\n",
    "    hour_of_day = np.array([t.hour for t in timestamps])\n",
    "    day_of_year = np.array([t.timetuple().tm_yday for t in timestamps])\n",
    "    month = np.array([t.month for t in timestamps])\n",
    "\n",
    "    hour_sin = np.sin(2 * np.pi * hour_of_day / 24)\n",
    "    hour_cos = np.cos(2 * np.pi * hour_of_day / 24)\n",
    "    day_sin = np.sin(2 * np.pi * day_of_year / 366)\n",
    "    day_cos = np.cos(2 * np.pi * day_of_year / 366)\n",
    "    month_sin = np.sin(2 * np.pi * month / 12)\n",
    "    month_cos = np.cos(2 * np.pi * month / 12)\n",
    "\n",
    "    return np.column_stack([hour_sin, hour_cos, day_sin, day_cos, month_sin, month_cos])\n",
    "\n",
    "train_time_features = create_time_features(train_data['timestamps'])\n",
    "val_time_features = create_time_features(val_data['timestamps'])\n",
    "test_time_features = create_time_features(test_data['timestamps'])\n",
    "print(f\"Time features shape: {train_time_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(train_data, val_data, test_data):\n",
    "    temp_scaler = MinMaxScaler()\n",
    "    wind_scaler = MinMaxScaler()\n",
    "    ghi_scaler = MinMaxScaler()\n",
    "    coord_scaler = MinMaxScaler()\n",
    "    clear_sky_scaler = MinMaxScaler()\n",
    "\n",
    "    train_temp = train_data['air_temperature'].reshape(-1, 1)\n",
    "    train_wind = train_data['wind_speed'].reshape(-1, 1)\n",
    "    train_ghi = train_data['ghi'].reshape(-1, 1)\n",
    "    train_clear_sky = train_data['clear_sky_ghi'].reshape(-1, 1)\n",
    "\n",
    "    temp_scaler.fit(train_temp)\n",
    "    wind_scaler.fit(train_wind)\n",
    "    ghi_scaler.fit(train_ghi)\n",
    "    coord_scaler.fit(train_data['coordinates'])\n",
    "    clear_sky_scaler.fit(train_clear_sky)\n",
    "\n",
    "    norm_train_data = {\n",
    "        'air_temperature': temp_scaler.transform(train_temp).reshape(train_data['air_temperature'].shape),\n",
    "        'wind_speed': wind_scaler.transform(train_wind).reshape(train_data['wind_speed'].shape),\n",
    "        'coordinates': coord_scaler.transform(train_data['coordinates']),\n",
    "        'time_features': train_time_features,\n",
    "        'ghi': ghi_scaler.transform(train_ghi).reshape(train_data['ghi'].shape),\n",
    "        'nighttime_mask': train_data['nighttime_mask'],\n",
    "        'clear_sky_ghi': clear_sky_scaler.transform(train_clear_sky).reshape(train_data['clear_sky_ghi'].shape)\n",
    "    }\n",
    "\n",
    "    norm_val_data = {\n",
    "        'air_temperature': temp_scaler.transform(val_data['air_temperature'].reshape(-1, 1)).reshape(val_data['air_temperature'].shape),\n",
    "        'wind_speed': wind_scaler.transform(val_data['wind_speed'].reshape(-1, 1)).reshape(val_data['wind_speed'].shape),\n",
    "        'coordinates': coord_scaler.transform(val_data['coordinates']),\n",
    "        'time_features': val_time_features,\n",
    "        'ghi': ghi_scaler.transform(val_data['ghi'].reshape(-1, 1)).reshape(val_data['ghi'].shape),\n",
    "        'nighttime_mask': val_data['nighttime_mask'],\n",
    "        'clear_sky_ghi': clear_sky_scaler.transform(val_data['clear_sky_ghi'].reshape(-1, 1)).reshape(val_data['clear_sky_ghi'].shape)\n",
    "    }\n",
    "\n",
    "    norm_test_data = {\n",
    "        'air_temperature': temp_scaler.transform(test_data['air_temperature'].reshape(-1, 1)).reshape(test_data['air_temperature'].shape),\n",
    "        'wind_speed': wind_scaler.transform(test_data['wind_speed'].reshape(-1, 1)).reshape(test_data['wind_speed'].shape),\n",
    "        'coordinates': coord_scaler.transform(test_data['coordinates']),\n",
    "        'time_features': test_time_features,\n",
    "        'ghi': ghi_scaler.transform(test_data['ghi'].reshape(-1, 1)).reshape(test_data['ghi'].shape),\n",
    "        'nighttime_mask': test_data['nighttime_mask'],\n",
    "        'clear_sky_ghi': clear_sky_scaler.transform(test_data['clear_sky_ghi'].reshape(-1, 1)).reshape(test_data['clear_sky_ghi'].shape)\n",
    "    }\n",
    "\n",
    "    scalers = {\n",
    "        'temp_scaler': temp_scaler,\n",
    "        'wind_scaler': wind_scaler,\n",
    "        'ghi_scaler': ghi_scaler,\n",
    "        'coord_scaler': coord_scaler,\n",
    "        'clear_sky_scaler': clear_sky_scaler\n",
    "    }\n",
    "    return norm_train_data, norm_val_data, norm_test_data, scalers\n",
    "\n",
    "norm_train_data, norm_val_data, norm_test_data, scalers = normalize_data(train_data, val_data, test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Sequence Creation\n",
    "\n",
    "We'll include the nighttime mask in the sequences for the PINN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, lookback=24):\n",
    "    X_temp, X_wind, X_coord, X_time, y, X_target_nighttime, X_clear_sky = [], [], [], [], [], [], []\n",
    "    n_samples = data['air_temperature'].shape[1]\n",
    "\n",
    "    for loc in range(n_samples):\n",
    "        temp = data['air_temperature'][:, loc]\n",
    "        wind = data['wind_speed'][:, loc]\n",
    "        ghi = data['ghi'][:, loc]\n",
    "        coord = data['coordinates'][loc]\n",
    "        time_feat = data['time_features']\n",
    "        nighttime_mask = data['nighttime_mask'][:, loc]\n",
    "        clear_sky = data['clear_sky_ghi'][:, loc]\n",
    "\n",
    "        for i in range(len(temp) - lookback):\n",
    "            X_temp.append(temp[i:i+lookback])\n",
    "            X_wind.append(wind[i:i+lookback])\n",
    "            X_time.append(time_feat[i:i+lookback])\n",
    "            X_coord.append(np.tile(coord, (lookback, 1)))\n",
    "            y.append(ghi[i+lookback])\n",
    "            X_target_nighttime.append(nighttime_mask[i+lookback])\n",
    "            X_clear_sky.append(clear_sky[i+lookback])\n",
    "\n",
    "    return (np.array(X_temp), np.array(X_wind), np.array(X_coord), np.array(X_time),\n",
    "            np.array(y), np.array(X_target_nighttime), np.array(X_clear_sky))\n",
    "\n",
    "lookback = 24\n",
    "train_temp, train_wind, train_coord, train_time, train_y, train_target_nighttime, train_clear_sky = create_sequences(norm_train_data, lookback)\n",
    "val_temp, val_wind, val_coord, val_time, val_y, val_target_nighttime, val_clear_sky = create_sequences(norm_val_data, lookback)\n",
    "test_temp, test_wind, test_coord, test_time, test_y, test_target_nighttime, test_clear_sky = create_sequences(norm_test_data, lookback)\n",
    "print(f\"Train shapes: Temp {train_temp.shape}, y {train_y.shape}, Nighttime {train_target_nighttime.shape}, Clear-Sky {train_clear_sky.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Create PyTorch Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, temp, wind, coord, time, targets, target_nighttime, clear_sky):\n",
    "        self.temp = torch.tensor(temp, dtype=torch.float32)\n",
    "        self.wind = torch.tensor(wind, dtype=torch.float32)\n",
    "        self.coord = torch.tensor(coord, dtype=torch.float32)\n",
    "        self.time = torch.tensor(time, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32).unsqueeze(1)\n",
    "        self.target_nighttime = torch.tensor(target_nighttime, dtype=torch.float32).unsqueeze(1)\n",
    "        self.clear_sky = torch.tensor(clear_sky, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "        self.features = torch.cat([self.temp.unsqueeze(2), self.wind.unsqueeze(2), self.time], dim=2)\n",
    "        self.coord_static = self.coord[:, 0, :]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': self.features[idx],\n",
    "            'coord': self.coord_static[idx],\n",
    "            'target': self.targets[idx],\n",
    "            'target_nighttime': self.target_nighttime[idx],\n",
    "            'clear_sky': self.clear_sky[idx]\n",
    "        }\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_temp, train_wind, train_coord, train_time, train_y, train_target_nighttime, train_clear_sky)\n",
    "val_dataset = TimeSeriesDataset(val_temp, val_wind, val_coord, val_time, val_y, val_target_nighttime, val_clear_sky)\n",
    "test_dataset = TimeSeriesDataset(test_temp, test_wind, test_coord, test_time, test_y, test_target_nighttime, test_clear_sky)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(f\"Features shape: {batch['features'].shape}, Target shape: {batch['target'].shape}, \"\n",
    "          f\"Nighttime shape: {batch['target_nighttime'].shape}, Clear-Sky shape: {batch['clear_sky'].shape}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, coord_dim, hidden_dim=64, num_layers=2, dropout=0.2):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.coord_proj = nn.Sequential(nn.Linear(coord_dim, 16), nn.BatchNorm1d(16), nn.ReLU())\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim + 16, 32), nn.BatchNorm1d(32), nn.ReLU(), nn.Dropout(dropout), nn.Linear(32, 1))\n",
    "\n",
    "    def forward(self, x, coord):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        lstm_out = self.bn1(lstm_out)\n",
    "        coord_out = self.coord_proj(coord)\n",
    "        combined = torch.cat([lstm_out, coord_out], dim=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "input_dim = train_dataset.features.shape[2]\n",
    "coord_dim = train_dataset.coord_static.shape[1]\n",
    "lstm_model = LSTMModel(input_dim=input_dim, coord_dim=coord_dim, hidden_dim=64, num_layers=2, dropout=0.3).to(device)\n",
    "print(lstm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, coord_dim, hidden_dim=64, num_filters=64, num_layers=1, dropout=0.2):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, num_filters, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_filters),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(num_filters, num_filters*2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(num_filters*2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        cnn_output_dim = num_filters * 2\n",
    "        self.lstm = nn.LSTM(input_size=cnn_output_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.bn_lstm = nn.BatchNorm1d(hidden_dim)\n",
    "        self.coord_proj = nn.Sequential(nn.Linear(coord_dim, 16), nn.BatchNorm1d(16), nn.ReLU())\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dim + 16, 32), nn.BatchNorm1d(32), nn.ReLU(), nn.Dropout(dropout), nn.Linear(32, 1))\n",
    "\n",
    "    def forward(self, x, coord):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        cnn_out = self.cnn(x)\n",
    "        cnn_out = cnn_out.permute(0, 2, 1)\n",
    "        lstm_out, _ = self.lstm(cnn_out)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        lstm_out = self.bn_lstm(lstm_out)\n",
    "        coord_out = self.coord_proj(coord)\n",
    "        combined = torch.cat([lstm_out, coord_out], dim=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "cnn_lstm_model = CNNLSTMModel(input_dim=input_dim, coord_dim=coord_dim, hidden_dim=64, num_filters=64, num_layers=1, dropout=0.3).to(device)\n",
    "print(cnn_lstm_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multi-Layer Perceptron (MLP) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, coord_dim, hidden_dims=[128, 256, 128, 64], dropout=0.3):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.flatten_dim = input_dim * lookback\n",
    "        layers = [nn.Linear(self.flatten_dim, hidden_dims[0]), nn.BatchNorm1d(hidden_dims[0]), nn.Tanh(), nn.Dropout(dropout)]\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            layers.extend([nn.Linear(hidden_dims[i], hidden_dims[i+1]), nn.BatchNorm1d(hidden_dims[i+1]), nn.Tanh(), nn.Dropout(dropout)])\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        self.coord_proj = nn.Sequential(nn.Linear(coord_dim, 16), nn.BatchNorm1d(16), nn.Tanh())\n",
    "        self.fc = nn.Sequential(nn.Linear(hidden_dims[-1] + 16, 32), nn.BatchNorm1d(32), nn.Tanh(), nn.Dropout(dropout), nn.Linear(32, 1))\n",
    "\n",
    "    def forward(self, x, coord):\n",
    "        batch_size = x.shape[0]\n",
    "        x_flat = x.reshape(batch_size, -1)\n",
    "        mlp_out = self.mlp(x_flat)\n",
    "        coord_out = self.coord_proj(coord)\n",
    "        combined = torch.cat([mlp_out, coord_out], dim=1)\n",
    "        return self.fc(combined)\n",
    "\n",
    "mlp_model = MLPModel(input_dim=input_dim, coord_dim=coord_dim, hidden_dims=[256, 512, 256, 128], dropout=0.3).to(device)\n",
    "print(mlp_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Physics-Informed MLP Model\n",
    "\n",
    "We use the same MLP architecture but will train it with a physics-informed loss to enforce zero GHI during nighttime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn_mlp_model = MLPModel(input_dim=input_dim, coord_dim=coord_dim, hidden_dims=[256, 512, 256, 128], dropout=0.3).to(device)\n",
    "print(pinn_mlp_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=100, patience=10, lr=0.001):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mae = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            coord = batch['coord'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features, coord)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * features.size(0)\n",
    "            train_mae += F.l1_loss(output, target, reduction='sum').item()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_mae /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_mae = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(device)\n",
    "                coord = batch['coord'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "                output = model(features, coord)\n",
    "                loss = criterion(output, target)\n",
    "                val_loss += loss.item() * features.size(0)\n",
    "                val_mae += F.l1_loss(output, target, reduction='sum').item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_mae /= len(val_loader.dataset)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['val_mae'].append(val_mae)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f} | Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return history\n",
    "\n",
    "def train_pinn_model(model, train_loader, val_loader, epochs=50, patience=10, lr=0.001,\n",
    "                     lambda_night=1.0, lambda_neg=0.5, lambda_clear=0.1):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_mae': [], 'val_mae': []}\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mae = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            coord = batch['coord'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            target_nighttime = batch['target_nighttime'].to(device)\n",
    "            clear_sky = batch['clear_sky'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features, coord)\n",
    "\n",
    "            # Data loss\n",
    "            mse_loss = criterion(output, target)\n",
    "\n",
    "            # Nighttime loss (GHI = 0 when nighttime_mask = 1)\n",
    "            night_loss = torch.mean((output * target_nighttime) ** 2)\n",
    "\n",
    "            # Non-negativity loss (penalize GHI < 0)\n",
    "            neg_loss = torch.mean(torch.relu(-output) ** 2)\n",
    "\n",
    "            # Clear-sky loss (align with clear-sky GHI during daytime)\n",
    "            clear_mask = 1 - target_nighttime\n",
    "            clear_loss = torch.mean((clear_mask * (output - clear_sky)) ** 2)\n",
    "\n",
    "            # Total loss\n",
    "            total_loss = mse_loss + lambda_night * night_loss + lambda_neg * neg_loss + lambda_clear * clear_loss\n",
    "\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += total_loss.item() * features.size(0)\n",
    "            train_mae += torch.abs(output - target).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_mae /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_mae = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                features = batch['features'].to(device)\n",
    "                coord = batch['coord'].to(device)\n",
    "                target = batch['target'].to(device)\n",
    "                target_nighttime = batch['target_nighttime'].to(device)\n",
    "                clear_sky = batch['clear_sky'].to(device)\n",
    "\n",
    "                output = model(features, coord)\n",
    "                mse_loss = criterion(output, target)\n",
    "                night_loss = torch.mean((output * target_nighttime) ** 2)\n",
    "                neg_loss = torch.mean(torch.relu(-output) ** 2)\n",
    "                clear_mask = 1 - target_nighttime\n",
    "                clear_loss = torch.mean((clear_mask * (output - clear_sky)) ** 2)\n",
    "                total_loss = mse_loss + lambda_night * night_loss + lambda_neg * neg_loss + lambda_clear * clear_loss\n",
    "\n",
    "                val_loss += total_loss.item() * features.size(0)\n",
    "                val_mae += torch.abs(output - target).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_mae /= len(val_loader.dataset)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_mae'].append(train_mae)\n",
    "        history['val_mae'].append(val_mae)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f}, Train MAE: {train_mae:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val MAE: {val_mae:.4f} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return history\n",
    "\n",
    "def evaluate_model(model, data_loader, scaler):\n",
    "    model.eval()\n",
    "    all_outputs = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            coord = batch['coord'].to(device)\n",
    "            target = batch['target'].to(device)\n",
    "            output = model(features, coord)\n",
    "            all_outputs.append(output.cpu().numpy())\n",
    "            all_targets.append(target.cpu().numpy())\n",
    "\n",
    "    all_outputs = np.vstack(all_outputs)\n",
    "    all_targets = np.vstack(all_targets)\n",
    "    y_pred_orig = scaler.inverse_transform(all_outputs)\n",
    "    y_true_orig = scaler.inverse_transform(all_targets)\n",
    "\n",
    "    mse = mean_squared_error(y_true_orig, y_pred_orig)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
    "    r2 = r2_score(y_true_orig, y_pred_orig)\n",
    "\n",
    "    print(f\"Evaluation Metrics:\\n  MSE: {mse:.2f}\\n  RMSE: {rmse:.2f}\\n  MAE: {mae:.2f}\\n  R²: {r2:.4f}\")\n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2, 'y_pred': y_pred_orig, 'y_true': y_true_orig}\n",
    "\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train')\n",
    "    plt.plot(history['val_loss'], label='Validation')\n",
    "    plt.title('Model Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_mae'], label='Train')\n",
    "    plt.plot(history['val_mae'], label='Validation')\n",
    "    plt.title('Model MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_predictions(metrics, model_name=''):\n",
    "    residuals = metrics['y_true'] - metrics['y_pred']\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    sample_size = min(1000, len(metrics['y_true']))\n",
    "    sample_indices = np.random.choice(len(metrics['y_true']), sample_size, replace=False)\n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(metrics['y_true'][sample_indices], metrics['y_pred'][sample_indices], alpha=0.5)\n",
    "    plt.plot([metrics['y_true'].min(), metrics['y_true'].max()], [metrics['y_true'].min(), metrics['y_true'].max()], 'r--')\n",
    "    plt.title(f'{model_name} - Actual vs Predicted GHI')\n",
    "    plt.xlabel('Actual GHI (W/m²)')\n",
    "    plt.ylabel('Predicted GHI (W/m²)')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.hist(residuals, bins=50, alpha=0.7)\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.title(f'{model_name} - Residuals Distribution')\n",
    "    plt.xlabel('Residual')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(metrics['y_pred'][sample_indices], residuals[sample_indices], alpha=0.5)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')\n",
    "    plt.title(f'{model_name} - Residuals vs Predicted')\n",
    "    plt.xlabel('Predicted GHI (W/m²)')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.axis('off')\n",
    "    stats_text = f\"Metrics:\\nMSE: {metrics['mse']:.2f}\\nRMSE: {metrics['rmse']:.2f}\\nMAE: {metrics['mae']:.2f}\\nR²: {metrics['r2']:.4f}\\n\\n\"\n",
    "    stats_text += f\"Residual Stats:\\nMean: {np.mean(residuals):.2f}\\nStd: {np.std(residuals):.2f}\"\n",
    "    plt.text(0.1, 0.5, stats_text, fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train and Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Train and Evaluate LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training LSTM model...\")\n",
    "lstm_history = train_model(lstm_model, train_loader, val_loader, epochs=50, patience=10, lr=0.001)\n",
    "plot_training_history(lstm_history)\n",
    "\n",
    "print(\"Evaluating LSTM model on validation set...\")\n",
    "lstm_val_metrics = evaluate_model(lstm_model, val_loader, scalers['ghi_scaler'])\n",
    "plot_predictions(lstm_val_metrics, model_name='LSTM - Validation')\n",
    "\n",
    "print(\"\\nEvaluating LSTM model on test set...\")\n",
    "lstm_test_metrics = evaluate_model(lstm_model, test_loader, scalers['ghi_scaler'])\n",
    "plot_predictions(lstm_test_metrics, model_name='LSTM - Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Train and Evaluate CNN-LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training CNN-LSTM model...\")\n",
    "cnn_lstm_history = train_model(cnn_lstm_model, train_loader, val_loader, epochs=50, patience=10, lr=0.001)\n",
    "plot_training_history(cnn_lstm_history)\n",
    "\n",
    "print(\"Evaluating CNN-LSTM model on validation set...\")\n",
    "cnn_lstm_val_metrics = evaluate_model(cnn_lstm_model, val_loader, scalers['ghi_scaler'])\n",
    "plot_predictions(cnn_lstm_val_metrics, model_name='CNN-LSTM - Validation')\n",
    "\n",
    "print(\"\\nEvaluating CNN-LSTM model on test set...\")\n",
    "cnn_lstm_test_metrics = evaluate_model(cnn_lstm_model, test_loader, scalers['ghi_scaler'])\n",
    "plot_predictions(cnn_lstm_test_metrics, model_name='CNN-LSTM - Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Train and Evaluate MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training MLP model...\")\n",
    "mlp_history = train_model(mlp_model, train_loader, val_loader, epochs=50, patience=10, lr=0.001)\n",
    "plot_training_history(mlp_history)\n",
    "\n",
    "print(\"Evaluating MLP model on validation set...\")\n",
    "mlp_val_metrics = evaluate_model(mlp_model, val_loader, scalers['ghi_scaler'])\n",
    "plot_predictions(mlp_val_metrics, model_name='MLP - Validation')\n",
    "\n",
    "print(\"\\nEvaluating MLP model on test set...\")\n",
    "mlp_test_metrics = evaluate_model(mlp_model, test_loader, scalers['ghi_scaler'])\n",
    "plot_predictions(mlp_test_metrics, model_name='MLP - Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Train and Evaluate PINN-MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training PINN-MLP model...\")\n",
    "pinn_mlp_history = train_pinn_model(pinn_mlp_model, train_loader, val_loader, epochs=50, patience=10, lr=0.001, lambda_phys=1.0)\n",
    "plot_training_history(pinn_mlp_history)\n",
    "\n",
    "print(\"Evaluating PINN-MLP model on validation set...\")\n",
    "pinn_mlp_val_metrics = evaluate_model(pinn_mlp_model, val_loader, scalers['ghi_scaler'])\n",
    "plot_predictions(pinn_mlp_val_metrics, model_name='PINN-MLP - Validation')\n",
    "\n",
    "print(\"\\nEvaluating PINN-MLP model on test set...\")\n",
    "pinn_mlp_test_metrics = evaluate_model(pinn_mlp_model, test_loader, scalers['ghi_scaler'])\n",
    "plot_predictions(pinn_mlp_test_metrics, model_name='PINN-MLP - Test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(metrics_list, model_names):\n",
    "    metrics = ['mse', 'rmse', 'mae', 'r2']\n",
    "    metric_labels = ['MSE', 'RMSE', 'MAE', 'R²']\n",
    "    comparison = pd.DataFrame(index=metric_labels, columns=model_names)\n",
    "\n",
    "    for i, model_metrics in enumerate(metrics_list):\n",
    "        for j, metric in enumerate(metrics):\n",
    "            comparison.iloc[j, i] = model_metrics[metric]\n",
    "\n",
    "    print(\"Model Comparison:\")\n",
    "    print(comparison)\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    for i, metric in enumerate(metrics):\n",
    "        row, col = i // 2, i % 2\n",
    "        ax = axes[row, col]\n",
    "        values = [metrics_dict[metric] for metrics_dict in metrics_list]\n",
    "        bars = ax.bar(model_names, values)\n",
    "        for bar, value in zip(bars, values):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + (0.01 if metric == 'r2' else 0.1),\n",
    "                    f\"{value:.4f}\" if metric == 'r2' else f\"{value:.2f}\", ha='center', va='bottom')\n",
    "        ax.set_title(f'{metric_labels[i]} Comparison')\n",
    "        ax.set_ylabel(metric_labels[i])\n",
    "        ax.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Validation Set Comparison:\")\n",
    "compare_models([lstm_val_metrics, cnn_lstm_val_metrics, mlp_val_metrics, pinn_mlp_val_metrics],\n",
    "               ['LSTM', 'CNN-LSTM', 'MLP', 'PINN-MLP'])\n",
    "\n",
    "print(\"\\nTest Set Comparison:\")\n",
    "compare_models([lstm_test_metrics, cnn_lstm_test_metrics, mlp_test_metrics, pinn_mlp_test_metrics],\n",
    "               ['LSTM', 'CNN-LSTM', 'MLP', 'PINN-MLP'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Time Series Predictions\n",
    "\n",
    "Visualize predictions over time, including the PINN-MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions_over_time(models, model_names, data_loader, scaler, num_samples=200, start_idx=0):\n",
    "    all_features = []\n",
    "    all_coords = []\n",
    "    all_targets = []\n",
    "\n",
    "    for batch in data_loader:\n",
    "        all_features.extend(batch['features'])\n",
    "        all_coords.extend(batch['coord'])\n",
    "        all_targets.extend(batch['target'])\n",
    "        if len(all_targets) >= start_idx + num_samples:\n",
    "            break\n",
    "\n",
    "    features = torch.stack(all_features[start_idx:start_idx+num_samples]).to(device)\n",
    "    coords = torch.stack(all_coords[start_idx:start_idx+num_samples]).to(device)\n",
    "    targets = torch.stack(all_targets[start_idx:start_idx+num_samples]).cpu().numpy()\n",
    "\n",
    "    predictions = []\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features, coords).cpu().numpy()\n",
    "            predictions.append(outputs)\n",
    "\n",
    "    y_true_orig = scaler.inverse_transform(targets)\n",
    "    y_pred_orig_list = [scaler.inverse_transform(pred) for pred in predictions]\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(y_true_orig, 'k-', label='Actual GHI', linewidth=2)\n",
    "    colors = ['b-', 'r-', 'g-', 'm-']\n",
    "    for i, (pred, name) in enumerate(zip(y_pred_orig_list, model_names)):\n",
    "        plt.plot(pred, colors[i], label=f'{name} Predicted', alpha=0.7)\n",
    "    plt.title('GHI Predictions Over Time')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('GHI (W/m²)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions_over_time(\n",
    "    models=[lstm_model, cnn_lstm_model, mlp_model, pinn_mlp_model],\n",
    "    model_names=['LSTM', 'CNN-LSTM', 'MLP', 'PINN-MLP'],\n",
    "    data_loader=test_loader,\n",
    "    scaler=scalers['ghi_scaler'],\n",
    "    num_samples=200,\n",
    "    start_idx=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "This notebook implemented four deep learning models for GHI forecasting:\n",
    "\n",
    "1. **LSTM Model**: Captures long-term temporal dependencies.\n",
    "2. **CNN-LSTM Model**: Combines local pattern extraction with temporal modeling.\n",
    "3. **MLP Model**: Processes flattened time series with static features.\n",
    "4. **PINN-MLP Model**: Enhances MLP with a physics-informed loss to enforce zero GHI at night.\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "- The PINN-MLP model incorporates physical constraints, potentially improving generalization.\n",
    "- Performance varies across models, with trade-offs in complexity and accuracy.\n",
    "- Spatial and temporal features enhance model capability.\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "- Tune `lambda_phys` in PINN-MLP for optimal balance.\n",
    "- Incorporate additional physical constraints (e.g., clear-sky models).\n",
    "- Explore ensemble methods and multi-step forecasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm_model.state_dict(), 'lstm_ghi_forecasting_model.pt')\n",
    "torch.save(cnn_lstm_model.state_dict(), 'cnn_lstm_ghi_forecasting_model.pt')\n",
    "torch.save(mlp_model.state_dict(), 'mlp_ghi_forecasting_model.pt')\n",
    "torch.save(pinn_mlp_model.state_dict(), 'pinn_mlp_ghi_forecasting_model.pt')\n",
    "print(\"Models saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
